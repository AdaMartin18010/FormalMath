# æ•°å€¼çº¿æ€§ä»£æ•° - å›½é™…æ ‡å‡†ç‰ˆ

## ğŸ“š æ¦‚è¿°

æœ¬æ–‡æ¡£åŸºäºå›½é™…æ ‡å‡†å’Œ2025å¹´è‘—åå¤§å­¦æ•°å€¼åˆ†æè¯¾ç¨‹ï¼Œå…¨é¢é˜è¿°æ•°å€¼çº¿æ€§ä»£æ•°çš„æ ¸å¿ƒç®—æ³•ã€ç¨³å®šæ€§åˆ†æå’Œç°ä»£åº”ç”¨ã€‚

## ğŸ¯ å¯¹æ ‡å›½é™…æ ‡å‡†

### å›½é™…æƒå¨æ ‡å‡†

- **Wikipedia**: Numerical linear algebra, Matrix decomposition, Eigenvalue algorithm
- **MIT**: 18.335 Introduction to Numerical Methods
- **Stanford**: Math 104 Applied Matrix Theory
- **Cambridge**: Part II Numerical Analysis
- **Oxford**: Numerical Linear Algebra
- **Golub & Van Loan**: Matrix Computations (ç»å…¸æ•™æ)

## 1. çŸ©é˜µåˆ†è§£ç†è®º

### 1.1 LUåˆ†è§£

**å®šä¹‰ 1.1** (LUåˆ†è§£)
è®¾ $A$ æ˜¯ $n \times n$ çŸ©é˜µï¼Œå¦‚æœå­˜åœ¨ä¸‹ä¸‰è§’çŸ©é˜µ $L$ å’Œä¸Šä¸‰è§’çŸ©é˜µ $U$ ä½¿å¾—ï¼š
$$A = LU$$
åˆ™ç§°è¿™æ˜¯ $A$ çš„LUåˆ†è§£ã€‚

**ç®—æ³•å®ç°**:

```python
import numpy as np
from typing import Tuple, Optional

def lu_decomposition(A: np.ndarray, pivot: bool = True) -> Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]:
    """
    LUåˆ†è§£ç®—æ³•
    
    Args:
        A: è¾“å…¥çŸ©é˜µ
        pivot: æ˜¯å¦ä½¿ç”¨é€‰ä¸»å…ƒ
        
    Returns:
        L: ä¸‹ä¸‰è§’çŸ©é˜µ
        U: ä¸Šä¸‰è§’çŸ©é˜µ
        P: ç½®æ¢çŸ©é˜µï¼ˆå¦‚æœpivot=Trueï¼‰
    """
    n = A.shape[0]
    A_copy = A.copy().astype(float)
    
    if pivot:
        P = np.eye(n)
        L = np.eye(n)
        
        for k in range(n-1):
            # é€‰ä¸»å…ƒ
            pivot_row = k + np.argmax(np.abs(A_copy[k:, k]))
            if pivot_row != k:
                A_copy[[k, pivot_row]] = A_copy[[pivot_row, k]]
                P[[k, pivot_row]] = P[[pivot_row, k]]
                if k > 0:
                    L[[k, pivot_row], :k] = L[[pivot_row, k], :k]
            
            # æ¶ˆå…ƒ
            for i in range(k+1, n):
                L[i, k] = A_copy[i, k] / A_copy[k, k]
                A_copy[i, k:] -= L[i, k] * A_copy[k, k:]
        
        U = np.triu(A_copy)
        return L, U, P
    else:
        L = np.eye(n)
        
        for k in range(n-1):
            for i in range(k+1, n):
                L[i, k] = A_copy[i, k] / A_copy[k, k]
                A_copy[i, k:] -= L[i, k] * A_copy[k, k:]
        
        U = np.triu(A_copy)
        return L, U, None

def solve_lu(L: np.ndarray, U: np.ndarray, b: np.ndarray, P: Optional[np.ndarray] = None) -> np.ndarray:
    """
    ä½¿ç”¨LUåˆ†è§£æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„ Ax = b
    """
    n = L.shape[0]
    
    # å‰å‘ä»£å…¥ Ly = Pb
    if P is not None:
        y = np.linalg.solve(L, P @ b)
    else:
        y = np.linalg.solve(L, b)
    
    # åå‘ä»£å…¥ Ux = y
    x = np.linalg.solve(U, y)
    
    return x
```

### 1.2 QRåˆ†è§£

**å®šä¹‰ 1.2** (QRåˆ†è§£)
è®¾ $A$ æ˜¯ $m \times n$ çŸ©é˜µï¼Œå¦‚æœå­˜åœ¨æ­£äº¤çŸ©é˜µ $Q$ å’Œä¸Šä¸‰è§’çŸ©é˜µ $R$ ä½¿å¾—ï¼š
$$A = QR$$
åˆ™ç§°è¿™æ˜¯ $A$ çš„QRåˆ†è§£ã€‚

**Gram-Schmidtæ­£äº¤åŒ–**:

```python
def gram_schmidt_qr(A: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    ä½¿ç”¨Gram-Schmidtæ­£äº¤åŒ–çš„QRåˆ†è§£
    """
    m, n = A.shape
    Q = np.zeros((m, n))
    R = np.zeros((n, n))
    
    for j in range(n):
        v = A[:, j].copy()
        
        # å‡å»å‰é¢æ‰€æœ‰å‘é‡çš„æŠ•å½±
        for i in range(j):
            R[i, j] = np.dot(Q[:, i], A[:, j])
            v -= R[i, j] * Q[:, i]
        
        # å½’ä¸€åŒ–
        R[j, j] = np.linalg.norm(v)
        if R[j, j] > 1e-12:
            Q[:, j] = v / R[j, j]
        else:
            Q[:, j] = v
    
    return Q, R

def householder_qr(A: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    ä½¿ç”¨Householderå˜æ¢çš„QRåˆ†è§£ï¼ˆæ›´ç¨³å®šï¼‰
    """
    m, n = A.shape
    A_copy = A.copy().astype(float)
    Q = np.eye(m)
    
    for k in range(min(m-1, n)):
        # æ„é€ Householderå‘é‡
        x = A_copy[k:, k]
        e1 = np.zeros_like(x)
        e1[0] = 1
        
        u = x - np.linalg.norm(x) * e1
        if np.linalg.norm(u) > 1e-12:
            u = u / np.linalg.norm(u)
        else:
            u = np.zeros_like(u)
        
        # HouseholderçŸ©é˜µ H = I - 2uu^T
        H = np.eye(m-k) - 2 * np.outer(u, u)
        
        # æ›´æ–°Aå’ŒQ
        A_copy[k:, k:] = H @ A_copy[k:, k:]
        Q[k:, :] = H @ Q[k:, :]
    
    R = np.triu(A_copy[:n, :])
    Q = Q.T
    
    return Q, R
```

### 1.3 å¥‡å¼‚å€¼åˆ†è§£ (SVD)

**å®šä¹‰ 1.3** (å¥‡å¼‚å€¼åˆ†è§£)
è®¾ $A$ æ˜¯ $m \times n$ çŸ©é˜µï¼Œå­˜åœ¨æ­£äº¤çŸ©é˜µ $U$ã€å¯¹è§’çŸ©é˜µ $\Sigma$ å’Œæ­£äº¤çŸ©é˜µ $V^T$ ä½¿å¾—ï¼š
$$A = U\Sigma V^T$$

**ç®—æ³•å®ç°**:

```python
def power_iteration(A: np.ndarray, max_iter: int = 100, tol: float = 1e-10) -> Tuple[float, np.ndarray]:
    """
    å¹‚è¿­ä»£æ³•è®¡ç®—æœ€å¤§ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
    """
    n = A.shape[0]
    x = np.random.randn(n)
    x = x / np.linalg.norm(x)
    
    for _ in range(max_iter):
        x_new = A @ x
        x_new = x_new / np.linalg.norm(x_new)
        
        if np.linalg.norm(x_new - x) < tol:
            break
        x = x_new
    
    eigenvalue = np.dot(x, A @ x)
    return eigenvalue, x

def svd_power_method(A: np.ndarray, k: int = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    ä½¿ç”¨å¹‚è¿­ä»£æ³•çš„SVDåˆ†è§£ï¼ˆç®€åŒ–ç‰ˆï¼‰
    """
    if k is None:
        k = min(A.shape)
    
    m, n = A.shape
    U = np.zeros((m, k))
    S = np.zeros(k)
    Vt = np.zeros((k, n))
    
    # è®¡ç®—A^T Açš„æœ€å¤§ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
    ATA = A.T @ A
    for i in range(k):
        eigenvalue, eigenvector = power_iteration(ATA)
        S[i] = np.sqrt(eigenvalue)
        Vt[i, :] = eigenvector
        U[:, i] = (A @ eigenvector) / S[i]
    
    return U, S, Vt

def svd_truncated(A: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    æˆªæ–­SVDåˆ†è§£
    """
    U, s, Vt = np.linalg.svd(A, full_matrices=False)
    return U[:, :k], s[:k], Vt[:k, :]
```

## 2. ç‰¹å¾å€¼è®¡ç®—

### 2.1 å¹‚è¿­ä»£æ³•

**ç®—æ³•æè¿°**:
å¯¹äºçŸ©é˜µ $A$ï¼Œå¹‚è¿­ä»£æ³•é€šè¿‡è¿­ä»£ $x_{k+1} = \frac{Ax_k}{\|Ax_k\|}$ æ¥è®¡ç®—æœ€å¤§ç‰¹å¾å€¼ã€‚

```python
def inverse_power_iteration(A: np.ndarray, sigma: float = 0, max_iter: int = 100) -> Tuple[complex, np.ndarray]:
    """
    åå¹‚è¿­ä»£æ³•è®¡ç®—æœ€æ¥è¿‘sigmaçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
    """
    n = A.shape[0]
    I = np.eye(n)
    B = A - sigma * I
    
    x = np.random.randn(n)
    x = x / np.linalg.norm(x)
    
    for _ in range(max_iter):
        # è§£çº¿æ€§æ–¹ç¨‹ç»„ Bx_new = x
        x_new = np.linalg.solve(B, x)
        x_new = x_new / np.linalg.norm(x_new)
        
        if np.linalg.norm(x_new - x) < 1e-10:
            break
        x = x_new
    
    eigenvalue = sigma + 1 / np.dot(x, np.linalg.solve(B, x))
    return eigenvalue, x
```

### 2.2 QRç®—æ³•

**QRç®—æ³•**æ˜¯è®¡ç®—æ‰€æœ‰ç‰¹å¾å€¼çš„æ ‡å‡†æ–¹æ³•ï¼š

```python
def qr_algorithm(A: np.ndarray, max_iter: int = 100, tol: float = 1e-10) -> Tuple[np.ndarray, np.ndarray]:
    """
    QRç®—æ³•è®¡ç®—æ‰€æœ‰ç‰¹å¾å€¼
    """
    n = A.shape[0]
    A_copy = A.copy().astype(float)
    Q_total = np.eye(n)
    
    for iteration in range(max_iter):
        # QRåˆ†è§£
        Q, R = np.linalg.qr(A_copy)
        
        # æ›´æ–°A
        A_copy = R @ Q
        Q_total = Q_total @ Q
        
        # æ£€æŸ¥æ”¶æ•›æ€§
        if np.max(np.abs(np.tril(A_copy, -1))) < tol:
            break
    
    eigenvalues = np.diag(A_copy)
    eigenvectors = Q_total
    
    return eigenvalues, eigenvectors
```

## 3. çº¿æ€§æ–¹ç¨‹ç»„æ±‚è§£

### 3.1 ç›´æ¥æ–¹æ³•

**é«˜æ–¯æ¶ˆå»æ³•**:

```python
def gaussian_elimination(A: np.ndarray, b: np.ndarray) -> np.ndarray:
    """
    é«˜æ–¯æ¶ˆå»æ³•æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„
    """
    n = A.shape[0]
    A_aug = np.column_stack([A, b])
    
    # å‰å‘æ¶ˆå…ƒ
    for k in range(n-1):
        for i in range(k+1, n):
            factor = A_aug[i, k] / A_aug[k, k]
            A_aug[i, k:] -= factor * A_aug[k, k:]
    
    # åå‘ä»£å…¥
    x = np.zeros(n)
    for i in range(n-1, -1, -1):
        x[i] = (A_aug[i, -1] - np.dot(A_aug[i, i+1:n], x[i+1:])) / A_aug[i, i]
    
    return x
```

### 3.2 è¿­ä»£æ–¹æ³•

**é›…å¯æ¯”è¿­ä»£æ³•**:

```python
def jacobi_iteration(A: np.ndarray, b: np.ndarray, x0: np.ndarray = None, 
                    max_iter: int = 1000, tol: float = 1e-10) -> np.ndarray:
    """
    é›…å¯æ¯”è¿­ä»£æ³•æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„
    """
    n = A.shape[0]
    if x0 is None:
        x0 = np.zeros(n)
    
    x = x0.copy()
    
    for iteration in range(max_iter):
        x_new = np.zeros(n)
        
        for i in range(n):
            sum_ax = 0
            for j in range(n):
                if i != j:
                    sum_ax += A[i, j] * x[j]
            x_new[i] = (b[i] - sum_ax) / A[i, i]
        
        if np.linalg.norm(x_new - x) < tol:
            break
        x = x_new
    
    return x
```

**å…±è½­æ¢¯åº¦æ³•**:

```python
def conjugate_gradient(A: np.ndarray, b: np.ndarray, x0: np.ndarray = None,
                      max_iter: int = 1000, tol: float = 1e-10) -> np.ndarray:
    """
    å…±è½­æ¢¯åº¦æ³•æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„ï¼ˆé€‚ç”¨äºå¯¹ç§°æ­£å®šçŸ©é˜µï¼‰
    """
    n = A.shape[0]
    if x0 is None:
        x0 = np.zeros(n)
    
    x = x0.copy()
    r = b - A @ x
    p = r.copy()
    
    for iteration in range(max_iter):
        Ap = A @ p
        alpha = np.dot(r, r) / np.dot(p, Ap)
        x = x + alpha * p
        r_new = r - alpha * Ap
        
        if np.linalg.norm(r_new) < tol:
            break
        
        beta = np.dot(r_new, r_new) / np.dot(r, r)
        p = r_new + beta * p
        r = r_new
    
    return x
```

## 4. æ•°å€¼ç¨³å®šæ€§åˆ†æ

### 4.1 æ¡ä»¶æ•°

**å®šä¹‰ 4.1** (çŸ©é˜µæ¡ä»¶æ•°)
çŸ©é˜µ $A$ çš„æ¡ä»¶æ•°å®šä¹‰ä¸ºï¼š
$$\kappa(A) = \|A\| \cdot \|A^{-1}\|$$

```python
def condition_number(A: np.ndarray, norm_type: str = "2") -> float:
    """
    è®¡ç®—çŸ©é˜µæ¡ä»¶æ•°
    """
    if norm_type == "2":
        return np.linalg.cond(A, 2)
    elif norm_type == "1":
        return np.linalg.cond(A, 1)
    elif norm_type == "inf":
        return np.linalg.cond(A, np.inf)
    else:
        raise ValueError("ä¸æ”¯æŒçš„èŒƒæ•°ç±»å‹")

def backward_error_analysis(A: np.ndarray, b: np.ndarray, x_computed: np.ndarray) -> float:
    """
    åå‘è¯¯å·®åˆ†æ
    """
    residual = b - A @ x_computed
    backward_error = np.linalg.norm(residual) / np.linalg.norm(b)
    return backward_error
```

### 4.2 ç¨³å®šæ€§æµ‹è¯•

```python
def stability_test():
    """
    æ•°å€¼ç¨³å®šæ€§æµ‹è¯•
    """
    # æ„é€ ç—…æ€çŸ©é˜µ
    n = 10
    A = np.random.randn(n, n)
    A = A @ A.T  # å¯¹ç§°æ­£å®šçŸ©é˜µ
    
    # æ·»åŠ æ‰°åŠ¨ä½¿å…¶ç—…æ€
    A[0, 0] = 1e-12
    A[n-1, n-1] = 1e12
    
    b = np.random.randn(n)
    x_true = np.linalg.solve(A, b)
    
    # ä½¿ç”¨ä¸åŒæ–¹æ³•æ±‚è§£
    methods = {
        "ç›´æ¥æ±‚è§£": lambda: np.linalg.solve(A, b),
        "LUåˆ†è§£": lambda: solve_lu(*lu_decomposition(A)[:2], b),
        "QRåˆ†è§£": lambda: np.linalg.solve(A, b),  # ç®€åŒ–
        "å…±è½­æ¢¯åº¦": lambda: conjugate_gradient(A, b)
    }
    
    print("æ•°å€¼ç¨³å®šæ€§æµ‹è¯•ç»“æœ:")
    print(f"æ¡ä»¶æ•°: {condition_number(A):.2e}")
    print()
    
    for method_name, method_func in methods.items():
        try:
            x_computed = method_func()
            error = np.linalg.norm(x_computed - x_true) / np.linalg.norm(x_true)
            backward_error = backward_error_analysis(A, b, x_computed)
            print(f"{method_name}:")
            print(f"  ç›¸å¯¹è¯¯å·®: {error:.2e}")
            print(f"  åå‘è¯¯å·®: {backward_error:.2e}")
        except Exception as e:
            print(f"{method_name}: å¤±è´¥ - {e}")
        print()

if __name__ == "__main__":
    stability_test()
```

## 5. ç°ä»£åº”ç”¨

### 5.1 å¤§è§„æ¨¡ç¨€ç–çŸ©é˜µ

```python
from scipy import sparse

def sparse_matrix_operations():
    """
    ç¨€ç–çŸ©é˜µæ“ä½œç¤ºä¾‹
    """
    # æ„é€ ç¨€ç–çŸ©é˜µ
    n = 1000
    A_sparse = sparse.random(n, n, density=0.01, format='csr')
    b = np.random.randn(n)
    
    # ç¨€ç–çŸ©é˜µæ±‚è§£
    x_sparse = sparse.linalg.spsolve(A_sparse, b)
    
    # ç‰¹å¾å€¼è®¡ç®—
    eigenvalues = sparse.linalg.eigs(A_sparse, k=5, which='LM')[0]
    
    return x_sparse, eigenvalues
```

### 5.2 å¹¶è¡Œè®¡ç®—

```python
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor

def parallel_matrix_multiply(A: np.ndarray, B: np.ndarray, n_jobs: int = -1) -> np.ndarray:
    """
    å¹¶è¡ŒçŸ©é˜µä¹˜æ³•
    """
    if n_jobs == -1:
        n_jobs = mp.cpu_count()
    
    m, n = A.shape
    n, p = B.shape
    
    def multiply_block(args):
        i_start, i_end = args
        return A[i_start:i_end, :] @ B
    
    # åˆ†å—
    block_size = m // n_jobs
    blocks = [(i * block_size, min((i + 1) * block_size, m)) for i in range(n_jobs)]
    
    with ProcessPoolExecutor(max_workers=n_jobs) as executor:
        results = list(executor.map(multiply_block, blocks))
    
    return np.vstack(results)
```

## 6. æ€§èƒ½ä¼˜åŒ–

### 6.1 å†…å­˜ä¼˜åŒ–

```python
def memory_efficient_qr(A: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    å†…å­˜é«˜æ•ˆçš„QRåˆ†è§£
    """
    m, n = A.shape
    A_copy = A.copy()
    Q = np.eye(m)
    
    for k in range(min(m-1, n)):
        # å°±åœ°è®¡ç®—Householderå˜æ¢
        x = A_copy[k:, k]
        norm_x = np.linalg.norm(x)
        
        if norm_x > 1e-12:
            if x[0] >= 0:
                x[0] += norm_x
            else:
                x[0] -= norm_x
            
            beta = 2.0 / np.dot(x, x)
            
            # æ›´æ–°A
            for j in range(k, n):
                v_dot_a = np.dot(x, A_copy[k:, j])
                A_copy[k:, j] -= beta * v_dot_a * x
            
            # æ›´æ–°Q
            for j in range(m):
                v_dot_q = np.dot(x, Q[k:, j])
                Q[k:, j] -= beta * v_dot_q * x
    
    R = np.triu(A_copy[:n, :])
    return Q.T, R
```

### 6.2 æ•°å€¼ç²¾åº¦ä¼˜åŒ–

```python
def high_precision_svd(A: np.ndarray, precision: str = "double") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    é«˜ç²¾åº¦SVDåˆ†è§£
    """
    if precision == "double":
        dtype = np.float64
    elif precision == "longdouble":
        dtype = np.longdouble
    else:
        raise ValueError("ä¸æ”¯æŒçš„ç²¾åº¦ç±»å‹")
    
    A_high = A.astype(dtype)
    U, s, Vt = np.linalg.svd(A_high, full_matrices=False)
    
    return U, s, Vt
```

## å‚è€ƒæ–‡çŒ®

### å›½é™…æ ‡å‡†æ–‡çŒ®

1. Wikipedia contributors. (2024). *Numerical linear algebra*. Wikipedia.
2. Wikipedia contributors. (2024). *Matrix decomposition*. Wikipedia.
3. Wikipedia contributors. (2024). *Eigenvalue algorithm*. Wikipedia.

### å›½é™…å¤§å­¦æ ‡å‡†

1. MIT Mathematics Department. (2025). *Introduction to Numerical Methods*. MIT OpenCourseWare.
2. Stanford Mathematics Department. (2025). *Applied Matrix Theory*. Stanford University.
3. Cambridge Mathematics Department. (2025). *Numerical Analysis*. University of Cambridge.

### ç»å…¸æ•™æ

1. Golub, G. H., & Van Loan, C. F. (2013). *Matrix Computations*. Johns Hopkins University Press.
2. Trefethen, L. N., & Bau, D. (1997). *Numerical Linear Algebra*. SIAM.
3. Demmel, J. W. (1997). *Applied Numerical Linear Algebra*. SIAM.

### å‰æ²¿å‘å±•

1. Higham, N. J. (2002). *Accuracy and Stability of Numerical Algorithms*. SIAM.
2. Saad, Y. (2003). *Iterative Methods for Sparse Linear Systems*. SIAM.
3. Anderson, E., et al. (1999). *LAPACK Users' Guide*. SIAM.

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ  
**ç»´æŠ¤è€…**: FormalMathé¡¹ç›®ç»„  
**è®¸å¯è¯**: MIT License
