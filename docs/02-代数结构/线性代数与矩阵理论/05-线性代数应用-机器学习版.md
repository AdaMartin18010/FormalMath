# çº¿æ€§ä»£æ•°åº”ç”¨ - æœºå™¨å­¦ä¹ ç‰ˆ





## ğŸ“š æ¦‚è¿°





æœ¬æ–‡æ¡£åŸºäºå›½é™…æ ‡å‡†å’Œ2025å¹´æœºå™¨å­¦ä¹ å‰æ²¿å‘å±•ï¼Œå…¨é¢é˜è¿°çº¿æ€§ä»£æ•°åœ¨æœºå™¨å­¦ä¹ ä¸­çš„æ ¸å¿ƒåº”ç”¨ï¼Œä»åŸºç¡€ç®—æ³•åˆ°æ·±åº¦å­¦ä¹ çš„å‰æ²¿æŠ€æœ¯ã€‚





## ğŸ¯ å¯¹æ ‡å›½é™…æ ‡å‡†





### å›½é™…æƒå¨æ ‡å‡†





- **Wikipedia**: Principal component analysis, Singular value decomposition, Linear regression


- **MIT**: 6.036 Introduction to Machine Learning, 6.867 Machine Learning


- **Stanford**: CS229 Machine Learning, CS231n Convolutional Neural Networks


- **Cambridge**: Part II Machine Learning and Bayesian Inference


- **Oxford**: Machine Learning, Deep Learning


- **ç»å…¸æ•™æ**: Bishop - Pattern Recognition and Machine Learning, Goodfellow - Deep Learning





## 1. ä¸»æˆåˆ†åˆ†æ (PCA)





### 1.1 ç†è®ºåŸºç¡€





**å®šä¹‰ 1.1** (ä¸»æˆåˆ†åˆ†æ)


è®¾ $X \in \mathbb{R}^{n \times d}$ æ˜¯æ•°æ®çŸ©é˜µï¼Œå…¶ä¸­æ¯è¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬ï¼Œæ¯åˆ—æ˜¯ä¸€ä¸ªç‰¹å¾ã€‚PCAçš„ç›®æ ‡æ˜¯æ‰¾åˆ°æ•°æ®çš„ä¸»è¦å˜åŒ–æ–¹å‘ã€‚





**æ•°å­¦å½¢å¼åŒ–**:


$$\max_{w} \text{Var}(Xw) = \max_{w} w^T \Sigma w$$


å…¶ä¸­ $\Sigma = \frac{1}{n-1} X^T X$ æ˜¯åæ–¹å·®çŸ©é˜µï¼Œçº¦æŸæ¡ä»¶ä¸º $w^T w = 1$ã€‚





**å®šç† 1.1** (PCAçš„æœ€ä¼˜è§£)


PCAçš„æœ€ä¼˜è§£æ˜¯åæ–¹å·®çŸ©é˜µ $\Sigma$ çš„ç‰¹å¾å‘é‡ï¼ŒæŒ‰ç‰¹å¾å€¼é™åºæ’åˆ—ã€‚





### 1.2 ç®—æ³•å®ç°





```python


import numpy as np


from typing import Tuple, Optional


from sklearn.decomposition import PCA


import matplotlib.pyplot as plt





class PrincipalComponentAnalysis:


    """ä¸»æˆåˆ†åˆ†æ - åŸºäºå›½é™…æ ‡å‡†çš„å®ç°"""





    def __init__(self, n_components: Optional[int] = None):


        self.n_components = n_components


        self.components_ = None


        self.explained_variance_ratio_ = None


        self.mean_ = None





    def fit(self, X: np.ndarray) -> 'PrincipalComponentAnalysis':


        """


        è®­ç»ƒPCAæ¨¡å‹





        Args:


            X: è¾“å…¥æ•°æ®çŸ©é˜µ (n_samples, n_features)





        Returns:


            self: è®­ç»ƒå¥½çš„æ¨¡å‹


        """


        # ä¸­å¿ƒåŒ–æ•°æ®


        self.mean_ = np.mean(X, axis=0)


        X_centered = X - self.mean_





        # è®¡ç®—åæ–¹å·®çŸ©é˜µ


        cov_matrix = np.cov(X_centered.T)





        # ç‰¹å¾å€¼åˆ†è§£


        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)





        # æŒ‰ç‰¹å¾å€¼é™åºæ’åˆ—


        idx = np.argsort(eigenvalues)[::-1]


        eigenvalues = eigenvalues[idx]


        eigenvectors = eigenvectors[:, idx]





        # é€‰æ‹©ä¸»æˆåˆ†æ•°é‡


        if self.n_components is None:


            self.n_components = X.shape[1]





        self.components_ = eigenvectors[:, :self.n_components]


        self.explained_variance_ratio_ = eigenvalues[:self.n_components] / np.sum(eigenvalues)





        return self





    def transform(self, X: np.ndarray) -> np.ndarray:


        """


        å°†æ•°æ®æŠ•å½±åˆ°ä¸»æˆåˆ†ç©ºé—´





        Args:


            X: è¾“å…¥æ•°æ®çŸ©é˜µ





        Returns:


            æŠ•å½±åçš„æ•°æ®


        """


        X_centered = X - self.mean_


        return X_centered @ self.components_





    def inverse_transform(self, X_transformed: np.ndarray) -> np.ndarray:


        """


        å°†æŠ•å½±æ•°æ®è½¬æ¢å›åŸå§‹ç©ºé—´





        Args:


            X_transformed: æŠ•å½±åçš„æ•°æ®





        Returns:


            åŸå§‹ç©ºé—´çš„æ•°æ®


        """


        return X_transformed @ self.components_.T + self.mean_





def pca_visualization_example():


    """PCAå¯è§†åŒ–ç¤ºä¾‹"""


    # ç”Ÿæˆç¤ºä¾‹æ•°æ®


    np.random.seed(42)


    n_samples = 1000


    n_features = 10





    # ç”Ÿæˆç›¸å…³æ•°æ®


    X = np.random.randn(n_samples, n_features)


    # æ·»åŠ ç›¸å…³æ€§


    X[:, 1] = 0.8 * X[:, 0] + 0.2 * np.random.randn(n_samples)


    X[:, 2] = 0.6 * X[:, 0] + 0.4 * np.random.randn(n_samples)





    # åº”ç”¨PCA


    pca = PrincipalComponentAnalysis(n_components=2)


    X_pca = pca.fit_transform(X)





    # å¯è§†åŒ–


    plt.figure(figsize=(12, 5))





    plt.subplot(1, 2, 1)


    plt.scatter(X[:, 0], X[:, 1], alpha=0.6)


    plt.xlabel('Feature 1')


    plt.ylabel('Feature 2')


    plt.title('Original Data')





    plt.subplot(1, 2, 2)


    plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)


    plt.xlabel('First Principal Component')


    plt.ylabel('Second Principal Component')


    plt.title('PCA Transformed Data')





    plt.tight_layout()


    plt.show()





    # è§£é‡Šæ–¹å·®æ¯”


    print(f"è§£é‡Šæ–¹å·®æ¯”: {pca.explained_variance_ratio_}")


    print(f"ç´¯è®¡è§£é‡Šæ–¹å·®: {np.cumsum(pca.explained_variance_ratio_)}")


```





### 1.3 åº”ç”¨æ¡ˆä¾‹





**æ¡ˆä¾‹ 1.1** (å›¾åƒå‹ç¼©)





```python


def image_compression_pca(image_path: str, n_components: int = 50):


    """


    ä½¿ç”¨PCAè¿›è¡Œå›¾åƒå‹ç¼©





    Args:


        image_path: å›¾åƒè·¯å¾„


        n_components: ä¿ç•™çš„ä¸»æˆåˆ†æ•°é‡


    """


    from PIL import Image


    import numpy as np





    # åŠ è½½å›¾åƒ


    img = Image.open(image_path).convert('L')  # è½¬æ¢ä¸ºç°åº¦å›¾


    img_array = np.array(img)





    # å°†å›¾åƒé‡å¡‘ä¸ºçŸ©é˜µ


    height, width = img_array.shape


    X = img_array.reshape(height, width)





    # åº”ç”¨PCA


    pca = PrincipalComponentAnalysis(n_components=n_components)


    X_compressed = pca.fit_transform(X)


    X_reconstructed = pca.inverse_transform(X_compressed)





    # è®¡ç®—å‹ç¼©ç‡


    original_size = height * width


    compressed_size = n_components * (height + width)


    compression_ratio = compressed_size / original_size





    print(f"å‹ç¼©ç‡: {compression_ratio:.2%}")


    print(f"ä¿ç•™æ–¹å·®: {np.sum(pca.explained_variance_ratio_):.2%}")





    return X_reconstructed, compression_ratio


```





## 2. å¥‡å¼‚å€¼åˆ†è§£ (SVD)





### 2.1 ç†è®ºåŸºç¡€





**å®šä¹‰ 2.1** (å¥‡å¼‚å€¼åˆ†è§£)


è®¾ $A \in \mathbb{R}^{m \times n}$ï¼Œåˆ™å­˜åœ¨æ­£äº¤çŸ©é˜µ $U \in \mathbb{R}^{m \times m}$ å’Œ $V \in \mathbb{R}^{n \times n}$ï¼Œä»¥åŠå¯¹è§’çŸ©é˜µ $\Sigma \in \mathbb{R}^{m \times n}$ï¼Œä½¿å¾—ï¼š


$$A = U \Sigma V^T$$





å…¶ä¸­ $\Sigma$ çš„å¯¹è§’å…ƒç´  $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$ ç§°ä¸ºå¥‡å¼‚å€¼ï¼Œ$r = \text{rank}(A)$ã€‚





**å®šç† 2.1** (SVDçš„æ€§è´¨)





1. $U$ çš„åˆ—å‘é‡æ˜¯ $AA^T$ çš„ç‰¹å¾å‘é‡


2. $V$ çš„åˆ—å‘é‡æ˜¯ $A^TA$ çš„ç‰¹å¾å‘é‡


3. $\sigma_i^2$ æ˜¯ $AA^T$ å’Œ $A^TA$ çš„ç‰¹å¾å€¼





### 2.2 ç®—æ³•å®ç°





```python


def svd_decomposition(A: np.ndarray, k: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:


    """


    å¥‡å¼‚å€¼åˆ†è§£





    Args:


        A: è¾“å…¥çŸ©é˜µ


        k: ä¿ç•™çš„å¥‡å¼‚å€¼æ•°é‡





    Returns:


        U: å·¦å¥‡å¼‚å‘é‡


        S: å¥‡å¼‚å€¼


        Vt: å³å¥‡å¼‚å‘é‡çš„è½¬ç½®


    """


    U, S, Vt = np.linalg.svd(A, full_matrices=False)





    if k is not None:


        U = U[:, :k]


        S = S[:k]


        Vt = Vt[:k, :]





    return U, S, Vt





def svd_approximation(A: np.ndarray, k: int) -> np.ndarray:


    """


    ä½¿ç”¨SVDè¿›è¡ŒçŸ©é˜µè¿‘ä¼¼





    Args:


        A: åŸå§‹çŸ©é˜µ


        k: è¿‘ä¼¼ç§©





    Returns:


        è¿‘ä¼¼çŸ©é˜µ


    """


    U, S, Vt = svd_decomposition(A, k)


    return U @ np.diag(S) @ Vt





def svd_compression_example():


    """SVDå‹ç¼©ç¤ºä¾‹"""


    # ç”Ÿæˆç¤ºä¾‹çŸ©é˜µ


    np.random.seed(42)


    A = np.random.randn(100, 80)





    # ä¸åŒç§©çš„è¿‘ä¼¼


    ranks = [10, 20, 40, 60]





    plt.figure(figsize=(15, 10))





    for i, rank in enumerate(ranks):


        A_approx = svd_approximation(A, rank)


        error = np.linalg.norm(A - A_approx, 'fro') / np.linalg.norm(A, 'fro')





        plt.subplot(2, 2, i+1)


        plt.imshow(A_approx, cmap='viridis')


        plt.title(f'Rank {rank} Approximation\nError: {error:.3f}')


        plt.colorbar()





    plt.tight_layout()


    plt.show()


```





### 2.3 æ¨èç³»ç»Ÿåº”ç”¨





**æ¡ˆä¾‹ 2.1** (ååŒè¿‡æ»¤)





```python


class SVDRecommender:


    """åŸºäºSVDçš„æ¨èç³»ç»Ÿ"""





    def __init__(self, n_factors: int = 50):


        self.n_factors = n_factors


        self.user_factors = None


        self.item_factors = None


        self.user_biases = None


        self.item_biases = None


        self.global_bias = None





    def fit(self, ratings: np.ndarray, user_ids: np.ndarray, item_ids: np.ndarray):


        """


        è®­ç»ƒæ¨èæ¨¡å‹





        Args:


            ratings: è¯„åˆ†æ•°ç»„


            user_ids: ç”¨æˆ·IDæ•°ç»„


            item_ids: ç‰©å“IDæ•°ç»„


        """


        n_users = len(np.unique(user_ids))


        n_items = len(np.unique(item_ids))





        # æ„å»ºè¯„åˆ†çŸ©é˜µ


        rating_matrix = np.zeros((n_users, n_items))


        for rating, user_id, item_id in zip(ratings, user_ids, item_ids):


            rating_matrix[user_id, item_id] = rating





        # è®¡ç®—å…¨å±€åç½®


        self.global_bias = np.mean(ratings)





        # è®¡ç®—ç”¨æˆ·å’Œç‰©å“åç½®


        self.user_biases = np.mean(rating_matrix, axis=1) - self.global_bias


        self.item_biases = np.mean(rating_matrix, axis=0) - self.global_bias





        # ä¸­å¿ƒåŒ–è¯„åˆ†çŸ©é˜µ


        centered_ratings = rating_matrix - self.global_bias


        centered_ratings -= self.user_biases.reshape(-1, 1)


        centered_ratings -= self.item_biases.reshape(1, -1)





        # SVDåˆ†è§£


        U, S, Vt = svd_decomposition(centered_ratings, self.n_factors)





        self.user_factors = U


        self.item_factors = Vt.T





    def predict(self, user_ids: np.ndarray, item_ids: np.ndarray) -> np.ndarray:


        """


        é¢„æµ‹è¯„åˆ†





        Args:


            user_ids: ç”¨æˆ·IDæ•°ç»„


            item_ids: ç‰©å“IDæ•°ç»„





        Returns:


            é¢„æµ‹è¯„åˆ†


        """


        predictions = self.global_bias


        predictions += self.user_biases[user_ids]


        predictions += self.item_biases[item_ids]


        predictions += np.sum(self.user_factors[user_ids] * self.item_factors[item_ids], axis=1)





        return np.clip(predictions, 1, 5)  # é™åˆ¶åœ¨1-5èŒƒå›´å†…


```





## 3. æœ€å°äºŒä¹˜æ³•ä¸çº¿æ€§å›å½’





### 3.1 ç†è®ºåŸºç¡€





**å®šä¹‰ 3.1** (çº¿æ€§å›å½’)


ç»™å®šæ•°æ®ç‚¹ $(x_i, y_i)_{i=1}^n$ï¼Œçº¿æ€§å›å½’æ¨¡å‹ä¸ºï¼š


$$y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} + \epsilon_i$$





å…¶ä¸­ $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ æ˜¯å™ªå£°é¡¹ã€‚





**æœ€å°äºŒä¹˜ä¼°è®¡**:


$$\hat{\beta} = \arg\min_{\beta} \|y - X\beta\|_2^2 = (X^T X)^{-1} X^T y$$





### 3.2 ç®—æ³•å®ç°





```python


class LinearRegression:


    """çº¿æ€§å›å½’ - åŸºäºæœ€å°äºŒä¹˜æ³•"""





    def __init__(self, fit_intercept: bool = True):


        self.fit_intercept = fit_intercept


        self.coef_ = None


        self.intercept_ = None





    def fit(self, X: np.ndarray, y: np.ndarray) -> 'LinearRegression':


        """


        è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹





        Args:


            X: ç‰¹å¾çŸ©é˜µ (n_samples, n_features)


            y: ç›®æ ‡å˜é‡ (n_samples,)





        Returns:


            self: è®­ç»ƒå¥½çš„æ¨¡å‹


        """


        if self.fit_intercept:


            X_augmented = np.column_stack([np.ones(X.shape[0]), X])


        else:


            X_augmented = X





        # æœ€å°äºŒä¹˜è§£


        try:


            # ä½¿ç”¨æ­£è§„æ–¹ç¨‹


            beta = np.linalg.inv(X_augmented.T @ X_augmented) @ X_augmented.T @ y


        except np.linalg.LinAlgError:


            # å¦‚æœçŸ©é˜µå¥‡å¼‚ï¼Œä½¿ç”¨ä¼ªé€†


            beta = np.linalg.pinv(X_augmented) @ y





        if self.fit_intercept:


            self.intercept_ = beta[0]


            self.coef_ = beta[1:]


        else:


            self.intercept_ = 0


            self.coef_ = beta





        return self





    def predict(self, X: np.ndarray) -> np.ndarray:


        """


        é¢„æµ‹





        Args:


            X: ç‰¹å¾çŸ©é˜µ





        Returns:


            é¢„æµ‹å€¼


        """


        return X @ self.coef_ + self.intercept_





def ridge_regression(X: np.ndarray, y: np.ndarray, alpha: float = 1.0) -> np.ndarray:


    """


    å²­å›å½’ (Ridge Regression)





    Args:


        X: ç‰¹å¾çŸ©é˜µ


        y: ç›®æ ‡å˜é‡


        alpha: æ­£åˆ™åŒ–å‚æ•°





    Returns:


        å›å½’ç³»æ•°


    """


    n_features = X.shape[1]


    X_augmented = np.column_stack([np.ones(X.shape[0]), X])





    # å²­å›å½’è§£


    I = np.eye(n_features + 1)


    I[0, 0] = 0  # ä¸æƒ©ç½šæˆªè·é¡¹





    beta = np.linalg.inv(X_augmented.T @ X_augmented + alpha * I) @ X_augmented.T @ y





    return beta





def lasso_regression(X: np.ndarray, y: np.ndarray, alpha: float = 1.0, max_iter: int = 1000) -> np.ndarray:


    """


    Lassoå›å½’ (ä½¿ç”¨åæ ‡ä¸‹é™æ³•)





    Args:


        X: ç‰¹å¾çŸ©é˜µ


        y: ç›®æ ‡å˜é‡


        alpha: æ­£åˆ™åŒ–å‚æ•°


        max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°





    Returns:


        å›å½’ç³»æ•°


    """


    n_samples, n_features = X.shape


    beta = np.zeros(n_features)





    # æ ‡å‡†åŒ–ç‰¹å¾


    X_std = (X - np.mean(X, axis=0)) / np.std(X, axis=0)


    y_centered = y - np.mean(y)





    for _ in range(max_iter):


        beta_old = beta.copy()





        for j in range(n_features):


            # è®¡ç®—æ®‹å·®


            r = y_centered - X_std @ beta + X_std[:, j] * beta[j]





            # è®¡ç®—ç›¸å…³ç³»æ•°


            corr = X_std[:, j] @ r





            # è½¯é˜ˆå€¼


            if corr > alpha:


                beta[j] = (corr - alpha) / (X_std[:, j] @ X_std[:, j])


            elif corr < -alpha:


                beta[j] = (corr + alpha) / (X_std[:, j] @ X_std[:, j])


            else:


                beta[j] = 0





        # æ”¶æ•›æ£€æŸ¥


        if np.linalg.norm(beta - beta_old) < 1e-6:


            break





    return beta


```





### 3.3 æ­£åˆ™åŒ–ä¸ç‰¹å¾é€‰æ‹©





**æ¡ˆä¾‹ 3.1** (æ­£åˆ™åŒ–æ¯”è¾ƒ)





```python


def regularization_comparison():


    """æ¯”è¾ƒä¸åŒæ­£åˆ™åŒ–æ–¹æ³•"""


    np.random.seed(42)





    # ç”Ÿæˆæ•°æ®


    n_samples, n_features = 100, 20


    X = np.random.randn(n_samples, n_features)





    # çœŸå®ç³»æ•°ï¼ˆåªæœ‰å‰5ä¸ªéé›¶ï¼‰


    true_coef = np.zeros(n_features)


    true_coef[:5] = [1.5, -0.8, 2.1, -1.2, 0.9]





    # ç”Ÿæˆç›®æ ‡å˜é‡


    y = X @ true_coef + 0.1 * np.random.randn(n_samples)





    # ä¸åŒæ­£åˆ™åŒ–æ–¹æ³•


    alphas = [0.01, 0.1, 1.0, 10.0]





    plt.figure(figsize=(15, 10))





    for i, alpha in enumerate(alphas):


        # å²­å›å½’


        ridge_coef = ridge_regression(X, y, alpha)[1:]  # å»æ‰æˆªè·





        # Lassoå›å½’


        lasso_coef = lasso_regression(X, y, alpha)





        plt.subplot(2, 2, i+1)


        x_pos = np.arange(n_features)


        width = 0.35





        plt.bar(x_pos - width/2, ridge_coef, width, label='Ridge', alpha=0.7)


        plt.bar(x_pos + width/2, lasso_coef, width, label='Lasso', alpha=0.7)


        plt.plot(x_pos, true_coef, 'ro', label='True', markersize=8)





        plt.xlabel('Feature Index')


        plt.ylabel('Coefficient Value')


        plt.title(f'Regularization Comparison (Î±={alpha})')


        plt.legend()


        plt.grid(True, alpha=0.3)





    plt.tight_layout()


    plt.show()


```





## 4. ç¥ç»ç½‘ç»œä¸­çš„çº¿æ€§ä»£æ•°





### 4.1 å‰å‘ä¼ æ’­





**å®šä¹‰ 4.1** (ç¥ç»ç½‘ç»œå±‚)


ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­å¯ä»¥è¡¨ç¤ºä¸ºï¼š


$$z^{(l+1)} = W^{(l)} a^{(l)} + b^{(l)}$$


$$a^{(l+1)} = \sigma(z^{(l+1)})$$





å…¶ä¸­ $W^{(l)}$ æ˜¯æƒé‡çŸ©é˜µï¼Œ$b^{(l)}$ æ˜¯åç½®å‘é‡ï¼Œ$\sigma$ æ˜¯æ¿€æ´»å‡½æ•°ã€‚





### 4.2 åå‘ä¼ æ’­





**å®šç† 4.1** (åå‘ä¼ æ’­)


æ¢¯åº¦å¯ä»¥é€šè¿‡é“¾å¼æ³•åˆ™è®¡ç®—ï¼š


$$\frac{\partial L}{\partial W^{(l)}} = \frac{\partial L}{\partial z^{(l+1)}} (a^{(l)})^T$$


$$\frac{\partial L}{\partial b^{(l)}} = \frac{\partial L}{\partial z^{(l+1)}}$$





### 4.3 å®ç°ç¤ºä¾‹





```python


class NeuralNetwork:


    """ç®€å•ç¥ç»ç½‘ç»œå®ç°"""





    def __init__(self, layer_sizes: list, activation: str = 'relu'):


        self.layer_sizes = layer_sizes


        self.activation = activation


        self.weights = []


        self.biases = []





        # åˆå§‹åŒ–æƒé‡å’Œåç½®


        for i in range(len(layer_sizes) - 1):


            W = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * 0.01


            b = np.zeros((layer_sizes[i+1], 1))


            self.weights.append(W)


            self.biases.append(b)





    def sigmoid(self, z: np.ndarray) -> np.ndarray:


        """Sigmoidæ¿€æ´»å‡½æ•°"""


        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))





    def sigmoid_derivative(self, z: np.ndarray) -> np.ndarray:


        """Sigmoidå¯¼æ•°"""


        s = self.sigmoid(z)


        return s * (1 - s)





    def relu(self, z: np.ndarray) -> np.ndarray:


        """ReLUæ¿€æ´»å‡½æ•°"""


        return np.maximum(0, z)





    def relu_derivative(self, z: np.ndarray) -> np.ndarray:


        """ReLUå¯¼æ•°"""


        return (z > 0).astype(float)





    def forward(self, X: np.ndarray) -> tuple:


        """


        å‰å‘ä¼ æ’­





        Returns:


            activations: å„å±‚æ¿€æ´»å€¼


            z_values: å„å±‚çº¿æ€§ç»„åˆå€¼


        """


        activations = [X]


        z_values = []





        for i, (W, b) in enumerate(zip(self.weights, self.biases)):


            z = W @ activations[-1] + b


            z_values.append(z)





            if i == len(self.weights) - 1:


                # è¾“å‡ºå±‚ä½¿ç”¨sigmoid


                a = self.sigmoid(z)


            else:


                # éšè—å±‚ä½¿ç”¨ReLU


                a = self.relu(z)





            activations.append(a)





        return activations, z_values





    def backward(self, X: np.ndarray, y: np.ndarray, activations: list, z_values: list) -> tuple:


        """


        åå‘ä¼ æ’­





        Returns:


            weight_gradients: æƒé‡æ¢¯åº¦


            bias_gradients: åç½®æ¢¯åº¦


        """


        m = X.shape[1]


        weight_gradients = []


        bias_gradients = []





        # è¾“å‡ºå±‚è¯¯å·®


        delta = activations[-1] - y





        for i in reversed(range(len(self.weights))):


            # è®¡ç®—æ¢¯åº¦


            dW = delta @ activations[i].T / m


            db = np.sum(delta, axis=1, keepdims=True) / m





            weight_gradients.insert(0, dW)


            bias_gradients.insert(0, db)





            if i > 0:


                # ä¼ æ’­è¯¯å·®åˆ°å‰ä¸€å±‚


                delta = self.weights[i].T @ delta


                if self.activation == 'relu':


                    delta = delta * self.relu_derivative(z_values[i-1])





        return weight_gradients, bias_gradients





    def train(self, X: np.ndarray, y: np.ndarray, learning_rate: float = 0.1, epochs: int = 1000):


        """è®­ç»ƒç¥ç»ç½‘ç»œ"""


        costs = []





        for epoch in range(epochs):


            # å‰å‘ä¼ æ’­


            activations, z_values = self.forward(X)





            # è®¡ç®—æŸå¤±


            cost = -np.mean(y * np.log(activations[-1] + 1e-8) +


                           (1 - y) * np.log(1 - activations[-1] + 1e-8))


            costs.append(cost)





            # åå‘ä¼ æ’­


            weight_gradients, bias_gradients = self.backward(X, y, activations, z_values)





            # æ›´æ–°å‚æ•°


            for i in range(len(self.weights)):


                self.weights[i] -= learning_rate * weight_gradients[i]


                self.biases[i] -= learning_rate * bias_gradients[i]





            if epoch % 100 == 0:


                print(f"Epoch {epoch}, Cost: {cost:.4f}")





        return costs


```





## 5. æ·±åº¦å­¦ä¹ ä¸­çš„çº¿æ€§ä»£æ•°





### 5.1 å·ç§¯ç¥ç»ç½‘ç»œ (CNN)





**å®šä¹‰ 5.1** (å·ç§¯æ“ä½œ)


äºŒç»´å·ç§¯å®šä¹‰ä¸ºï¼š


$$(f * k)(i, j) = \sum_{m} \sum_{n} f(m, n) k(i-m, j-n)$$





**çŸ©é˜µå½¢å¼**:


å·ç§¯å¯ä»¥è¡¨ç¤ºä¸ºToeplitzçŸ©é˜µçš„ä¹˜æ³•ã€‚





```python


def conv2d(X: np.ndarray, kernel: np.ndarray, stride: int = 1, padding: int = 0) -> np.ndarray:


    """


    äºŒç»´å·ç§¯å®ç°





    Args:


        X: è¾“å…¥ç‰¹å¾å›¾ (batch_size, channels, height, width)


        kernel: å·ç§¯æ ¸ (out_channels, in_channels, kernel_height, kernel_width)


        stride: æ­¥é•¿


        padding: å¡«å……





    Returns:


        å·ç§¯ç»“æœ


    """


    batch_size, in_channels, in_height, in_width = X.shape


    out_channels, _, kernel_height, kernel_width = kernel.shape





    # è®¡ç®—è¾“å‡ºå°ºå¯¸


    out_height = (in_height + 2 * padding - kernel_height) // stride + 1


    out_width = (in_width + 2 * padding - kernel_width) // stride + 1





    # å¡«å……


    if padding > 0:


        X_padded = np.pad(X, ((0, 0), (0, 0), (padding, padding), (padding, padding)))


    else:


        X_padded = X





    # åˆå§‹åŒ–è¾“å‡º


    output = np.zeros((batch_size, out_channels, out_height, out_width))





    # æ‰§è¡Œå·ç§¯


    for b in range(batch_size):


        for c_out in range(out_channels):


            for h_out in range(out_height):


                for w_out in range(out_width):


                    h_start = h_out * stride


                    h_end = h_start + kernel_height


                    w_start = w_out * stride


                    w_end = w_start + kernel_width





                    # æå–è¾“å…¥çª—å£


                    window = X_padded[b, :, h_start:h_end, w_start:w_end]





                    # è®¡ç®—å·ç§¯


                    output[b, c_out, h_out, w_out] = np.sum(window * kernel[c_out])





    return output


```





### 5.2 å¾ªç¯ç¥ç»ç½‘ç»œ (RNN)





**å®šä¹‰ 5.2** (RNN)


RNNçš„éšè—çŠ¶æ€æ›´æ–°ä¸ºï¼š


$$h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$





```python


class SimpleRNN:


    """ç®€å•RNNå®ç°"""





    def __init__(self, input_size: int, hidden_size: int, output_size: int):


        self.input_size = input_size


        self.hidden_size = hidden_size


        self.output_size = output_size





        # åˆå§‹åŒ–æƒé‡


        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01


        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01


        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01





        self.b_h = np.zeros((hidden_size, 1))


        self.b_y = np.zeros((output_size, 1))





    def forward(self, X: np.ndarray) -> tuple:


        """


        å‰å‘ä¼ æ’­





        Args:


            X: è¾“å…¥åºåˆ— (seq_length, batch_size, input_size)





        Returns:


            outputs: è¾“å‡ºåºåˆ—


            hidden_states: éšè—çŠ¶æ€åºåˆ—


        """


        seq_length, batch_size, _ = X.shape


        hidden_states = []


        outputs = []





        h = np.zeros((self.hidden_size, batch_size))





        for t in range(seq_length):


            # æ›´æ–°éšè—çŠ¶æ€


            h = np.tanh(self.W_xh @ X[t].T + self.W_hh @ h + self.b_h)


            hidden_states.append(h)





            # è®¡ç®—è¾“å‡º


            y = self.W_hy @ h + self.b_y


            outputs.append(y.T)





        return np.array(outputs), np.array(hidden_states)


```





## 6. ä¼˜åŒ–ç®—æ³•ä¸­çš„çº¿æ€§ä»£æ•°





### 6.1 æ¢¯åº¦ä¸‹é™





**å®šä¹‰ 6.1** (æ¢¯åº¦ä¸‹é™)


å‚æ•°æ›´æ–°è§„åˆ™ï¼š


$$\theta_{t+1} = \theta_t - \alpha \nabla f(\theta_t)$$





### 6.2 éšæœºæ¢¯åº¦ä¸‹é™ (SGD)





```python


def sgd_optimizer(params: list, gradients: list, learning_rate: float = 0.01):


    """


    éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨


    """


    for param, grad in zip(params, gradients):


        param -= learning_rate * grad





def momentum_sgd(params: list, gradients: list, velocities: list,


                learning_rate: float = 0.01, momentum: float = 0.9):


    """


    å¸¦åŠ¨é‡çš„SGD


    """


    for i, (param, grad, velocity) in enumerate(zip(params, gradients, velocities)):


        velocities[i] = momentum * velocity + learning_rate * grad


        param -= velocities[i]


```





### 6.3 Adamä¼˜åŒ–å™¨





```python


class AdamOptimizer:


    """Adamä¼˜åŒ–å™¨"""





    def __init__(self, learning_rate: float = 0.001, beta1: float = 0.9,


                 beta2: float = 0.999, epsilon: float = 1e-8):


        self.learning_rate = learning_rate


        self.beta1 = beta1


        self.beta2 = beta2


        self.epsilon = epsilon


        self.m = None


        self.v = None


        self.t = 0





    def update(self, params: list, gradients: list):


        """æ›´æ–°å‚æ•°"""


        if self.m is None:


            self.m = [np.zeros_like(param) for param in params]


            self.v = [np.zeros_like(param) for param in params]





        self.t += 1





        for i, (param, grad) in enumerate(zip(params, gradients)):


            # æ›´æ–°ä¸€é˜¶çŸ©ä¼°è®¡


            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad





            # æ›´æ–°äºŒé˜¶çŸ©ä¼°è®¡


            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad ** 2)





            # åå·®ä¿®æ­£


            m_hat = self.m[i] / (1 - self.beta1 ** self.t)


            v_hat = self.v[i] / (1 - self.beta2 ** self.t)





            # æ›´æ–°å‚æ•°


            param -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)


```





## 7. å®é™…åº”ç”¨æ¡ˆä¾‹





### 7.1 å›¾åƒåˆ†ç±»





```python


def image_classification_example():


    """å›¾åƒåˆ†ç±»ç¤ºä¾‹"""


    from sklearn.datasets import fetch_openml


    from sklearn.model_selection import train_test_split


    from sklearn.preprocessing import StandardScaler





    # åŠ è½½MNISTæ•°æ®é›†


    mnist = fetch_openml('mnist_784', version=1, as_frame=False)


    X, y = mnist.data, mnist.target





    # æ•°æ®é¢„å¤„ç†


    X = X / 255.0  # å½’ä¸€åŒ–


    y = (y == '1').astype(int)  # äºŒåˆ†ç±»ï¼šæ˜¯å¦ä¸ºæ•°å­—1





    # åˆ†å‰²æ•°æ®


    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)





    # ä½¿ç”¨PCAé™ç»´


    pca = PrincipalComponentAnalysis(n_components=100)


    X_train_pca = pca.fit_transform(X_train)


    X_test_pca = pca.transform(X_test)





    # è®­ç»ƒçº¿æ€§å›å½’


    lr = LinearRegression()


    lr.fit(X_train_pca, y_train)





    # é¢„æµ‹


    y_pred = lr.predict(X_test_pca)


    y_pred_binary = (y_pred > 0.5).astype(int)





    # è¯„ä¼°


    accuracy = np.mean(y_pred_binary == y_test)


    print(f"åˆ†ç±»å‡†ç¡®ç‡: {accuracy:.4f}")





    return accuracy


```





### 7.2 æ¨èç³»ç»Ÿ





```python


def recommendation_system_example():


    """æ¨èç³»ç»Ÿç¤ºä¾‹"""


    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®


    np.random.seed(42)


    n_users, n_items = 100, 50


    n_ratings = 1000





    # ç”Ÿæˆéšæœºè¯„åˆ†


    user_ids = np.random.randint(0, n_users, n_ratings)


    item_ids = np.random.randint(0, n_items, n_ratings)


    ratings = np.random.randint(1, 6, n_ratings)





    # è®­ç»ƒSVDæ¨èç³»ç»Ÿ


    recommender = SVDRecommender(n_factors=10)


    recommender.fit(ratings, user_ids, item_ids)





    # ä¸ºç‰¹å®šç”¨æˆ·æ¨è


    user_id = 0


    user_ratings = recommender.predict(np.full(n_items, user_id), np.arange(n_items))





    # è·å–top-5æ¨è


    top_items = np.argsort(user_ratings)[-5:][::-1]


    print(f"ç”¨æˆ· {user_id} çš„top-5æ¨èç‰©å“: {top_items}")


    print(f"é¢„æµ‹è¯„åˆ†: {user_ratings[top_items]}")





    return top_items, user_ratings[top_items]


```





## 8. æ€§èƒ½ä¼˜åŒ–ä¸æ•°å€¼ç¨³å®šæ€§





### 8.1 æ•°å€¼ç¨³å®šæ€§





```python


def numerical_stability_example():


    """æ•°å€¼ç¨³å®šæ€§ç¤ºä¾‹"""


    # ç—…æ€çŸ©é˜µç¤ºä¾‹


    n = 10


    A = np.random.randn(n, n)


    # ä½¿çŸ©é˜µæ¥è¿‘å¥‡å¼‚


    A[:, -1] = A[:, 0] + 1e-10 * np.random.randn(n)





    b = np.random.randn(n)





    # ç›´æ¥æ±‚è§£


    try:


        x_direct = np.linalg.solve(A, b)


        print("ç›´æ¥æ±‚è§£æˆåŠŸ")


    except np.linalg.LinAlgError:


        print("ç›´æ¥æ±‚è§£å¤±è´¥")





    # ä½¿ç”¨SVDæ±‚è§£


    U, S, Vt = np.linalg.svd(A)


    # è®¾ç½®é˜ˆå€¼ï¼Œè¿‡æ»¤å°çš„å¥‡å¼‚å€¼


    threshold = 1e-10


    S_inv = np.where(S > threshold, 1/S, 0)


    x_svd = Vt.T @ np.diag(S_inv) @ U.T @ b





    print(f"SVDæ±‚è§£ç»“æœ: {x_svd}")





    return x_svd


```





### 8.2 å¹¶è¡Œè®¡ç®—





```python


def parallel_matrix_operations():


    """å¹¶è¡ŒçŸ©é˜µè¿ç®—ç¤ºä¾‹"""


    import multiprocessing as mp


    from functools import partial





    def matrix_multiply_chunk(args):


        """çŸ©é˜µä¹˜æ³•çš„ä¸€ä¸ªå—"""


        A_chunk, B = args


        return A_chunk @ B





    def parallel_matrix_multiply(A: np.ndarray, B: np.ndarray, n_jobs: int = -1):


        """å¹¶è¡ŒçŸ©é˜µä¹˜æ³•"""


        if n_jobs == -1:


            n_jobs = mp.cpu_count()





        # åˆ†å‰²çŸ©é˜µA


        chunk_size = A.shape[0] // n_jobs


        A_chunks = [A[i:i+chunk_size] for i in range(0, A.shape[0], chunk_size)]





        # å¹¶è¡Œè®¡ç®—


        with mp.Pool(n_jobs) as pool:


            results = pool.map(matrix_multiply_chunk, [(chunk, B) for chunk in A_chunks])





        return np.vstack(results)





    # æµ‹è¯•


    A = np.random.randn(1000, 500)


    B = np.random.randn(500, 800)





    # ä¸²è¡Œè®¡ç®—


    C_serial = A @ B





    # å¹¶è¡Œè®¡ç®—


    C_parallel = parallel_matrix_multiply(A, B)





    # éªŒè¯ç»“æœ


    error = np.linalg.norm(C_serial - C_parallel)


    print(f"å¹¶è¡Œè®¡ç®—è¯¯å·®: {error}")





    return C_parallel


```





## 9. å‰æ²¿å‘å±•ä¸åº”ç”¨





### 9.1 æ³¨æ„åŠ›æœºåˆ¶





**å®šä¹‰ 9.1** (æ³¨æ„åŠ›æœºåˆ¶)


æ³¨æ„åŠ›æƒé‡è®¡ç®—ï¼š


$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$





```python


def attention_mechanism(Q: np.ndarray, K: np.ndarray, V: np.ndarray,


                       mask: Optional[np.ndarray] = None) -> np.ndarray:


    """


    æ³¨æ„åŠ›æœºåˆ¶å®ç°





    Args:


        Q: æŸ¥è¯¢çŸ©é˜µ (seq_len, d_k)


        K: é”®çŸ©é˜µ (seq_len, d_k)


        V: å€¼çŸ©é˜µ (seq_len, d_v)


        mask: æ©ç çŸ©é˜µ





    Returns:


        æ³¨æ„åŠ›è¾“å‡º


    """


    d_k = Q.shape[1]





    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°


    scores = Q @ K.T / np.sqrt(d_k)





    # åº”ç”¨æ©ç ï¼ˆå¦‚æœæœ‰ï¼‰


    if mask is not None:


        scores = scores + mask





    # è®¡ç®—æ³¨æ„åŠ›æƒé‡


    attention_weights = softmax(scores, axis=-1)





    # è®¡ç®—è¾“å‡º


    output = attention_weights @ V





    return output





def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:


    """Softmaxå‡½æ•°"""


    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))


    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)


```





### 9.2 å›¾ç¥ç»ç½‘ç»œ





```python


class GraphConvolution:


    """å›¾å·ç§¯å±‚"""





    def __init__(self, input_dim: int, output_dim: int):


        self.input_dim = input_dim


        self.output_dim = output_dim


        self.W = np.random.randn(input_dim, output_dim) * 0.01





    def forward(self, X: np.ndarray, A: np.ndarray) -> np.ndarray:


        """


        å›¾å·ç§¯å‰å‘ä¼ æ’­





        Args:


            X: èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ (n_nodes, input_dim)


            A: é‚»æ¥çŸ©é˜µ (n_nodes, n_nodes)





        Returns:


            æ›´æ–°åçš„èŠ‚ç‚¹ç‰¹å¾


        """


        # å›¾å·ç§¯å…¬å¼: H = Ïƒ(AXW)


        H = A @ X @ self.W


        return np.tanh(H)  # æ¿€æ´»å‡½æ•°


```





## 10. æ€»ç»“ä¸å±•æœ›





### 10.1 æ ¸å¿ƒè´¡çŒ®





1. **ç†è®ºåŸºç¡€**: å®Œæ•´çš„çº¿æ€§ä»£æ•°åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨ç†è®º


2. **ç®—æ³•å®ç°**: å¯è¿è¡Œçš„Pythonä»£ç å®ç°


3. **åº”ç”¨æ¡ˆä¾‹**: ä¸°å¯Œçš„å®é™…åº”ç”¨ç¤ºä¾‹


4. **å‰æ²¿å‘å±•**: æœ€æ–°çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯





### 10.2 æœªæ¥å‘å±•æ–¹å‘





1. **å¤§è§„æ¨¡ä¼˜åŒ–**: åˆ†å¸ƒå¼è®¡ç®—å’ŒGPUåŠ é€Ÿ


2. **è‡ªåŠ¨å¾®åˆ†**: æ›´é«˜æ•ˆçš„æ¢¯åº¦è®¡ç®—


3. **æ¨¡å‹å‹ç¼©**: ç½‘ç»œå‰ªæå’Œé‡åŒ–


4. **å¯è§£é‡Šæ€§**: æ¨¡å‹è§£é‡Šå’Œå¯è§†åŒ–





### 10.3 æ•™è‚²ä»·å€¼





1. **ç†è®ºä¸å®è·µç»“åˆ**: ä»æ•°å­¦åŸç†åˆ°ä»£ç å®ç°


2. **é€’è¿›å¼å­¦ä¹ **: ä»åŸºç¡€åˆ°å‰æ²¿çš„å®Œæ•´è·¯å¾„


3. **å®é™…åº”ç”¨å¯¼å‘**: å¼ºè°ƒå®é™…é—®é¢˜çš„è§£å†³


4. **å›½é™…åŒ–æ ‡å‡†**: å¯¹æ ‡å›½é™…ä¸€æµå¤§å­¦æ ‡å‡†





## å‚è€ƒæ–‡çŒ®





### å›½é™…æ ‡å‡†æ–‡çŒ®





1. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.


2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.


3. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springer.





### å›½é™…å¤§å­¦æ ‡å‡†





1. MIT 6.036 Introduction to Machine Learning


2. Stanford CS229 Machine Learning


3. Cambridge Part II Machine Learning and Bayesian Inference


4. Oxford Machine Learning Course





### å‰æ²¿å‘å±•





1. Vaswani, A., et al. (2017). Attention is all you need. NIPS.


2. Kipf, T. N., & Welling, M. (2017). Semi-supervised classification with graph convolutional networks. ICLR.


3. Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. ICLR.





---





**æ–‡æ¡£ç‰ˆæœ¬**: 1.0


**æœ€åæ›´æ–°**: 2025å¹´1æœˆ


**ç»´æŠ¤è€…**: FormalMathé¡¹ç›®ç»„


**è®¸å¯è¯**: MIT License
