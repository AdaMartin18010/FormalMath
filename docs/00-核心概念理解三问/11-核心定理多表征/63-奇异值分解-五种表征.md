# 奇异值分解 - 五种表征

**创建日期**: 2025年12月1日
**领域**: 线性代数
**难度**: L1

---

## 元信息

| 属性 | 内容 |
|------|------|
| **定理** | 奇异值分解 (SVD) |
| **领域** | 线性代数 |
| **发现者** | Beltrami, Jordan (19世纪) |
| **前置知识** | 矩阵、正交矩阵 |

---

## 一、符号表征（形式化）

### 1.1 严格陈述

**奇异值分解定理**：设 $A$ 是 $m \times n$ 实矩阵（或复矩阵），则存在分解：

$$A = U \Sigma V^T$$

其中：
- $U$ 是 $m \times m$ 正交矩阵（或酉矩阵）
- $V$ 是 $n \times n$ 正交矩阵（或酉矩阵）
- $\Sigma$ 是 $m \times n$ 对角矩阵，对角元素 $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$（$r = \operatorname{rank}(A)$），其余为0

**奇异值**：$\sigma_i = \sqrt{\lambda_i(A^T A)}$，其中 $\lambda_i$ 是 $A^T A$ 的特征值。

### 1.2 紧凑形式

如果只保留非零奇异值：

$$A = U_r \Sigma_r V_r^T$$

其中 $U_r$ 是 $m \times r$，$\Sigma_r$ 是 $r \times r$，$V_r$ 是 $n \times r$。

### 1.3 形式化表述

$$\forall A \in \mathbb{R}^{m \times n}, \exists U \in O(m), V \in O(n), \Sigma \in \mathbb{R}^{m \times n}: A = U \Sigma V^T$$

---

## 二、几何表征（可视化）

### 2.1 线性变换的分解

```text
A的作用分解：

    输入空间         输出空间
        │              │
    V^T │              │ U
        ↓              ↓
    旋转到      →   旋转到
    标准基           标准基
        │              │
        ↓    Σ         ↓
    沿坐标轴缩放
        │
        ↓
    
A = 旋转 → 缩放 → 旋转
```

### 2.2 几何意义

```text
单位球面的变换：

    单位球面
        │
        │ A
        ↓
    椭球面
    
椭球面的半轴 = 奇异值
方向 = U的列向量
```

### 2.3 低秩近似

```text
低秩近似：

    A ≈ A_k = U_k Σ_k V_k^T
    
    只保留前k个奇异值
    得到最优k秩近似
```

---

## 三、直觉表征（类比）

### 3.1 物理类比

> **SVD**：任何线性变换 = 旋转 + 拉伸 + 旋转

**类比1：变形**

- 线性变换 = 物体的变形
- 旋转 = 改变方向
- 缩放 = 拉伸/压缩
- SVD = 将任意变形分解为旋转+缩放+旋转

**类比2：图像处理**

- 图像 = 矩阵
- SVD = 图像的主成分
- 低秩近似 = 图像压缩

### 3.2 计算类比

**类比**：SVD类似于：
- **主成分分析**（PCA）：找到主要方向
- **因子分解**：将矩阵分解为简单因子
- **压缩**：用较少信息表示矩阵

---

## 四、计算表征（算法）

### 4.1 基本SVD

```python
import numpy as np

def svd_decomposition(A):
    """
    计算矩阵A的SVD分解
    
    参数:
        A: m×n 矩阵
    
    返回:
        U: m×m 正交矩阵
        Sigma: m×n 对角矩阵
        Vt: n×n 正交矩阵（转置）
    """
    U, s, Vt = np.linalg.svd(A, full_matrices=True)
    
    # 构造Sigma矩阵
    Sigma = np.zeros_like(A, dtype=float)
    min_dim = min(A.shape)
    Sigma[:min_dim, :min_dim] = np.diag(s)
    
    return U, Sigma, Vt

# 验证：A = U Σ V^T
A = np.array([[1, 2], [3, 4], [5, 6]])
U, Sigma, Vt = svd_decomposition(A)
A_reconstructed = U @ Sigma @ Vt
print(f"重构误差: {np.linalg.norm(A - A_reconstructed):.2e}")
```

### 4.2 低秩近似

```python
def low_rank_approximation(A, k):
    """
    计算A的k秩近似
    
    参数:
        A: 矩阵
        k: 秩
    
    返回:
        A_k: k秩近似矩阵
        error: 近似误差
    """
    U, s, Vt = np.linalg.svd(A, full_matrices=False)
    
    # 只保留前k个奇异值
    U_k = U[:, :k]
    s_k = s[:k]
    Vt_k = Vt[:k, :]
    
    # 重构
    A_k = U_k @ np.diag(s_k) @ Vt_k
    
    # 计算误差（Frobenius范数）
    error = np.linalg.norm(A - A_k, 'fro')
    
    return A_k, error

# 例子：图像压缩
def compress_image(image_matrix, k):
    """
    使用SVD压缩图像
    
    参数:
        image_matrix: 图像矩阵
        k: 保留的奇异值个数
    
    返回:
        compressed: 压缩后的图像
        compression_ratio: 压缩比
    """
    A_k, error = low_rank_approximation(image_matrix, k)
    
    # 压缩比 = 原始存储 / 压缩后存储
    original_size = image_matrix.size
    compressed_size = k * (image_matrix.shape[0] + image_matrix.shape[1] + 1)
    compression_ratio = original_size / compressed_size
    
    return A_k, compression_ratio, error
```

### 4.3 应用：主成分分析

```python
def pca_via_svd(data):
    """
    使用SVD进行主成分分析
    
    参数:
        data: 数据矩阵（每行一个样本）
    
    返回:
        principal_components: 主成分
        explained_variance: 解释的方差
    """
    # 中心化
    mean = np.mean(data, axis=0)
    data_centered = data - mean
    
    # SVD
    U, s, Vt = np.linalg.svd(data_centered, full_matrices=False)
    
    # 主成分 = Vt的行
    principal_components = Vt
    
    # 解释的方差比例
    explained_variance = s**2 / np.sum(s**2)
    
    return principal_components, explained_variance

# 例子
data = np.random.randn(100, 10)  # 100个样本，10个特征
PCs, variance = pca_via_svd(data)
print(f"前3个主成分解释的方差: {np.sum(variance[:3]):.2%}")
```

---

## 五、范畴表征（抽象）

### 5.1 矩阵的标准形

**SVD**是矩阵在**正交群作用**下的标准形：

- **等价关系**：$A \sim B$ 如果存在正交矩阵 $U, V$ 使得 $A = UBV$
- **标准形**：对角矩阵（奇异值）
- **不变量**：奇异值（在等价关系下不变）

### 5.2 算子理论

在**算子理论**中：

- SVD对应算子的极分解
- 奇异值 = 算子的模
- 这是泛函分析的基础

### 5.3 优化视角

**SVD**解决优化问题：

- **低秩近似**：$\min_{\operatorname{rank}(B) \leq k} \|A - B\|_F$
- **最优解**：$B = U_k \Sigma_k V_k^T$
- 这是Eckart-Young定理

---

## 六、应用实例

### 6.1 经典例子

**例子1**：$A = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}$

- 奇异值：$\sigma_1 \approx 5.465$，$\sigma_2 \approx 0.366$
- $A$ 的作用：旋转 → 缩放 → 旋转

**例子2**：图像压缩

- 图像 = $m \times n$ 矩阵
- 使用前 $k$ 个奇异值
- 压缩比：$\frac{mn}{k(m+n+1)}$

**例子3**：推荐系统

- 用户-物品矩阵
- SVD找到潜在因子
- 用于预测评分

### 6.2 数值计算

**例子4**：求解最小二乘

- $Ax = b$ 的最小二乘解
- 使用SVD：$x = V \Sigma^{-1} U^T b$
- 数值稳定

**例子5**：矩阵条件数

- 条件数 = $\frac{\sigma_1}{\sigma_r}$
- 衡量矩阵的数值稳定性

---

## 七、历史背景

### 7.1 发现历史

- **19世纪**：Beltrami 和 Jordan 独立发现
- **20世纪**：成为数值计算的基础
- **现代**：广泛应用于数据科学

### 7.2 现代意义

SVD是：
- 数值线性代数的基础
- 数据科学的核心工具
- 机器学习的重要算法

---

## 八、证明思路

### 8.1 标准证明

**证明**：

1. **构造**：$A^T A$ 是对称半正定矩阵
2. **特征分解**：$A^T A = V \Lambda V^T$
3. **奇异值**：$\sigma_i = \sqrt{\lambda_i}$
4. **构造U**：$U_i = \frac{1}{\sigma_i} A V_i$
5. **验证**：$A = U \Sigma V^T$

### 8.2 几何证明

**证明思路**：

- 考虑 $A$ 将单位球面映射到椭球面
- 椭球面的主轴方向 = $U$ 的列
- 半轴长度 = 奇异值

---

## 九、推广与变体

### 9.1 广义SVD

对于矩阵对 $(A, B)$，有广义SVD。

### 9.2 张量SVD

对于高阶张量，有类似的分解（CP分解、Tucker分解）。

### 9.3 随机SVD

对于大规模矩阵，有随机SVD算法。

---

**状态**: ✅ 完成（已深化）
**字数**: 约2,400字
**数学公式数**: 8个
**例子数**: 6个
**最后更新**: 2026年01月02日
