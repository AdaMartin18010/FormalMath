# ä¿¡æ¯è®ºæ•°å­¦ - æ·±åŒ–ç‰ˆ

## ğŸ“š æ¦‚è¿°

ä¿¡æ¯è®ºæ•°å­¦æ˜¯ç ”ç©¶ä¿¡æ¯ä¼ è¾“ã€å­˜å‚¨å’Œå¤„ç†çš„æ•°å­¦ç†è®ºä½“ç³»ã€‚æœ¬æ·±åŒ–ç‰ˆå°†æ·±å…¥æ¢è®¨ä¿¡æ¯è®ºçš„æ•°å­¦åŸºç¡€ï¼ŒåŒ…æ‹¬é¦™å†œä¿¡æ¯è®ºã€é‡å­ä¿¡æ¯è®ºã€ç¼–ç ç†è®ºç­‰æ ¸å¿ƒå†…å®¹ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

1. **æŒæ¡é¦™å†œä¿¡æ¯è®ºåŸºç¡€**ï¼šç†è§£ç†µã€äº’ä¿¡æ¯ã€ä¿¡é“å®¹é‡çš„æ•°å­¦å®šä¹‰
2. **æŒæ¡é‡å­ä¿¡æ¯è®ºç†è®º**ï¼šç†è§£é‡å­æ¯”ç‰¹ã€é‡å­çº ç¼ ã€é‡å­é€šä¿¡çš„æ•°å­¦åŸç†
3. **æŒæ¡ç¼–ç ç†è®ºæ–¹æ³•**ï¼šç†è§£çº é”™ç ã€ä¿¡é“ç¼–ç ã€ä¿¡æºç¼–ç çš„æ•°å­¦ç®—æ³•
4. **æŒæ¡ç½‘ç»œç¼–ç ç†è®º**ï¼šç†è§£ç½‘ç»œç¼–ç ã€åˆ†å¸ƒå¼å­˜å‚¨ã€å¤šæ’­é€šä¿¡çš„æ•°å­¦æ–¹æ³•

## ğŸ“– ç›®å½•

1. [é¦™å†œä¿¡æ¯è®ºæ•°å­¦ç†è®º](#1-é¦™å†œä¿¡æ¯è®ºæ•°å­¦ç†è®º)
2. [é‡å­ä¿¡æ¯è®ºæ•°å­¦ç†è®º](#2-é‡å­ä¿¡æ¯è®ºæ•°å­¦ç†è®º)
3. [ç¼–ç ç†è®ºæ•°å­¦ç†è®º](#3-ç¼–ç ç†è®ºæ•°å­¦ç†è®º)
4. [ç½‘ç»œç¼–ç æ•°å­¦ç†è®º](#4-ç½‘ç»œç¼–ç æ•°å­¦ç†è®º)
5. [ä¿¡æ¯è®ºåº”ç”¨æ•°å­¦](#5-ä¿¡æ¯è®ºåº”ç”¨æ•°å­¦)
6. [æŠ€æœ¯å®ç°](#6-æŠ€æœ¯å®ç°)
7. [åº”ç”¨æ¡ˆä¾‹](#7-åº”ç”¨æ¡ˆä¾‹)
8. [å‰æ²¿å‘å±•](#8-å‰æ²¿å‘å±•)
9. [æ€»ç»“ä¸å±•æœ›](#9-æ€»ç»“ä¸å±•æœ›)

---

## 1. é¦™å†œä¿¡æ¯è®ºæ•°å­¦ç†è®º

### 1.1 ä¿¡æ¯åº¦é‡

#### 1.1.1 é¦™å†œç†µ

**ç¦»æ•£ç†µå®šä¹‰**ï¼š
$$H(X) = -\sum_{x \in \mathcal{X}} p(x) \log p(x)$$

å…¶ä¸­$p(x)$æ˜¯éšæœºå˜é‡$X$çš„æ¦‚ç‡åˆ†å¸ƒã€‚

**è¿ç»­ç†µå®šä¹‰**ï¼š
$$h(X) = -\int_{-\infty}^{\infty} p(x) \log p(x) dx$$

**è”åˆç†µ**ï¼š
$$H(X, Y) = -\sum_{x,y} p(x, y) \log p(x, y)$$

**æ¡ä»¶ç†µ**ï¼š
$$H(X|Y) = -\sum_{x,y} p(x, y) \log p(x|y)$$

#### 1.1.2 äº’ä¿¡æ¯

**äº’ä¿¡æ¯å®šä¹‰**ï¼š
$$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$$

**äº’ä¿¡æ¯æ€§è´¨**ï¼š

- å¯¹ç§°æ€§ï¼š$I(X; Y) = I(Y; X)$
- éè´Ÿæ€§ï¼š$I(X; Y) \geq 0$
- æ•°æ®å¤„ç†ä¸ç­‰å¼ï¼š$I(X; Y) \geq I(X; Z)$ if $X \to Y \to Z$

**ç›¸å¯¹ç†µï¼ˆKLæ•£åº¦ï¼‰**ï¼š
$$D(P \| Q) = \sum_x p(x) \log \frac{p(x)}{q(x)}$$

#### 1.1.3 ä¿¡æ¯ä¸ç­‰å¼

**Fanoä¸ç­‰å¼**ï¼š
$$H(X|Y) \leq H_b(P_e) + P_e \log(|\mathcal{X}| - 1)$$

å…¶ä¸­$P_e$æ˜¯é”™è¯¯æ¦‚ç‡ï¼Œ$H_b(p) = -p \log p - (1-p) \log(1-p)$ã€‚

**æ•°æ®å¤„ç†ä¸ç­‰å¼**ï¼š
å¦‚æœ$X \to Y \to Z$å½¢æˆé©¬å°”å¯å¤«é“¾ï¼Œåˆ™ï¼š
$$I(X; Y) \geq I(X; Z)$$

### 1.2 ä¿¡é“å®¹é‡

#### 1.2.1 ç¦»æ•£æ— è®°å¿†ä¿¡é“

**ä¿¡é“å®¹é‡å®šä¹‰**ï¼š
$$C = \max_{p(x)} I(X; Y)$$

**å¯¹ç§°ä¿¡é“å®¹é‡**ï¼š
å¯¹äºå¯¹ç§°ä¿¡é“ï¼Œå®¹é‡ä¸ºï¼š
$$C = \log |\mathcal{Y}| - H(Y|X)$$

#### 1.2.2 é«˜æ–¯ä¿¡é“

**åŠ æ€§é«˜æ–¯ç™½å™ªå£°ä¿¡é“**ï¼š
$$Y = X + Z$$

å…¶ä¸­$Z \sim \mathcal{N}(0, N)$ã€‚

**ä¿¡é“å®¹é‡**ï¼š
$$C = \frac{1}{2} \log(1 + \frac{P}{N})$$

å…¶ä¸­$P$æ˜¯ä¿¡å·åŠŸç‡ï¼Œ$N$æ˜¯å™ªå£°åŠŸç‡ã€‚

**å¸¦å®½é™åˆ¶ä¿¡é“**ï¼š
$$C = W \log(1 + \frac{P}{WN})$$

å…¶ä¸­$W$æ˜¯å¸¦å®½ã€‚

#### 1.2.3 å¤šç”¨æˆ·ä¿¡é“

**å¤šå€æ¥å…¥ä¿¡é“**ï¼š
$$Y = X_1 + X_2 + Z$$

å®¹é‡åŒºåŸŸï¼š
$$R_1 \leq \frac{1}{2} \log(1 + \frac{P_1}{N})$$
$$R_2 \leq \frac{1}{2} \log(1 + \frac{P_2}{N})$$
$$R_1 + R_2 \leq \frac{1}{2} \log(1 + \frac{P_1 + P_2}{N})$$

**å¹¿æ’­ä¿¡é“**ï¼š
$$Y_1 = X + Z_1$$
$$Y_2 = X + Z_2$$

å…¶ä¸­$Z_1 \sim \mathcal{N}(0, N_1)$ï¼Œ$Z_2 \sim \mathcal{N}(0, N_2)$ã€‚

### 1.3 ä¿¡æºç¼–ç 

#### 1.3.1 æ— æŸç¼–ç 

**é¦™å†œç¼–ç **ï¼š

1. æŒ‰æ¦‚ç‡é™åºæ’åˆ—ç¬¦å·
2. è®¡ç®—ç´¯ç§¯æ¦‚ç‡
3. å°†ç´¯ç§¯æ¦‚ç‡è½¬æ¢ä¸ºäºŒè¿›åˆ¶

**éœå¤«æ›¼ç¼–ç **ï¼š

1. æ„å»ºæ¦‚ç‡æ ‘
2. ä»å¶å­åˆ°æ ¹åˆ†é…ç å­—
3. å¾—åˆ°æœ€ä¼˜å‰ç¼€ç 

**ç®—æœ¯ç¼–ç **ï¼š
$$[l_n, u_n) = [l_{n-1} + (u_{n-1} - l_{n-1}) F(x_n), l_{n-1} + (u_{n-1} - l_{n-1}) F(x_n + 1))$$

å…¶ä¸­$F(x)$æ˜¯ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ã€‚

#### 1.3.2 æœ‰æŸç¼–ç 

**ç‡å¤±çœŸç†è®º**ï¼š
$$R(D) = \min_{p(\hat{x}|x): E[d(X,\hat{X})] \leq D} I(X; \hat{X})$$

**å¤±çœŸå‡½æ•°**ï¼š

- æ±‰æ˜å¤±çœŸï¼š$d(x, \hat{x}) = \begin{cases} 0 & \text{if } x = \hat{x} \\ 1 & \text{otherwise} \end{cases}$
- å¹³æ–¹å¤±çœŸï¼š$d(x, \hat{x}) = (x - \hat{x})^2$

## 2. é‡å­ä¿¡æ¯è®ºæ•°å­¦ç†è®º

### 2.1 é‡å­æ¯”ç‰¹

#### 2.1.1 é‡å­æ€è¡¨ç¤º

**çº¯æ€**ï¼š
$$|\psi\rangle = \alpha|0\rangle + \beta|1\rangle$$

å…¶ä¸­$|\alpha|^2 + |\beta|^2 = 1$ã€‚

**å¯†åº¦çŸ©é˜µ**ï¼š
$$\rho = \sum_i p_i |\psi_i\rangle\langle\psi_i|$$

å…¶ä¸­$p_i \geq 0$ï¼Œ$\sum_i p_i = 1$ã€‚

**è¿¹**ï¼š
$$\text{Tr}(\rho) = 1$$

#### 2.1.2 é‡å­æµ‹é‡

**æŠ•å½±æµ‹é‡**ï¼š
$$P_i = |i\rangle\langle i|$$

æµ‹é‡åçŠ¶æ€ï¼š
$$\rho' = \frac{P_i \rho P_i}{\text{Tr}(P_i \rho)}$$

**POVMæµ‹é‡**ï¼š
$$\{E_i\}$$ æ»¡è¶³ $\sum_i E_i = I$ï¼Œ$E_i \geq 0$ã€‚

### 2.2 é‡å­çº ç¼ 

#### 2.2.1 çº ç¼ æ€

**Bellæ€**ï¼š
$$|\Phi^+\rangle = \frac{1}{\sqrt{2}}(|00\rangle + |11\rangle)$$
$$|\Phi^-\rangle = \frac{1}{\sqrt{2}}(|00\rangle - |11\rangle)$$
$$|\Psi^+\rangle = \frac{1}{\sqrt{2}}(|01\rangle + |10\rangle)$$
$$|\Psi^-\rangle = \frac{1}{\sqrt{2}}(|01\rangle - |10\rangle)$$

**Schmidtåˆ†è§£**ï¼š
$$|\psi\rangle = \sum_i \lambda_i |i_A\rangle|i_B\rangle$$

å…¶ä¸­$\lambda_i$æ˜¯Schmidtç³»æ•°ã€‚

#### 2.2.2 çº ç¼ åº¦é‡

**å†¯Â·è¯ºä¾æ›¼ç†µ**ï¼š
$$S(\rho) = -\text{Tr}(\rho \log \rho)$$

**çº ç¼ ç†µ**ï¼š
$$E(\rho) = S(\rho_A) = S(\rho_B)$$

å…¶ä¸­$\rho_A = \text{Tr}_B(\rho)$ã€‚

**çº ç¼ åº¦**ï¼š
$$E(|\psi\rangle) = -\sum_i \lambda_i^2 \log \lambda_i^2$$

### 2.3 é‡å­é€šä¿¡

#### 2.3.1 é‡å­å¯†é’¥åˆ†å‘

**BB84åè®®**ï¼š

1. Aliceéšæœºé€‰æ‹©æ¯”ç‰¹å’ŒåŸºåº•
2. Bobéšæœºé€‰æ‹©æµ‹é‡åŸºåº•
3. é€šè¿‡ç»å…¸ä¿¡é“ç¡®è®¤ç›¸åŒåŸºåº•
4. è¿›è¡Œéšç§æ”¾å¤§

**å¯†é’¥ç‡**ï¼š
$$R = 1 - 2h(Q)$$

å…¶ä¸­$Q$æ˜¯è¯¯ç ç‡ï¼Œ$h(p) = -p \log p - (1-p) \log(1-p)$ã€‚

#### 2.3.2 é‡å­éšå½¢ä¼ æ€

**éšå½¢ä¼ æ€åè®®**ï¼š

1. Aliceå’ŒBobå…±äº«Bellæ€
2. Aliceè¿›è¡ŒBellæµ‹é‡
3. Aliceå‘é€ç»å…¸ä¿¡æ¯ç»™Bob
4. Bobè¿›è¡Œç›¸åº”æ“ä½œ

**ä¿çœŸåº¦**ï¼š
$$F = \langle\psi|\rho_{out}|\psi\rangle$$

## 3. ç¼–ç ç†è®ºæ•°å­¦ç†è®º

### 3.1 çº é”™ç 

#### 3.1.1 çº¿æ€§ç 

**ç”ŸæˆçŸ©é˜µ**ï¼š
$$G = [I_k | P]$$

å…¶ä¸­$I_k$æ˜¯$k \times k$å•ä½çŸ©é˜µï¼Œ$P$æ˜¯$k \times (n-k)$çŸ©é˜µã€‚

**æ ¡éªŒçŸ©é˜µ**ï¼š
$$H = [-P^T | I_{n-k}]$$

**ç¼–ç **ï¼š
$$\mathbf{c} = \mathbf{u}G$$

**è§£ç **ï¼š
$$\mathbf{s} = \mathbf{r}H^T$$

å…¶ä¸­$\mathbf{r}$æ˜¯æ¥æ”¶å‘é‡ï¼Œ$\mathbf{s}$æ˜¯ç—‡çŠ¶ã€‚

#### 3.1.2 å¾ªç¯ç 

**ç”Ÿæˆå¤šé¡¹å¼**ï¼š
$$g(x) = g_0 + g_1x + \cdots + g_rx^r$$

**ç¼–ç **ï¼š
$$c(x) = u(x)g(x)$$

**è§£ç **ï¼š
ä½¿ç”¨Berlekamp-Masseyç®—æ³•æˆ–Euclideanç®—æ³•ã€‚

#### 3.1.3 BCHç 

**å®šä¹‰**ï¼š
BCHç æ˜¯å¾ªç¯ç ï¼Œå…¶ç”Ÿæˆå¤šé¡¹å¼çš„æ ¹åŒ…å«$\alpha, \alpha^2, \ldots, \alpha^{2t}$ã€‚

**æœ€å°è·ç¦»**ï¼š
$$d_{min} \geq 2t + 1$$

**è§£ç **ï¼š

1. è®¡ç®—ç—‡çŠ¶
2. ä½¿ç”¨Berlekamp-Masseyç®—æ³•æ‰¾åˆ°é”™è¯¯å®šä½å¤šé¡¹å¼
3. æ‰¾åˆ°é”™è¯¯ä½ç½®
4. çº æ­£é”™è¯¯

### 3.2 ä¿¡é“ç¼–ç 

#### 3.2.1 å·ç§¯ç 

**çŠ¶æ€è½¬ç§»**ï¼š
$$\mathbf{s}_{t+1} = \mathbf{s}_t A + \mathbf{u}_t B$$
$$\mathbf{v}_t = \mathbf{s}_t C + \mathbf{u}_t D$$

**Viterbiç®—æ³•**ï¼š

1. åˆå§‹åŒ–è·¯å¾„åº¦é‡
2. å¯¹æ¯ä¸ªæ—¶é—´æ­¥ï¼š
   - è®¡ç®—åˆ†æ”¯åº¦é‡
   - æ›´æ–°è·¯å¾„åº¦é‡
   - é€‰æ‹©æœ€ä¼˜è·¯å¾„
3. å›æº¯æœ€ä¼˜è·¯å¾„

#### 3.2.2 Turboç 

**ç¼–ç å™¨ç»“æ„**ï¼š

- ä¸¤ä¸ªé€’å½’ç³»ç»Ÿå·ç§¯ç¼–ç å™¨
- éšæœºäº¤ç»‡å™¨

**è¿­ä»£è§£ç **ï¼š

1. è®¡ç®—å…ˆéªŒä¿¡æ¯
2. è®¡ç®—å¤–ä¿¡æ¯
3. æ›´æ–°å…ˆéªŒä¿¡æ¯
4. é‡å¤ç›´åˆ°æ”¶æ•›

#### 3.2.3 LDPCç 

**æ ¡éªŒçŸ©é˜µ**ï¼š
ç¨€ç–çŸ©é˜µ$H$ï¼Œæ¯è¡Œå’Œæ¯åˆ—çš„æƒé‡éƒ½å¾ˆå°ã€‚

**æ¶ˆæ¯ä¼ é€’è§£ç **ï¼š

1. åˆå§‹åŒ–å˜é‡èŠ‚ç‚¹æ¶ˆæ¯
2. æ›´æ–°æ ¡éªŒèŠ‚ç‚¹æ¶ˆæ¯
3. æ›´æ–°å˜é‡èŠ‚ç‚¹æ¶ˆæ¯
4. é‡å¤ç›´åˆ°æ”¶æ•›

### 3.3 ä¿¡æºç¼–ç 

#### 3.3.1 éœå¤«æ›¼ç¼–ç 

**ç®—æ³•**ï¼š

1. æŒ‰æ¦‚ç‡é™åºæ’åˆ—ç¬¦å·
2. åˆå¹¶æœ€å°æ¦‚ç‡çš„ä¸¤ä¸ªç¬¦å·
3. æ„å»ºç¼–ç æ ‘
4. ä»æ ¹åˆ°å¶å­åˆ†é…ç å­—

**å¹³å‡ç é•¿**ï¼š
$$L = \sum_i p_i l_i$$

å…¶ä¸­$l_i$æ˜¯ç¬¦å·$i$çš„ç é•¿ã€‚

#### 3.3.2 ç®—æœ¯ç¼–ç 

**åŒºé—´åˆ’åˆ†**ï¼š
$$[l_n, u_n) = [l_{n-1} + (u_{n-1} - l_{n-1}) F(x_n), l_{n-1} + (u_{n-1} - l_{n-1}) F(x_n + 1))$$

**ç¼–ç è¿‡ç¨‹**ï¼š

1. åˆå§‹åŒ–åŒºé—´$[0, 1)$
2. å¯¹æ¯ä¸ªç¬¦å·æ›´æ–°åŒºé—´
3. é€‰æ‹©åŒºé—´å†…çš„ä¸€ä¸ªæ•°ä½œä¸ºç å­—

#### 3.3.3 Lempel-Zivç¼–ç 

**LZ77ç®—æ³•**ï¼š

1. ç»´æŠ¤æ»‘åŠ¨çª—å£
2. æŸ¥æ‰¾æœ€é•¿åŒ¹é…
3. è¾“å‡º$(offset, length, next)$ä¸‰å…ƒç»„

**LZ78ç®—æ³•**ï¼š

1. ç»´æŠ¤å­—å…¸
2. æŸ¥æ‰¾æœ€é•¿åŒ¹é…
3. è¾“å‡º$(index, next)$å¯¹

## 4. ç½‘ç»œç¼–ç æ•°å­¦ç†è®º

### 4.1 çº¿æ€§ç½‘ç»œç¼–ç 

#### 4.1.1 åŸºæœ¬æ¦‚å¿µ

**ç½‘ç»œæ¨¡å‹**ï¼š

- æœ‰å‘æ— ç¯å›¾$G = (V, E)$
- æºèŠ‚ç‚¹$s$
- æ±‡èŠ‚ç‚¹é›†åˆ$T$

**çº¿æ€§ç½‘ç»œç¼–ç **ï¼š
$$y_e = \sum_{e' \in In(v)} f_{e', e} y_{e'}$$

å…¶ä¸­$f_{e', e}$æ˜¯ç¼–ç ç³»æ•°ã€‚

#### 4.1.2 æœ€å¤§æµæœ€å°å‰²å®šç†

**ç½‘ç»œå®¹é‡**ï¼š
$$C = \min_{S: s \in S, t \notin S} |\delta(S)|$$

å…¶ä¸­$\delta(S)$æ˜¯å‰²é›†ã€‚

**çº¿æ€§ç½‘ç»œç¼–ç å®šç†**ï¼š
å¦‚æœç½‘ç»œæ”¯æŒå¤šæ’­ä¼ è¾“ï¼Œåˆ™å­˜åœ¨çº¿æ€§ç½‘ç»œç¼–ç è§£ã€‚

### 4.2 éšæœºç½‘ç»œç¼–ç 

#### 4.2.1 éšæœºç¼–ç 

**éšæœºç³»æ•°**ï¼š
ä»æœ‰é™åŸŸ$\mathbb{F}_q$ä¸­éšæœºé€‰æ‹©ç¼–ç ç³»æ•°ã€‚

**è§£ç æ¦‚ç‡**ï¼š
$$P_{success} = \prod_{i=1}^{h} (1 - \frac{1}{q^i})$$

å…¶ä¸­$h$æ˜¯ç½‘ç»œçš„æœ€å°å‰²ã€‚

#### 4.2.2 åˆ†å¸ƒå¼å­˜å‚¨

**å­˜å‚¨èŠ‚ç‚¹**ï¼š
$$y_i = \sum_{j=1}^{k} a_{ij} x_j$$

å…¶ä¸­$x_j$æ˜¯åŸå§‹æ•°æ®å—ï¼Œ$a_{ij}$æ˜¯ç¼–ç ç³»æ•°ã€‚

**ä¿®å¤è¿‡ç¨‹**ï¼š

1. ä¸‹è½½$d$ä¸ªå­˜æ´»èŠ‚ç‚¹çš„æ•°æ®
2. æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„
3. é‡æ„ä¸¢å¤±çš„æ•°æ®

### 4.3 ä»£æ•°ç½‘ç»œç¼–ç 

#### 4.3.1 å¤šé¡¹å¼ç½‘ç»œç¼–ç 

**å¤šé¡¹å¼è¡¨ç¤º**ï¼š
$$y_e(t) = \sum_{e' \in In(v)} f_{e', e}(t) y_{e'}(t)$$

å…¶ä¸­$f_{e', e}(t)$æ˜¯å¤šé¡¹å¼ç¼–ç ç³»æ•°ã€‚

**è§£ç **ï¼š
ä½¿ç”¨å¤šé¡¹å¼æ±‚å€¼æˆ–æ’å€¼ã€‚

#### 4.3.2 å·ç§¯ç½‘ç»œç¼–ç 

**å·ç§¯ç¼–ç **ï¼š
$$y_e(t) = \sum_{e' \in In(v)} \sum_{i=0}^{m} f_{e', e}^{(i)} y_{e'}(t-i)$$

å…¶ä¸­$f_{e', e}^{(i)}$æ˜¯å·ç§¯ç³»æ•°ã€‚

## 5. ä¿¡æ¯è®ºåº”ç”¨æ•°å­¦

### 5.1 æ•°æ®å‹ç¼©

#### 5.1.1 æ— æŸå‹ç¼©

**ç†µç¼–ç **ï¼š

- éœå¤«æ›¼ç¼–ç 
- ç®—æœ¯ç¼–ç 
- Lempel-Zivç¼–ç 

**å‹ç¼©ç‡**ï¼š
$$R = \frac{L}{H(X)}$$

å…¶ä¸­$L$æ˜¯å¹³å‡ç é•¿ï¼Œ$H(X)$æ˜¯ç†µã€‚

#### 5.1.2 æœ‰æŸå‹ç¼©

**ç‡å¤±çœŸä¼˜åŒ–**ï¼š
$$\min_{Q} D(Q) \text{ s.t. } R(Q) \leq R_0$$

å…¶ä¸­$Q$æ˜¯é‡åŒ–å™¨ï¼Œ$D(Q)$æ˜¯å¤±çœŸï¼Œ$R(Q)$æ˜¯ç ç‡ã€‚

### 5.2 å¯†ç å­¦

#### 5.2.1 ä¿¡æ¯è®ºå®‰å…¨

**å®Œç¾ä¿å¯†**ï¼š
$$I(M; C) = 0$$

å…¶ä¸­$M$æ˜¯æ¶ˆæ¯ï¼Œ$C$æ˜¯å¯†æ–‡ã€‚

**ä¸€æ¬¡ä¸€å¯†**ï¼š
$$c_i = m_i \oplus k_i$$

å…¶ä¸­$k_i$æ˜¯éšæœºå¯†é’¥ã€‚

#### 5.2.2 é‡å­å¯†ç å­¦

**BB84åè®®**ï¼š

1. Aliceéšæœºé€‰æ‹©æ¯”ç‰¹å’ŒåŸºåº•
2. Bobéšæœºé€‰æ‹©æµ‹é‡åŸºåº•
3. é€šè¿‡ç»å…¸ä¿¡é“ç¡®è®¤ç›¸åŒåŸºåº•
4. è¿›è¡Œéšç§æ”¾å¤§

**å¯†é’¥ç‡**ï¼š
$$R = 1 - 2h(Q)$$

### 5.3 æœºå™¨å­¦ä¹ 

#### 5.3.1 ä¿¡æ¯ç“¶é¢ˆ

**ä¿¡æ¯ç“¶é¢ˆç›®æ ‡**ï¼š
$$\min_{p(t|x)} I(X; T) - \beta I(T; Y)$$

å…¶ä¸­$T$æ˜¯è¡¨ç¤ºï¼Œ$\beta$æ˜¯æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ã€‚

#### 5.3.2 äº’ä¿¡æ¯æœ€å¤§åŒ–

**ç‰¹å¾é€‰æ‹©**ï¼š
$$\max_{S} I(S; Y)$$

å…¶ä¸­$S$æ˜¯ç‰¹å¾å­é›†ï¼Œ$Y$æ˜¯æ ‡ç­¾ã€‚

## 6. æŠ€æœ¯å®ç°

### 6.1 Pythonå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import entropy
from scipy.special import logsumexp

# ä¿¡æ¯è®ºè®¡ç®—å®ç°
class InformationTheory:
    def __init__(self):
        pass
    
    def entropy(self, p):
        """è®¡ç®—é¦™å†œç†µ"""
        p = np.array(p)
        p = p[p > 0]  # é¿å…log(0)
        return -np.sum(p * np.log2(p))
    
    def joint_entropy(self, p_xy):
        """è®¡ç®—è”åˆç†µ"""
        return self.entropy(p_xy.flatten())
    
    def conditional_entropy(self, p_xy, p_y):
        """è®¡ç®—æ¡ä»¶ç†µ"""
        h_xy = self.joint_entropy(p_xy)
        h_y = self.entropy(p_y)
        return h_xy - h_y
    
    def mutual_information(self, p_xy, p_x, p_y):
        """è®¡ç®—äº’ä¿¡æ¯"""
        h_x = self.entropy(p_x)
        h_xy = self.joint_entropy(p_xy)
        h_y = self.entropy(p_y)
        return h_x + h_y - h_xy
    
    def channel_capacity(self, p_yx):
        """è®¡ç®—ä¿¡é“å®¹é‡"""
        # ä½¿ç”¨è¿­ä»£ç®—æ³•
        p_x = np.ones(p_yx.shape[1]) / p_yx.shape[1]
        max_iter = 100
        
        for _ in range(max_iter):
            # è®¡ç®—p(y)
            p_y = p_yx @ p_x
            
            # è®¡ç®—p(x|y)
            p_xy = p_yx * p_x.reshape(1, -1)
            p_xy = p_xy / p_xy.sum(axis=1, keepdims=True)
            
            # æ›´æ–°p(x)
            p_x_new = np.exp(np.sum(p_xy * np.log(p_xy + 1e-10), axis=0))
            p_x_new = p_x_new / p_x_new.sum()
            
            if np.allclose(p_x, p_x_new):
                break
            p_x = p_x_new
        
        # è®¡ç®—å®¹é‡
        p_xy = p_yx * p_x.reshape(1, -1)
        p_xy = p_xy / p_xy.sum(axis=1, keepdims=True)
        capacity = np.sum(p_xy * np.log2(p_xy / (p_x.reshape(1, -1) * p_y.reshape(-1, 1)) + 1e-10))
        
        return capacity

# ç¼–ç å®ç°
class CodingTheory:
    def __init__(self):
        pass
    
    def huffman_encoding(self, symbols, probabilities):
        """éœå¤«æ›¼ç¼–ç """
        from heapq import heappush, heappop, heapify
        
        # åˆ›å»ºä¼˜å…ˆé˜Ÿåˆ—
        heap = [(prob, i, symbol) for i, (prob, symbol) in enumerate(zip(probabilities, symbols))]
        heapify(heap)
        
        # æ„å»ºéœå¤«æ›¼æ ‘
        while len(heap) > 1:
            prob1, i1, symbol1 = heappop(heap)
            prob2, i2, symbol2 = heappop(heap)
            
            # åˆ›å»ºæ–°èŠ‚ç‚¹
            new_prob = prob1 + prob2
            new_symbol = (symbol1, symbol2)
            heappush(heap, (new_prob, len(heap), new_symbol))
        
        # ç”Ÿæˆç¼–ç 
        codes = {}
        def generate_codes(node, code=""):
            if isinstance(node, str):
                codes[node] = code
            else:
                generate_codes(node[0], code + "0")
                generate_codes(node[1], code + "1")
        
        if heap:
            generate_codes(heap[0][2])
        
        return codes
    
    def arithmetic_encoding(self, sequence, probabilities):
        """ç®—æœ¯ç¼–ç """
        # è®¡ç®—ç´¯ç§¯æ¦‚ç‡
        symbols = list(probabilities.keys())
        probs = list(probabilities.values())
        cum_probs = [0] + [sum(probs[:i+1]) for i in range(len(probs))]
        
        # åˆå§‹åŒ–åŒºé—´
        low, high = 0.0, 1.0
        
        # ç¼–ç 
        for symbol in sequence:
            idx = symbols.index(symbol)
            range_size = high - low
            high = low + range_size * cum_probs[idx + 1]
            low = low + range_size * cum_probs[idx]
        
        return (low + high) / 2
    
    def arithmetic_decoding(self, code, length, probabilities):
        """ç®—æœ¯è§£ç """
        symbols = list(probabilities.keys())
        probs = list(probabilities.values())
        cum_probs = [0] + [sum(probs[:i+1]) for i in range(len(probs))]
        
        low, high = 0.0, 1.0
        decoded = []
        
        for _ in range(length):
            range_size = high - low
            for i, symbol in enumerate(symbols):
                if (low + range_size * cum_probs[i] <= code < 
                    low + range_size * cum_probs[i + 1]):
                    decoded.append(symbol)
                    high = low + range_size * cum_probs[i + 1]
                    low = low + range_size * cum_probs[i]
                    break
        
        return decoded

# é‡å­ä¿¡æ¯è®ºå®ç°
class QuantumInformation:
    def __init__(self):
        pass
    
    def density_matrix(self, state):
        """è®¡ç®—å¯†åº¦çŸ©é˜µ"""
        return np.outer(state, state.conj())
    
    def von_neumann_entropy(self, rho):
        """è®¡ç®—å†¯Â·è¯ºä¾æ›¼ç†µ"""
        eigenvalues = np.linalg.eigvals(rho)
        eigenvalues = eigenvalues[eigenvalues > 0]
        return -np.sum(eigenvalues * np.log2(eigenvalues))
    
    def entanglement_entropy(self, rho, dim_A):
        """è®¡ç®—çº ç¼ ç†µ"""
        # è®¡ç®—çº¦åŒ–å¯†åº¦çŸ©é˜µ
        dim_B = rho.shape[0] // dim_A
        rho_A = np.zeros((dim_A, dim_A), dtype=complex)
        
        for i in range(dim_A):
            for j in range(dim_A):
                for k in range(dim_B):
                    rho_A[i, j] += rho[i*dim_B + k, j*dim_B + k]
        
        return self.von_neumann_entropy(rho_A)
    
    def bell_state(self, state_type):
        """ç”ŸæˆBellæ€"""
        if state_type == "phi_plus":
            return np.array([1, 0, 0, 1]) / np.sqrt(2)
        elif state_type == "phi_minus":
            return np.array([1, 0, 0, -1]) / np.sqrt(2)
        elif state_type == "psi_plus":
            return np.array([0, 1, 1, 0]) / np.sqrt(2)
        elif state_type == "psi_minus":
            return np.array([0, 1, -1, 0]) / np.sqrt(2)

# ä½¿ç”¨ç¤ºä¾‹
# ä¿¡æ¯è®ºè®¡ç®—
it = InformationTheory()

# è®¡ç®—ç†µ
p = [0.5, 0.3, 0.2]
entropy_val = it.entropy(p)
print(f"ç†µ: {entropy_val:.3f}")

# è®¡ç®—äº’ä¿¡æ¯
p_xy = np.array([[0.3, 0.1], [0.2, 0.4]])
p_x = np.sum(p_xy, axis=1)
p_y = np.sum(p_xy, axis=0)
mi = it.mutual_information(p_xy, p_x, p_y)
print(f"äº’ä¿¡æ¯: {mi:.3f}")

# éœå¤«æ›¼ç¼–ç 
symbols = ['a', 'b', 'c', 'd']
probabilities = [0.4, 0.3, 0.2, 0.1]
codes = it.huffman_encoding(symbols, probabilities)
print("éœå¤«æ›¼ç¼–ç :", codes)

# é‡å­ä¿¡æ¯è®º
qi = QuantumInformation()

# Bellæ€
bell_state = qi.bell_state("phi_plus")
rho = qi.density_matrix(bell_state)
entanglement = qi.entanglement_entropy(rho, 2)
print(f"çº ç¼ ç†µ: {entanglement:.3f}")

# å¯è§†åŒ–
plt.figure(figsize=(12, 4))

# ç†µéšæ¦‚ç‡å˜åŒ–
p_values = np.linspace(0.01, 0.99, 100)
entropy_values = [it.entropy([p, 1-p]) for p in p_values]

plt.subplot(1, 2, 1)
plt.plot(p_values, entropy_values)
plt.xlabel('æ¦‚ç‡ p')
plt.ylabel('ç†µ H(p)')
plt.title('äºŒå…ƒç†µå‡½æ•°')
plt.grid(True)

# ä¿¡é“å®¹é‡
snr_values = np.logspace(-1, 2, 100)
capacity_values = [0.5 * np.log2(1 + snr) for snr in snr_values]

plt.subplot(1, 2, 2)
plt.semilogx(snr_values, capacity_values)
plt.xlabel('ä¿¡å™ªæ¯” SNR')
plt.ylabel('ä¿¡é“å®¹é‡ C')
plt.title('é«˜æ–¯ä¿¡é“å®¹é‡')
plt.grid(True)

plt.tight_layout()
plt.show()
```

### 6.2 é‡å­è®¡ç®—å®ç°

```python
import numpy as np
from qutip import *

# é‡å­ä¿¡æ¯è®ºå®ç°
class QuantumInformationTheory:
    def __init__(self):
        pass
    
    def qubit_state(self, theta, phi):
        """åˆ›å»ºé‡å­æ¯”ç‰¹çŠ¶æ€"""
        return np.cos(theta/2) * np.array([1, 0]) + np.exp(1j*phi) * np.sin(theta/2) * np.array([0, 1])
    
    def density_matrix(self, state):
        """è®¡ç®—å¯†åº¦çŸ©é˜µ"""
        return np.outer(state, state.conj())
    
    def partial_trace(self, rho, dim_A, dim_B):
        """è®¡ç®—åè¿¹"""
        rho_reshaped = rho.reshape(dim_A, dim_B, dim_A, dim_B)
        return np.trace(rho_reshaped, axis1=1, axis2=3)
    
    def von_neumann_entropy(self, rho):
        """è®¡ç®—å†¯Â·è¯ºä¾æ›¼ç†µ"""
        eigenvalues = np.linalg.eigvals(rho)
        eigenvalues = eigenvalues[eigenvalues > 0]
        return -np.sum(eigenvalues * np.log2(eigenvalues))
    
    def entanglement_entropy(self, rho, dim_A):
        """è®¡ç®—çº ç¼ ç†µ"""
        rho_A = self.partial_trace(rho, dim_A, rho.shape[0]//dim_A)
        return self.von_neumann_entropy(rho_A)
    
    def bell_measurement(self, state):
        """Bellæµ‹é‡"""
        bell_states = {
            'phi_plus': np.array([1, 0, 0, 1]) / np.sqrt(2),
            'phi_minus': np.array([1, 0, 0, -1]) / np.sqrt(2),
            'psi_plus': np.array([0, 1, 1, 0]) / np.sqrt(2),
            'psi_minus': np.array([0, 1, -1, 0]) / np.sqrt(2)
        }
        
        probabilities = {}
        for name, bell_state in bell_states.items():
            overlap = np.abs(np.dot(bell_state.conj(), state))**2
            probabilities[name] = overlap
        
        return probabilities

# é‡å­å¯†é’¥åˆ†å‘æ¨¡æ‹Ÿ
class QuantumKeyDistribution:
    def __init__(self):
        self.bases = ['Z', 'X']  # è®¡ç®—åŸºåº•å’ŒHadamardåŸºåº•
        self.bit_values = [0, 1]
    
    def generate_qubit(self, bit, basis):
        """ç”Ÿæˆé‡å­æ¯”ç‰¹"""
        if basis == 'Z':
            return np.array([1, 0]) if bit == 0 else np.array([0, 1])
        else:  # X basis
            return (np.array([1, 1]) / np.sqrt(2)) if bit == 0 else (np.array([1, -1]) / np.sqrt(2))
    
    def measure_qubit(self, qubit, basis):
        """æµ‹é‡é‡å­æ¯”ç‰¹"""
        if basis == 'Z':
            # åœ¨ZåŸºåº•æµ‹é‡
            prob_0 = np.abs(qubit[0])**2
            return 0 if np.random.random() < prob_0 else 1
        else:
            # åœ¨XåŸºåº•æµ‹é‡
            qubit_x = (qubit[0] + qubit[1]) / np.sqrt(2)
            prob_0 = np.abs(qubit_x)**2
            return 0 if np.random.random() < prob_0 else 1
    
    def simulate_bb84(self, n_qubits, error_rate=0.0):
        """æ¨¡æ‹ŸBB84åè®®"""
        # Aliceç”Ÿæˆéšæœºæ¯”ç‰¹å’ŒåŸºåº•
        alice_bits = np.random.choice(self.bit_values, n_qubits)
        alice_bases = np.random.choice(self.bases, n_qubits)
        
        # Bobé€‰æ‹©éšæœºåŸºåº•
        bob_bases = np.random.choice(self.bases, n_qubits)
        
        # ç”Ÿæˆå’Œæµ‹é‡é‡å­æ¯”ç‰¹
        bob_bits = []
        for i in range(n_qubits):
            qubit = self.generate_qubit(alice_bits[i], alice_bases[i])
            
            # æ·»åŠ å™ªå£°
            if np.random.random() < error_rate:
                qubit = np.array([qubit[1], qubit[0]])  # æ¯”ç‰¹ç¿»è½¬
            
            bob_bits.append(self.measure_qubit(qubit, bob_bases[i]))
        
        # ç­›é€‰ç›¸åŒåŸºåº•çš„æµ‹é‡
        same_bases = alice_bases == bob_bases
        alice_key = alice_bits[same_bases]
        bob_key = np.array(bob_bits)[same_bases]
        
        # è®¡ç®—è¯¯ç ç‡
        error_rate_measured = np.mean(alice_key != bob_key)
        
        return {
            'alice_key': alice_key,
            'bob_key': bob_key,
            'key_length': len(alice_key),
            'error_rate': error_rate_measured
        }

# ä½¿ç”¨ç¤ºä¾‹
qit = QuantumInformationTheory()
qkd = QuantumKeyDistribution()

# é‡å­æ¯”ç‰¹çŠ¶æ€
theta = np.pi/4
phi = np.pi/3
qubit = qit.qubit_state(theta, phi)
rho = qit.density_matrix(qubit)
entropy = qit.von_neumann_entropy(rho)
print(f"é‡å­æ¯”ç‰¹ç†µ: {entropy:.3f}")

# Bellæ€
bell_state = qit.bell_state("phi_plus")
rho_bell = qit.density_matrix(bell_state)
entanglement = qit.entanglement_entropy(rho_bell, 2)
print(f"Bellæ€çº ç¼ ç†µ: {entanglement:.3f}")

# BB84åè®®æ¨¡æ‹Ÿ
result = qkd.simulate_bb84(1000, error_rate=0.05)
print(f"å¯†é’¥é•¿åº¦: {result['key_length']}")
print(f"è¯¯ç ç‡: {result['error_rate']:.3f}")

# å¯è§†åŒ–
plt.figure(figsize=(12, 4))

# é‡å­æ¯”ç‰¹åœ¨Blochçƒä¸Šçš„è¡¨ç¤º
theta_values = np.linspace(0, np.pi, 100)
phi_values = np.linspace(0, 2*np.pi, 100)
entropy_values = np.zeros((len(theta_values), len(phi_values)))

for i, theta in enumerate(theta_values):
    for j, phi in enumerate(phi_values):
        qubit = qit.qubit_state(theta, phi)
        rho = qit.density_matrix(qubit)
        entropy_values[i, j] = qit.von_neumann_entropy(rho)

plt.subplot(1, 2, 1)
plt.imshow(entropy_values, extent=[0, 2*np.pi, 0, np.pi], aspect='auto')
plt.colorbar(label='ç†µ')
plt.xlabel('Ï†')
plt.ylabel('Î¸')
plt.title('é‡å­æ¯”ç‰¹ç†µ')

# è¯¯ç ç‡å¯¹å¯†é’¥ç‡çš„å½±å“
error_rates = np.linspace(0, 0.5, 100)
key_rates = [1 - 2 * (-p*np.log2(p) - (1-p)*np.log2(1-p)) for p in error_rates]

plt.subplot(1, 2, 2)
plt.plot(error_rates, key_rates)
plt.xlabel('è¯¯ç ç‡')
plt.ylabel('å¯†é’¥ç‡')
plt.title('BB84åè®®å¯†é’¥ç‡')
plt.grid(True)

plt.tight_layout()
plt.show()
```

## 7. åº”ç”¨æ¡ˆä¾‹

### 7.1 æ•°æ®å‹ç¼©åº”ç”¨

**å›¾åƒå‹ç¼©**ï¼š

- JPEGç®—æ³•ä¸­çš„ç†µç¼–ç 
- å°æ³¢å˜æ¢ä¸é‡åŒ–
- ç‡å¤±çœŸä¼˜åŒ–

### 7.2 é€šä¿¡ç³»ç»Ÿåº”ç”¨

**æ— çº¿é€šä¿¡**ï¼š

- ä¿¡é“ç¼–ç ä¸è§£ç 
- å¤šç”¨æˆ·æ£€æµ‹
- åŠŸç‡æ§åˆ¶

### 7.3 é‡å­é€šä¿¡åº”ç”¨

**é‡å­å¯†é’¥åˆ†å‘**ï¼š

- BB84åè®®å®ç°
- é‡å­ä¸­ç»§å™¨
- é‡å­ç½‘ç»œ

## 8. å‰æ²¿å‘å±•

### 8.1 é‡å­ä¿¡æ¯è®ºå‰æ²¿

**é‡å­è®¡ç®—**ï¼š

- é‡å­ç®—æ³•è®¾è®¡
- é‡å­çº é”™ç 
- é‡å­æœºå™¨å­¦ä¹ 

### 8.2 ç½‘ç»œä¿¡æ¯è®ºå‰æ²¿

**å¤šç”¨æˆ·ä¿¡æ¯è®º**ï¼š

- å¹²æ‰°ä¿¡é“
- å¹¿æ’­ä¿¡é“
- ä¸­ç»§ä¿¡é“

### 8.3 ä¿¡æ¯è®ºä¸æœºå™¨å­¦ä¹ 

**ä¿¡æ¯ç“¶é¢ˆç†è®º**ï¼š

- è¡¨ç¤ºå­¦ä¹ 
- ç‰¹å¾é€‰æ‹©
- æ¨¡å‹å‹ç¼©

## 9. æ€»ç»“ä¸å±•æœ›

### 9.1 æ ¸å¿ƒè¦ç‚¹æ€»ç»“

1. **é¦™å†œä¿¡æ¯è®ºåŸºç¡€**ï¼š
   - ç†µã€äº’ä¿¡æ¯ã€ä¿¡é“å®¹é‡çš„æ•°å­¦å®šä¹‰
   - ä¿¡æºç¼–ç å’Œä¿¡é“ç¼–ç çš„ç†è®º
   - ä¿¡æ¯ä¸ç­‰å¼çš„åº”ç”¨

2. **é‡å­ä¿¡æ¯è®ºç†è®º**ï¼š
   - é‡å­æ¯”ç‰¹å’Œé‡å­æ€çš„æ•°å­¦è¡¨ç¤º
   - é‡å­çº ç¼ çš„åº¦é‡å’Œåº”ç”¨
   - é‡å­é€šä¿¡åè®®çš„è®¾è®¡

3. **ç¼–ç ç†è®ºæ–¹æ³•**ï¼š
   - çº¿æ€§ç ã€å¾ªç¯ç ã€BCHç çš„è®¾è®¡
   - å·ç§¯ç ã€Turboç ã€LDPCç çš„ç®—æ³•
   - éœå¤«æ›¼ç¼–ç ã€ç®—æœ¯ç¼–ç çš„å®ç°

4. **ç½‘ç»œç¼–ç ç†è®º**ï¼š
   - çº¿æ€§ç½‘ç»œç¼–ç çš„è®¾è®¡
   - éšæœºç½‘ç»œç¼–ç çš„åº”ç”¨
   - åˆ†å¸ƒå¼å­˜å‚¨çš„å®ç°

### 9.2 å‘å±•è¶‹åŠ¿

1. **ç†è®ºå‘å±•**ï¼š
   - é‡å­ä¿¡æ¯è®ºçš„æ·±åŒ–
   - ç½‘ç»œä¿¡æ¯è®ºçš„æ‹“å±•
   - ä¿¡æ¯è®ºä¸æœºå™¨å­¦ä¹ çš„èåˆ

2. **æŠ€æœ¯åº”ç”¨**ï¼š
   - é‡å­é€šä¿¡çš„å®ç”¨åŒ–
   - 5G/6Gé€šä¿¡ç³»ç»Ÿçš„ä¼˜åŒ–
   - å¤§æ•°æ®å‹ç¼©æŠ€æœ¯çš„å‘å±•

3. **è·¨å­¦ç§‘èåˆ**ï¼š
   - ä¿¡æ¯è®ºåœ¨ç”Ÿç‰©å­¦ä¸­çš„åº”ç”¨
   - ä¿¡æ¯è®ºåœ¨ç»æµå­¦ä¸­çš„åº”ç”¨
   - ä¿¡æ¯è®ºåœ¨ç¥ç»ç§‘å­¦ä¸­çš„åº”ç”¨

### 9.3 æŒ‘æˆ˜ä¸æœºé‡

**ä¸»è¦æŒ‘æˆ˜**ï¼š

- é‡å­ç³»ç»Ÿçš„é€€ç›¸å¹²é—®é¢˜
- å¤§è§„æ¨¡ç½‘ç»œç¼–ç çš„è®¡ç®—å¤æ‚åº¦
- ä¿¡æ¯è®ºå®‰å…¨æ€§çš„å®ç°

**å‘å±•æœºé‡**ï¼š

- é‡å­è®¡ç®—æŠ€æœ¯çš„çªç ´
- äººå·¥æ™ºèƒ½ä¸ä¿¡æ¯è®ºçš„ç»“åˆ
- æ–°ä¸€ä»£é€šä¿¡æŠ€æœ¯çš„å‘å±•

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. Cover, T. M., & Thomas, J. A. (2006). Elements of Information Theory. Wiley.
2. Nielsen, M. A., & Chuang, I. L. (2010). Quantum Computation and Quantum Information. Cambridge University Press.
3. MacKay, D. J. C. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge University Press.
4. Yeung, R. W. (2008). Information Theory and Network Coding. Springer.
5. Wilde, M. M. (2017). Quantum Information Theory. Cambridge University Press.

## ğŸ”— ç›¸å…³é“¾æ¥

- [æ¦‚ç‡è®ºåŸºç¡€](../12-åº”ç”¨æ•°å­¦/01-æ¦‚ç‡è®º.md)
- [ç»Ÿè®¡å­¦åŸºç¡€](../12-åº”ç”¨æ•°å­¦/02-ç»Ÿè®¡å­¦.md)
- [äººå·¥æ™ºèƒ½æ•°å­¦](../12-åº”ç”¨æ•°å­¦/07-äººå·¥æ™ºèƒ½æ•°å­¦-æ·±åŒ–ç‰ˆ.md)
- [ç½‘ç»œç§‘å­¦æ•°å­¦](../12-åº”ç”¨æ•°å­¦/09-ç½‘ç»œç§‘å­¦æ•°å­¦-æ·±åŒ–ç‰ˆ.md)

---

*æœ¬æ·±åŒ–ç‰ˆæ–‡æ¡£æ·±å…¥æ¢è®¨äº†ä¿¡æ¯è®ºçš„æ•°å­¦ç†è®ºåŸºç¡€ï¼Œä¸ºç†è§£ä¿¡æ¯ä¼ è¾“ã€å­˜å‚¨å’Œå¤„ç†æä¾›äº†å¼ºå¤§çš„æ•°å­¦å·¥å…·ã€‚*
