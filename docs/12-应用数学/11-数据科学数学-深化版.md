# æ•°æ®ç§‘å­¦æ•°å­¦ - æ·±åŒ–ç‰ˆ

## ðŸ“š æ¦‚è¿°

æ•°æ®ç§‘å­¦æ•°å­¦æ˜¯æ•°æ®ç§‘å­¦é¢†åŸŸçš„æ•°å­¦ç†è®ºåŸºç¡€ï¼Œæ¶µç›–äº†æ•°æ®æŒ–æŽ˜ã€æ•°æ®å¯è§†åŒ–ã€å¤§æ•°æ®å¤„ç†ç­‰æ ¸å¿ƒæŠ€æœ¯çš„æ•°å­¦åŽŸç†ã€‚æœ¬æ·±åŒ–ç‰ˆå°†æ·±å…¥æŽ¢è®¨æ•°æ®ç§‘å­¦çš„æ•°å­¦åŸºç¡€ï¼ŒåŒ…æ‹¬ç»Ÿè®¡å­¦ä¹ ã€æœºå™¨å­¦ä¹ ã€æ•°æ®æŒ–æŽ˜ç­‰æ ¸å¿ƒå†…å®¹ã€‚

## ðŸŽ¯ å­¦ä¹ ç›®æ ‡

1. **æŽŒæ¡æ•°æ®æŒ–æŽ˜æ•°å­¦åŸºç¡€**ï¼šç†è§£èšç±»åˆ†æžã€å…³è”è§„åˆ™æŒ–æŽ˜ã€å¼‚å¸¸æ£€æµ‹çš„æ•°å­¦å»ºæ¨¡
2. **æŽŒæ¡æ•°æ®å¯è§†åŒ–æ•°å­¦ç†è®º**ï¼šç†è§£é™ç»´æŠ€æœ¯ã€æµå½¢å­¦ä¹ ã€æ‹“æ‰‘æ•°æ®åˆ†æžçš„æ•°å­¦æ–¹æ³•
3. **æŽŒæ¡å¤§æ•°æ®å¤„ç†æ•°å­¦åŽŸç†**ï¼šç†è§£åˆ†å¸ƒå¼è®¡ç®—ã€æµå¤„ç†ã€å›¾è®¡ç®—çš„æ•°å­¦ç†è®º
4. **æŽŒæ¡ç»Ÿè®¡å­¦ä¹ æ•°å­¦æ–¹æ³•**ï¼šç†è§£ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸºç¡€

## ðŸ“– ç›®å½•

- [æ•°æ®ç§‘å­¦æ•°å­¦ - æ·±åŒ–ç‰ˆ](#æ•°æ®ç§‘å­¦æ•°å­¦---æ·±åŒ–ç‰ˆ)
  - [ðŸ“š æ¦‚è¿°](#-æ¦‚è¿°)
  - [ðŸŽ¯ å­¦ä¹ ç›®æ ‡](#-å­¦ä¹ ç›®æ ‡)
  - [ðŸ“– ç›®å½•](#-ç›®å½•)
  - [1. æ•°æ®æŒ–æŽ˜æ•°å­¦ç†è®º](#1-æ•°æ®æŒ–æŽ˜æ•°å­¦ç†è®º)
    - [1.1 èšç±»åˆ†æžæ•°å­¦](#11-èšç±»åˆ†æžæ•°å­¦)
      - [1.1.1 K-meansèšç±»](#111-k-meansèšç±»)
      - [1.1.2 å±‚æ¬¡èšç±»](#112-å±‚æ¬¡èšç±»)
      - [1.1.3 å¯†åº¦èšç±»](#113-å¯†åº¦èšç±»)
    - [1.2 å…³è”è§„åˆ™æŒ–æŽ˜æ•°å­¦](#12-å…³è”è§„åˆ™æŒ–æŽ˜æ•°å­¦)
      - [1.2.1 æ”¯æŒåº¦å’Œç½®ä¿¡åº¦](#121-æ”¯æŒåº¦å’Œç½®ä¿¡åº¦)
      - [1.2.2 Aprioriç®—æ³•](#122-aprioriç®—æ³•)
      - [1.2.3 FP-Growthç®—æ³•](#123-fp-growthç®—æ³•)
    - [1.3 å¼‚å¸¸æ£€æµ‹æ•°å­¦](#13-å¼‚å¸¸æ£€æµ‹æ•°å­¦)
      - [1.3.1 ç»Ÿè®¡æ–¹æ³•](#131-ç»Ÿè®¡æ–¹æ³•)
      - [1.3.2 åŸºäºŽè·ç¦»çš„æ–¹æ³•](#132-åŸºäºŽè·ç¦»çš„æ–¹æ³•)
      - [1.3.3 åŸºäºŽå¯†åº¦çš„æ–¹æ³•](#133-åŸºäºŽå¯†åº¦çš„æ–¹æ³•)
  - [2. æ•°æ®å¯è§†åŒ–æ•°å­¦ç†è®º](#2-æ•°æ®å¯è§†åŒ–æ•°å­¦ç†è®º)
    - [2.1 é™ç»´æŠ€æœ¯æ•°å­¦](#21-é™ç»´æŠ€æœ¯æ•°å­¦)
      - [2.1.1 ä¸»æˆåˆ†åˆ†æžï¼ˆPCAï¼‰](#211-ä¸»æˆåˆ†åˆ†æžpca)
      - [2.1.2 çº¿æ€§åˆ¤åˆ«åˆ†æžï¼ˆLDAï¼‰](#212-çº¿æ€§åˆ¤åˆ«åˆ†æžlda)
      - [2.1.3 t-SNE](#213-t-sne)
    - [2.2 æµå½¢å­¦ä¹ æ•°å­¦](#22-æµå½¢å­¦ä¹ æ•°å­¦)
      - [2.2.1 å±€éƒ¨çº¿æ€§åµŒå…¥ï¼ˆLLEï¼‰](#221-å±€éƒ¨çº¿æ€§åµŒå…¥lle)
      - [2.2.2 ç­‰è·æ˜ å°„ï¼ˆIsomapï¼‰](#222-ç­‰è·æ˜ å°„isomap)
      - [2.2.3 æ‹‰æ™®æ‹‰æ–¯ç‰¹å¾æ˜ å°„](#223-æ‹‰æ™®æ‹‰æ–¯ç‰¹å¾æ˜ å°„)
    - [2.3 æ‹“æ‰‘æ•°æ®åˆ†æžæ•°å­¦](#23-æ‹“æ‰‘æ•°æ®åˆ†æžæ•°å­¦)
      - [2.3.1 æŒä¹…åŒè°ƒ](#231-æŒä¹…åŒè°ƒ)
      - [2.3.2 æŒä¹…å›¾](#232-æŒä¹…å›¾)
      - [2.3.3 Mapperç®—æ³•](#233-mapperç®—æ³•)
  - [3. å¤§æ•°æ®å¤„ç†æ•°å­¦ç†è®º](#3-å¤§æ•°æ®å¤„ç†æ•°å­¦ç†è®º)
    - [3.1 åˆ†å¸ƒå¼è®¡ç®—æ•°å­¦](#31-åˆ†å¸ƒå¼è®¡ç®—æ•°å­¦)
      - [3.1.1 MapReduceæ¨¡åž‹](#311-mapreduceæ¨¡åž‹)
      - [3.1.2 åˆ†å¸ƒå¼ä¼˜åŒ–](#312-åˆ†å¸ƒå¼ä¼˜åŒ–)
      - [3.1.3 ä¸€è‡´æ€§ç®—æ³•](#313-ä¸€è‡´æ€§ç®—æ³•)
    - [3.2 æµå¤„ç†æ•°å­¦](#32-æµå¤„ç†æ•°å­¦)
      - [3.2.1 æ»‘åŠ¨çª—å£](#321-æ»‘åŠ¨çª—å£)
      - [3.2.2 æµç®—æ³•](#322-æµç®—æ³•)
      - [3.2.3 æµèšç±»](#323-æµèšç±»)
    - [3.3 å›¾è®¡ç®—æ•°å­¦](#33-å›¾è®¡ç®—æ•°å­¦)
      - [3.3.1 PageRankç®—æ³•](#331-pagerankç®—æ³•)
      - [3.3.2 ç¤¾åŒºæ£€æµ‹](#332-ç¤¾åŒºæ£€æµ‹)
      - [3.3.3 æœ€çŸ­è·¯å¾„](#333-æœ€çŸ­è·¯å¾„)
  - [4. ç»Ÿè®¡å­¦ä¹ æ•°å­¦ç†è®º](#4-ç»Ÿè®¡å­¦ä¹ æ•°å­¦ç†è®º)
    - [4.1 ç›‘ç£å­¦ä¹ æ•°å­¦](#41-ç›‘ç£å­¦ä¹ æ•°å­¦)
      - [4.1.1 çº¿æ€§å›žå½’](#411-çº¿æ€§å›žå½’)
      - [4.1.2 é€»è¾‘å›žå½’](#412-é€»è¾‘å›žå½’)
      - [4.1.3 æ”¯æŒå‘é‡æœº](#413-æ”¯æŒå‘é‡æœº)
    - [4.2 æ— ç›‘ç£å­¦ä¹ æ•°å­¦](#42-æ— ç›‘ç£å­¦ä¹ æ•°å­¦)
      - [4.2.1 ä¸»æˆåˆ†åˆ†æž](#421-ä¸»æˆåˆ†åˆ†æž)
      - [4.2.2 ç‹¬ç«‹æˆåˆ†åˆ†æž](#422-ç‹¬ç«‹æˆåˆ†åˆ†æž)
      - [4.2.3 è‡ªç¼–ç å™¨](#423-è‡ªç¼–ç å™¨)
    - [4.3 å¼ºåŒ–å­¦ä¹ æ•°å­¦](#43-å¼ºåŒ–å­¦ä¹ æ•°å­¦)
      - [4.3.1 Qå­¦ä¹ ](#431-qå­¦ä¹ )
      - [4.3.2 ç­–ç•¥æ¢¯åº¦](#432-ç­–ç•¥æ¢¯åº¦)
      - [4.3.3 Actor-Critic](#433-actor-critic)
  - [5. æŠ€æœ¯å®žçŽ°](#5-æŠ€æœ¯å®žçŽ°)
    - [5.1 Pythonå®žçŽ°](#51-pythonå®žçŽ°)
    - [5.2 åˆ†å¸ƒå¼è®¡ç®—å®žçŽ°](#52-åˆ†å¸ƒå¼è®¡ç®—å®žçŽ°)
  - [6. åº”ç”¨æ¡ˆä¾‹](#6-åº”ç”¨æ¡ˆä¾‹)
    - [6.1 æ•°æ®æŒ–æŽ˜åº”ç”¨](#61-æ•°æ®æŒ–æŽ˜åº”ç”¨)
    - [6.2 æ•°æ®å¯è§†åŒ–åº”ç”¨](#62-æ•°æ®å¯è§†åŒ–åº”ç”¨)
    - [6.3 å¤§æ•°æ®å¤„ç†åº”ç”¨](#63-å¤§æ•°æ®å¤„ç†åº”ç”¨)
  - [7. å‰æ²¿å‘å±•](#7-å‰æ²¿å‘å±•)
    - [7.1 æ·±åº¦å­¦ä¹ ä¸Žæ•°æ®ç§‘å­¦](#71-æ·±åº¦å­¦ä¹ ä¸Žæ•°æ®ç§‘å­¦)
    - [7.2 å›¾ç¥žç»ç½‘ç»œ](#72-å›¾ç¥žç»ç½‘ç»œ)
    - [7.3 è”é‚¦å­¦ä¹ ](#73-è”é‚¦å­¦ä¹ )
  - [8. æ€»ç»“ä¸Žå±•æœ›](#8-æ€»ç»“ä¸Žå±•æœ›)
    - [8.1 æ ¸å¿ƒè¦ç‚¹æ€»ç»“](#81-æ ¸å¿ƒè¦ç‚¹æ€»ç»“)
    - [8.2 å‘å±•è¶‹åŠ¿](#82-å‘å±•è¶‹åŠ¿)
    - [8.3 æŒ‘æˆ˜ä¸Žæœºé‡](#83-æŒ‘æˆ˜ä¸Žæœºé‡)
  - [ðŸ“š å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)
  - [ðŸ”— ç›¸å…³é“¾æŽ¥](#-ç›¸å…³é“¾æŽ¥)

---

## 1. æ•°æ®æŒ–æŽ˜æ•°å­¦ç†è®º

### 1.1 èšç±»åˆ†æžæ•°å­¦

#### 1.1.1 K-meansèšç±»

**ç›®æ ‡å‡½æ•°**ï¼š
$$J = \sum_{i=1}^{k} \sum_{x \in C_i} \|x - \mu_i\|^2$$

å…¶ä¸­ï¼š

- $C_i$æ˜¯ç¬¬$i$ä¸ªèšç±»
- $\mu_i$æ˜¯ç¬¬$i$ä¸ªèšç±»çš„ä¸­å¿ƒ
- $k$æ˜¯èšç±»æ•°

**ç®—æ³•æ­¥éª¤**ï¼š

1. éšæœºåˆå§‹åŒ–$k$ä¸ªèšç±»ä¸­å¿ƒ
2. å°†æ¯ä¸ªç‚¹åˆ†é…åˆ°æœ€è¿‘çš„èšç±»ä¸­å¿ƒ
3. é‡æ–°è®¡ç®—èšç±»ä¸­å¿ƒ
4. é‡å¤æ­¥éª¤2-3ç›´åˆ°æ”¶æ•›

**æ”¶æ•›æ€§**ï¼š
ç›®æ ‡å‡½æ•°$J$åœ¨æ¯æ¬¡è¿­ä»£åŽå•è°ƒé€’å‡ï¼Œå› æ­¤ç®—æ³•å¿…ç„¶æ”¶æ•›ã€‚

#### 1.1.2 å±‚æ¬¡èšç±»

**è·ç¦»åº¦é‡**ï¼š

- å•é“¾æŽ¥ï¼š$d(C_i, C_j) = \min_{x \in C_i, y \in C_j} d(x, y)$
- å®Œå…¨é“¾æŽ¥ï¼š$d(C_i, C_j) = \max_{x \in C_i, y \in C_j} d(x, y)$
- å¹³å‡é“¾æŽ¥ï¼š$d(C_i, C_j) = \frac{1}{|C_i||C_j|} \sum_{x \in C_i, y \in C_j} d(x, y)$

**ç®—æ³•æ­¥éª¤**ï¼š

1. æ¯ä¸ªç‚¹ä½œä¸ºä¸€ä¸ªèšç±»
2. æ‰¾åˆ°è·ç¦»æœ€è¿‘çš„ä¸¤ä¸ªèšç±»
3. åˆå¹¶è¿™ä¸¤ä¸ªèšç±»
4. é‡å¤æ­¥éª¤2-3ç›´åˆ°åªå‰©ä¸€ä¸ªèšç±»

#### 1.1.3 å¯†åº¦èšç±»

**DBSCANç®—æ³•**ï¼š

- æ ¸å¿ƒç‚¹ï¼šé‚»åŸŸå†…ç‚¹æ•°$\geq \text{minPts}$
- è¾¹ç•Œç‚¹ï¼šä¸æ˜¯æ ¸å¿ƒç‚¹ä½†åœ¨æ ¸å¿ƒç‚¹é‚»åŸŸå†…
- å™ªå£°ç‚¹ï¼šæ—¢ä¸æ˜¯æ ¸å¿ƒç‚¹ä¹Ÿä¸æ˜¯è¾¹ç•Œç‚¹

**å¯†åº¦å¯è¾¾æ€§**ï¼š
ç‚¹$p$å¯†åº¦å¯è¾¾ç‚¹$q$ï¼Œå¦‚æžœå­˜åœ¨ç‚¹åºåˆ—$p_1, p_2, \ldots, p_n$ï¼Œä½¿å¾—$p_1 = p$ï¼Œ$p_n = q$ï¼Œä¸”$p_{i+1}$åœ¨$p_i$çš„$\epsilon$-é‚»åŸŸå†…ã€‚

### 1.2 å…³è”è§„åˆ™æŒ–æŽ˜æ•°å­¦

#### 1.2.1 æ”¯æŒåº¦å’Œç½®ä¿¡åº¦

**æ”¯æŒåº¦**ï¼š
$$\text{support}(A \to B) = \frac{|\{T \in D : A \cup B \subseteq T\}|}{|D|}$$

**ç½®ä¿¡åº¦**ï¼š
$$\text{confidence}(A \to B) = \frac{\text{support}(A \cup B)}{\text{support}(A)}$$

**æå‡åº¦**ï¼š
$$\text{lift}(A \to B) = \frac{\text{confidence}(A \to B)}{\text{support}(B)}$$

#### 1.2.2 Aprioriç®—æ³•

**å‘ä¸‹é—­åŒ…æ€§è´¨**ï¼š
å¦‚æžœé¡¹é›†$X$æ˜¯é¢‘ç¹çš„ï¼Œåˆ™$X$çš„æ‰€æœ‰å­é›†éƒ½æ˜¯é¢‘ç¹çš„ã€‚

**ç®—æ³•æ­¥éª¤**ï¼š

1. ç”Ÿæˆ1-é¡¹é›†
2. ç”Ÿæˆ$k$-é¡¹é›†ï¼ˆ$k \geq 2$ï¼‰
3. è®¡ç®—æ”¯æŒåº¦
4. å‰ªæžéžé¢‘ç¹é¡¹é›†
5. é‡å¤æ­¥éª¤2-4ç›´åˆ°æ²¡æœ‰é¢‘ç¹é¡¹é›†

#### 1.2.3 FP-Growthç®—æ³•

**FPæ ‘æž„é€ **ï¼š

1. æ‰«ææ•°æ®åº“ï¼Œè®¡ç®—é¡¹çš„æ”¯æŒåº¦
2. æŒ‰æ”¯æŒåº¦é™åºæŽ’åˆ—é¡¹
3. æž„é€ FPæ ‘

**é¢‘ç¹æ¨¡å¼æŒ–æŽ˜**ï¼š

1. ä»ŽFPæ ‘ä¸­æå–æ¡ä»¶æ¨¡å¼åŸº
2. æž„é€ æ¡ä»¶FPæ ‘
3. é€’å½’æŒ–æŽ˜é¢‘ç¹æ¨¡å¼

### 1.3 å¼‚å¸¸æ£€æµ‹æ•°å­¦

#### 1.3.1 ç»Ÿè®¡æ–¹æ³•

**Z-scoreæ–¹æ³•**ï¼š
$$z_i = \frac{x_i - \mu}{\sigma}$$

å…¶ä¸­$\mu$æ˜¯å‡å€¼ï¼Œ$\sigma$æ˜¯æ ‡å‡†å·®ã€‚

**IQRæ–¹æ³•**ï¼š
$$Q_1 = \text{25th percentile}$$
$$Q_3 = \text{75th percentile}$$
$$\text{IQR} = Q_3 - Q_1$$
$$\text{Lower bound} = Q_1 - 1.5 \times \text{IQR}$$
$$\text{Upper bound} = Q_3 + 1.5 \times \text{IQR}$$

#### 1.3.2 åŸºäºŽè·ç¦»çš„æ–¹æ³•

**k-æœ€è¿‘é‚»**ï¼š
å¯¹äºŽç‚¹$x$ï¼Œè®¡ç®—åˆ°å…¶$k$ä¸ªæœ€è¿‘é‚»çš„å¹³å‡è·ç¦»ï¼š
$$d_k(x) = \frac{1}{k} \sum_{i=1}^{k} d(x, x_i)$$

**å±€éƒ¨å¼‚å¸¸å› å­ï¼ˆLOFï¼‰**ï¼š
$$\text{LOF}(x) = \frac{\sum_{y \in N_k(x)} \frac{\text{reach-dist}_k(y, x)}{\text{reach-dist}_k(x, y)}}{|N_k(x)|}$$

å…¶ä¸­$\text{reach-dist}_k(x, y) = \max\{d_k(x), d(x, y)\}$ã€‚

#### 1.3.3 åŸºäºŽå¯†åº¦çš„æ–¹æ³•

**å±€éƒ¨å¯†åº¦**ï¼š
$$\text{local-density}(x) = \frac{1}{\frac{1}{|N_k(x)|} \sum_{y \in N_k(x)} d(x, y)}$$

**å¼‚å¸¸åˆ†æ•°**ï¼š
$$\text{anomaly-score}(x) = \frac{\text{local-density}(x)}{\frac{1}{|N_k(x)|} \sum_{y \in N_k(x)} \text{local-density}(y)}$$

## 2. æ•°æ®å¯è§†åŒ–æ•°å­¦ç†è®º

### 2.1 é™ç»´æŠ€æœ¯æ•°å­¦

#### 2.1.1 ä¸»æˆåˆ†åˆ†æžï¼ˆPCAï¼‰

**åæ–¹å·®çŸ©é˜µ**ï¼š
$$C = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})(x_i - \bar{x})^T$$

**ç‰¹å¾å€¼åˆ†è§£**ï¼š
$$C = V \Lambda V^T$$

å…¶ä¸­$V$æ˜¯ç‰¹å¾å‘é‡çŸ©é˜µï¼Œ$\Lambda$æ˜¯ç‰¹å¾å€¼å¯¹è§’çŸ©é˜µã€‚

**æŠ•å½±**ï¼š
$$y_i = V^T x_i$$

å…¶ä¸­$y_i$æ˜¯é™ç»´åŽçš„æ•°æ®ã€‚

#### 2.1.2 çº¿æ€§åˆ¤åˆ«åˆ†æžï¼ˆLDAï¼‰

**ç±»é—´æ•£åº¦çŸ©é˜µ**ï¼š
$$S_B = \sum_{i=1}^{c} n_i (\mu_i - \mu)(\mu_i - \mu)^T$$

**ç±»å†…æ•£åº¦çŸ©é˜µ**ï¼š
$$S_W = \sum_{i=1}^{c} \sum_{x \in C_i} (x - \mu_i)(x - \mu_i)^T$$

**ç›®æ ‡å‡½æ•°**ï¼š
$$\max_w \frac{w^T S_B w}{w^T S_W w}$$

**è§£**ï¼š
$$w = S_W^{-1} (\mu_1 - \mu_2)$$

#### 2.1.3 t-SNE

**ç›¸ä¼¼åº¦è®¡ç®—**ï¼š
$$p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|x_i - x_k\|^2 / 2\sigma_i^2)}$$

**ä½Žç»´ç›¸ä¼¼åº¦**ï¼š
$$q_{ij} = \frac{(1 + \|y_i - y_j\|^2)^{-1}}{\sum_{k \neq l} (1 + \|y_k - y_l\|^2)^{-1}}$$

**KLæ•£åº¦**ï¼š
$$C = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}}$$

### 2.2 æµå½¢å­¦ä¹ æ•°å­¦

#### 2.2.1 å±€éƒ¨çº¿æ€§åµŒå…¥ï¼ˆLLEï¼‰

**é‡æž„æƒé‡**ï¼š
$$\min_W \sum_{i=1}^{n} \|x_i - \sum_{j \in N_i} W_{ij} x_j\|^2$$

çº¦æŸæ¡ä»¶ï¼š
$$\sum_{j \in N_i} W_{ij} = 1$$

**ä½Žç»´åµŒå…¥**ï¼š
$$\min_Y \sum_{i=1}^{n} \|y_i - \sum_{j \in N_i} W_{ij} y_j\|^2$$

#### 2.2.2 ç­‰è·æ˜ å°„ï¼ˆIsomapï¼‰

**æµ‹åœ°è·ç¦»**ï¼š
ä½¿ç”¨æœ€çŸ­è·¯å¾„ç®—æ³•è®¡ç®—æµ‹åœ°è·ç¦»ã€‚

**å¤šç»´ç¼©æ”¾ï¼ˆMDSï¼‰**ï¼š
$$\min_Y \sum_{i,j} (d_{ij} - \|y_i - y_j\|)^2$$

å…¶ä¸­$d_{ij}$æ˜¯æµ‹åœ°è·ç¦»ã€‚

#### 2.2.3 æ‹‰æ™®æ‹‰æ–¯ç‰¹å¾æ˜ å°„

**æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ**ï¼š
$$L = D - W$$

å…¶ä¸­$W$æ˜¯ç›¸ä¼¼åº¦çŸ©é˜µï¼Œ$D$æ˜¯åº¦çŸ©é˜µã€‚

**ç‰¹å¾å€¼é—®é¢˜**ï¼š
$$L f = \lambda D f$$

**åµŒå…¥**ï¼š
ä½¿ç”¨æœ€å°çš„$k$ä¸ªéžé›¶ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚

### 2.3 æ‹“æ‰‘æ•°æ®åˆ†æžæ•°å­¦

#### 2.3.1 æŒä¹…åŒè°ƒ

**å•çº¯å¤å½¢**ï¼š

- 0-å•çº¯å½¢ï¼šç‚¹
- 1-å•çº¯å½¢ï¼šè¾¹
- 2-å•çº¯å½¢ï¼šä¸‰è§’å½¢
- $k$-å•çº¯å½¢ï¼š$k+1$ä¸ªç‚¹çš„å‡¸åŒ…

**è¾¹ç•Œç®—å­**ï¼š
$$\partial_k : C_k \to C_{k-1}$$

å…¶ä¸­$C_k$æ˜¯$k$-é“¾ç¾¤ã€‚

**åŒè°ƒç¾¤**ï¼š
$$H_k = \frac{\ker \partial_k}{\text{im } \partial_{k+1}}$$

#### 2.3.2 æŒä¹…å›¾

**æŒä¹…æ€§**ï¼š
$$(\text{birth}, \text{death})$$

å…¶ä¸­birthæ˜¯ç‰¹å¾å‡ºçŽ°çš„æ—¶åˆ»ï¼Œdeathæ˜¯ç‰¹å¾æ¶ˆå¤±çš„æ—¶åˆ»ã€‚

**æŒä¹…æ€§å›¾**ï¼š
åœ¨å¹³é¢ä¸Šç»˜åˆ¶ç‚¹$(\text{birth}, \text{death})$ã€‚

#### 2.3.3 Mapperç®—æ³•

**è¦†ç›–**ï¼š
å°†æ•°æ®ç©ºé—´åˆ’åˆ†ä¸ºé‡å çš„çƒã€‚

**èšç±»**ï¼š
åœ¨æ¯ä¸ªçƒå†…è¿›è¡Œèšç±»ã€‚

**ç¥žç»å›¾**ï¼š
å°†èšç±»ä½œä¸ºèŠ‚ç‚¹ï¼Œé‡å å…³ç³»ä½œä¸ºè¾¹ã€‚

## 3. å¤§æ•°æ®å¤„ç†æ•°å­¦ç†è®º

### 3.1 åˆ†å¸ƒå¼è®¡ç®—æ•°å­¦

#### 3.1.1 MapReduceæ¨¡åž‹

**Mapå‡½æ•°**ï¼š
$$(k_1, v_1) \to \text{list}(k_2, v_2)$$

**Reduceå‡½æ•°**ï¼š
$$(k_2, \text{list}(v_2)) \to \text{list}(k_3, v_3)$$

**è®¡ç®—å¤æ‚åº¦**ï¼š

- æ—¶é—´å¤æ‚åº¦ï¼š$O(n/p + \log p)$
- ç©ºé—´å¤æ‚åº¦ï¼š$O(n)$

å…¶ä¸­$n$æ˜¯æ•°æ®å¤§å°ï¼Œ$p$æ˜¯å¤„ç†å™¨æ•°é‡ã€‚

#### 3.1.2 åˆ†å¸ƒå¼ä¼˜åŒ–

**æ¢¯åº¦ä¸‹é™**ï¼š
$$\theta_{t+1} = \theta_t - \alpha \frac{1}{n} \sum_{i=1}^{n} \nabla f_i(\theta_t)$$

**éšæœºæ¢¯åº¦ä¸‹é™**ï¼š
$$\theta_{t+1} = \theta_t - \alpha \nabla f_{i_t}(\theta_t)$$

å…¶ä¸­$i_t$æ˜¯éšæœºé€‰æ‹©çš„æ ·æœ¬ç´¢å¼•ã€‚

#### 3.1.3 ä¸€è‡´æ€§ç®—æ³•

**å¹³å‡ä¸€è‡´æ€§**ï¼š
$$x_i(t+1) = \sum_{j \in N_i} W_{ij} x_j(t)$$

å…¶ä¸­$W$æ˜¯æƒé‡çŸ©é˜µï¼Œæ»¡è¶³ï¼š
$$\sum_{j} W_{ij} = 1$$

**æ”¶æ•›æ¡ä»¶**ï¼š
$$\lim_{t \to \infty} x_i(t) = \frac{1}{n} \sum_{j=1}^{n} x_j(0)$$

### 3.2 æµå¤„ç†æ•°å­¦

#### 3.2.1 æ»‘åŠ¨çª—å£

**æ—¶é—´çª—å£**ï¼š
$$W(t) = \{x_i : t - w \leq t_i \leq t\}$$

å…¶ä¸­$w$æ˜¯çª—å£å¤§å°ã€‚

**è®¡æ•°çª—å£**ï¼š
$$W(t) = \{x_i : i \in [t-k+1, t]\}$$

å…¶ä¸­$k$æ˜¯çª—å£å¤§å°ã€‚

#### 3.2.2 æµç®—æ³•

**Count-Min Sketch**ï¼š
$$h_i(x) = (a_i x + b_i) \bmod m$$

å…¶ä¸­$a_i, b_i$æ˜¯éšæœºæ•°ã€‚

**ä¼°è®¡é¢‘çŽ‡**ï¼š
$$\hat{f}(x) = \min_i C[i, h_i(x)]$$

å…¶ä¸­$C$æ˜¯è®¡æ•°å™¨çŸ©é˜µã€‚

#### 3.2.3 æµèšç±»

**BIRCHç®—æ³•**ï¼š

- æž„å»ºCFæ ‘
- èšç±»ç‰¹å¾ï¼š$(N, LS, SS)$
- å…¶ä¸­$N$æ˜¯ç‚¹æ•°ï¼Œ$LS$æ˜¯çº¿æ€§å’Œï¼Œ$SS$æ˜¯å¹³æ–¹å’Œ

**æ›´æ–°è§„åˆ™**ï¼š
$$CF_1 + CF_2 = (N_1 + N_2, LS_1 + LS_2, SS_1 + SS_2)$$

### 3.3 å›¾è®¡ç®—æ•°å­¦

#### 3.3.1 PageRankç®—æ³•

**PageRankæ–¹ç¨‹**ï¼š
$$PR(p) = \frac{1-d}{N} + d \sum_{q \in B_p} \frac{PR(q)}{C(q)}$$

å…¶ä¸­ï¼š

- $d$æ˜¯é˜»å°¼å› å­
- $N$æ˜¯é¡µé¢æ€»æ•°
- $B_p$æ˜¯æŒ‡å‘é¡µé¢$p$çš„é¡µé¢é›†åˆ
- $C(q)$æ˜¯é¡µé¢$q$çš„å‡ºé“¾æ•°

**è¿­ä»£æ±‚è§£**ï¼š
$$PR^{(t+1)} = (1-d) \frac{1}{N} \mathbf{1} + d M PR^{(t)}$$

å…¶ä¸­$M$æ˜¯è½¬ç§»çŸ©é˜µã€‚

#### 3.3.2 ç¤¾åŒºæ£€æµ‹

**æ¨¡å—åº¦**ï¼š
$$Q = \frac{1}{2m} \sum_{ij} \left[A_{ij} - \frac{k_i k_j}{2m}\right] \delta(c_i, c_j)$$

å…¶ä¸­ï¼š

- $A_{ij}$æ˜¯é‚»æŽ¥çŸ©é˜µ
- $k_i$æ˜¯èŠ‚ç‚¹$i$çš„åº¦
- $m$æ˜¯æ€»è¾¹æ•°
- $c_i$æ˜¯èŠ‚ç‚¹$i$çš„ç¤¾åŒºæ ‡ç­¾

#### 3.3.3 æœ€çŸ­è·¯å¾„

**Dijkstraç®—æ³•**ï¼š

1. åˆå§‹åŒ–è·ç¦»ï¼š$d(s) = 0$ï¼Œ$d(v) = \infty$ for $v \neq s$
2. é€‰æ‹©æœªè®¿é—®çš„æœ€å°è·ç¦»èŠ‚ç‚¹$u$
3. æ›´æ–°é‚»å±…è·ç¦»ï¼š$d(v) = \min(d(v), d(u) + w(u,v))$
4. é‡å¤æ­¥éª¤2-3ç›´åˆ°æ‰€æœ‰èŠ‚ç‚¹è¢«è®¿é—®

**Floyd-Warshallç®—æ³•**ï¼š
$$d_{ij}^{(k)} = \min(d_{ij}^{(k-1)}, d_{ik}^{(k-1)} + d_{kj}^{(k-1)})$$

## 4. ç»Ÿè®¡å­¦ä¹ æ•°å­¦ç†è®º

### 4.1 ç›‘ç£å­¦ä¹ æ•°å­¦

#### 4.1.1 çº¿æ€§å›žå½’

**æ¨¡åž‹**ï¼š
$$y = X\beta + \epsilon$$

å…¶ä¸­$\epsilon \sim \mathcal{N}(0, \sigma^2 I)$ã€‚

**æœ€å°äºŒä¹˜ä¼°è®¡**ï¼š
$$\hat{\beta} = (X^T X)^{-1} X^T y$$

**é¢„æµ‹**ï¼š
$$\hat{y} = X\hat{\beta}$$

#### 4.1.2 é€»è¾‘å›žå½’

**æ¨¡åž‹**ï¼š
$$P(y=1|x) = \frac{1}{1 + e^{-x^T \beta}}$$

**å¯¹æ•°ä¼¼ç„¶**ï¼š
$$L(\beta) = \sum_{i=1}^{n} [y_i \log p_i + (1-y_i) \log(1-p_i)]$$

å…¶ä¸­$p_i = P(y_i=1|x_i)$ã€‚

**æ¢¯åº¦**ï¼š
$$\nabla L(\beta) = X^T(y - p)$$

å…¶ä¸­$p = [p_1, \ldots, p_n]^T$ã€‚

#### 4.1.3 æ”¯æŒå‘é‡æœº

**åŽŸå§‹é—®é¢˜**ï¼š
$$\min_{w,b} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^{n} \xi_i$$

çº¦æŸæ¡ä»¶ï¼š
$$y_i(w^T x_i + b) \geq 1 - \xi_i$$
$$\xi_i \geq 0$$

**å¯¹å¶é—®é¢˜**ï¼š
$$\max_{\alpha} \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j x_i^T x_j$$

çº¦æŸæ¡ä»¶ï¼š
$$0 \leq \alpha_i \leq C$$
$$\sum_{i=1}^{n} \alpha_i y_i = 0$$

### 4.2 æ— ç›‘ç£å­¦ä¹ æ•°å­¦

#### 4.2.1 ä¸»æˆåˆ†åˆ†æž

**ç›®æ ‡å‡½æ•°**ï¼š
$$\max_{w} \frac{w^T S w}{w^T w}$$

å…¶ä¸­$S$æ˜¯åæ–¹å·®çŸ©é˜µã€‚

**ç‰¹å¾å€¼é—®é¢˜**ï¼š
$$S w = \lambda w$$

**ä¸»æˆåˆ†**ï¼š
$$y_i = w_i^T x$$

å…¶ä¸­$w_i$æ˜¯ç¬¬$i$ä¸ªç‰¹å¾å‘é‡ã€‚

#### 4.2.2 ç‹¬ç«‹æˆåˆ†åˆ†æž

**æ¨¡åž‹**ï¼š
$$x = As$$

å…¶ä¸­$A$æ˜¯æ··åˆçŸ©é˜µï¼Œ$s$æ˜¯ç‹¬ç«‹æˆåˆ†ã€‚

**ç›®æ ‡å‡½æ•°**ï¼š
$$\max_W \sum_{i=1}^{n} \log p_i(W_i^T x)$$

å…¶ä¸­$W = A^{-1}$ã€‚

#### 4.2.3 è‡ªç¼–ç å™¨

**ç¼–ç å™¨**ï¼š
$$h = f(W_1 x + b_1)$$

**è§£ç å™¨**ï¼š
$$\hat{x} = g(W_2 h + b_2)$$

**ç›®æ ‡å‡½æ•°**ï¼š
$$\min_{W_1, W_2, b_1, b_2} \|x - \hat{x}\|^2$$

### 4.3 å¼ºåŒ–å­¦ä¹ æ•°å­¦

#### 4.3.1 Qå­¦ä¹ 

**Qå‡½æ•°æ›´æ–°**ï¼š
$$Q(s, a) \leftarrow Q(s, a) + \alpha[r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$$

å…¶ä¸­ï¼š

- $\alpha$æ˜¯å­¦ä¹ çŽ‡
- $\gamma$æ˜¯æŠ˜æ‰£å› å­
- $r$æ˜¯å¥–åŠ±
- $s'$æ˜¯ä¸‹ä¸€ä¸ªçŠ¶æ€

#### 4.3.2 ç­–ç•¥æ¢¯åº¦

**ç›®æ ‡å‡½æ•°**ï¼š
$$J(\theta) = \mathbb{E}_{\pi_\theta}[\sum_{t=0}^{\infty} \gamma^t r_t]$$

**ç­–ç•¥æ¢¯åº¦å®šç†**ï¼š
$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) Q^\pi(s, a)]$$

#### 4.3.3 Actor-Critic

**Actoræ›´æ–°**ï¼š
$$\theta \leftarrow \theta + \alpha_\theta \nabla_\theta \log \pi_\theta(a|s) \delta$$

**Criticæ›´æ–°**ï¼š
$$w \leftarrow w + \alpha_w \delta \nabla_w V_w(s)$$

å…¶ä¸­$\delta = r + \gamma V_w(s') - V_w(s)$æ˜¯TDè¯¯å·®ã€‚

## 5. æŠ€æœ¯å®žçŽ°

### 5.1 Pythonå®žçŽ°

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
import pandas as pd

# æ•°æ®æŒ–æŽ˜å®žçŽ°
class DataMining:
    def __init__(self):
        pass
    
    def kmeans_clustering(self, data, k, max_iter=100):
        """K-meansèšç±»"""
        # éšæœºåˆå§‹åŒ–èšç±»ä¸­å¿ƒ
        n_samples, n_features = data.shape
        centroids = data[np.random.choice(n_samples, k, replace=False)]
        
        for _ in range(max_iter):
            # åˆ†é…ç‚¹åˆ°æœ€è¿‘çš„ä¸­å¿ƒ
            distances = np.sqrt(((data - centroids[:, np.newaxis])**2).sum(axis=2))
            labels = np.argmin(distances, axis=0)
            
            # æ›´æ–°èšç±»ä¸­å¿ƒ
            new_centroids = np.array([data[labels == i].mean(axis=0) 
                                     for i in range(k)])
            
            # æ£€æŸ¥æ”¶æ•›
            if np.allclose(centroids, new_centroids):
                break
            centroids = new_centroids
        
        return labels, centroids
    
    def dbscan_clustering(self, data, eps, min_samples):
        """DBSCANèšç±»"""
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(data)
        return labels
    
    def association_rules(self, transactions, min_support=0.1, min_confidence=0.5):
        """å…³è”è§„åˆ™æŒ–æŽ˜"""
        # è®¡ç®—é¡¹é›†æ”¯æŒåº¦
        item_counts = {}
        for transaction in transactions:
            for item in transaction:
                item_counts[item] = item_counts.get(item, 0) + 1
        
        n_transactions = len(transactions)
        frequent_items = {item: count/n_transactions 
                         for item, count in item_counts.items() 
                         if count/n_transactions >= min_support}
        
        # ç”Ÿæˆå…³è”è§„åˆ™
        rules = []
        for item1 in frequent_items:
            for item2 in frequent_items:
                if item1 != item2:
                    # è®¡ç®—æ”¯æŒåº¦å’Œç½®ä¿¡åº¦
                    support_both = sum(1 for t in transactions 
                                     if item1 in t and item2 in t) / n_transactions
                    support_item1 = frequent_items[item1]
                    confidence = support_both / support_item1
                    
                    if confidence >= min_confidence:
                        rules.append((item1, item2, support_both, confidence))
        
        return rules

# æ•°æ®å¯è§†åŒ–å®žçŽ°
class DataVisualization:
    def __init__(self):
        pass
    
    def pca_reduction(self, data, n_components=2):
        """PCAé™ç»´"""
        pca = PCA(n_components=n_components)
        reduced_data = pca.fit_transform(data)
        explained_variance = pca.explained_variance_ratio_
        return reduced_data, explained_variance
    
    def tsne_reduction(self, data, n_components=2, perplexity=30):
        """t-SNEé™ç»´"""
        tsne = TSNE(n_components=n_components, perplexity=perplexity)
        reduced_data = tsne.fit_transform(data)
        return reduced_data
    
    def lda_reduction(self, data, labels, n_components=2):
        """LDAé™ç»´"""
        from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
        lda = LinearDiscriminantAnalysis(n_components=n_components)
        reduced_data = lda.fit_transform(data, labels)
        return reduced_data

# å¤§æ•°æ®å¤„ç†å®žçŽ°
class BigDataProcessing:
    def __init__(self):
        pass
    
    def map_reduce_example(self, data, map_func, reduce_func):
        """MapReduceç¤ºä¾‹"""
        # Mapé˜¶æ®µ
        mapped_data = [map_func(item) for item in data]
        
        # Shuffleé˜¶æ®µï¼ˆç®€åŒ–ï¼‰
        grouped_data = {}
        for key, value in mapped_data:
            if key not in grouped_data:
                grouped_data[key] = []
            grouped_data[key].append(value)
        
        # Reduceé˜¶æ®µ
        result = {}
        for key, values in grouped_data.items():
            result[key] = reduce_func(key, values)
        
        return result
    
    def streaming_algorithm(self, data_stream, window_size=100):
        """æµå¤„ç†ç®—æ³•"""
        window = []
        results = []
        
        for item in data_stream:
            window.append(item)
            if len(window) > window_size:
                window.pop(0)
            
            # è®¡ç®—çª—å£ç»Ÿè®¡é‡
            if len(window) > 0:
                mean_val = np.mean(window)
                results.append(mean_val)
        
        return results

# ç»Ÿè®¡å­¦ä¹ å®žçŽ°
class StatisticalLearning:
    def __init__(self):
        pass
    
    def linear_regression(self, X, y):
        """çº¿æ€§å›žå½’"""
        # æ·»åŠ åç½®é¡¹
        X_b = np.c_[np.ones((X.shape[0], 1)), X]
        
        # æœ€å°äºŒä¹˜è§£
        theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
        return theta
    
    def logistic_regression(self, X, y, learning_rate=0.01, n_iterations=1000):
        """é€»è¾‘å›žå½’"""
        n_samples, n_features = X.shape
        theta = np.zeros(n_features)
        
        for _ in range(n_iterations):
            # è®¡ç®—é¢„æµ‹æ¦‚çŽ‡
            z = X.dot(theta)
            h = 1 / (1 + np.exp(-z))
            
            # è®¡ç®—æ¢¯åº¦
            gradient = X.T.dot(h - y) / n_samples
            
            # æ›´æ–°å‚æ•°
            theta -= learning_rate * gradient
        
        return theta
    
    def svm_classifier(self, X, y, C=1.0):
        """æ”¯æŒå‘é‡æœº"""
        from sklearn.svm import SVC
        svm = SVC(kernel='linear', C=C)
        svm.fit(X, y)
        return svm

# ä½¿ç”¨ç¤ºä¾‹
# ç”Ÿæˆç¤ºä¾‹æ•°æ®
np.random.seed(42)
n_samples = 300
n_features = 2

# ç”Ÿæˆèšç±»æ•°æ®
centers = [[0, 0], [2, 2], [4, 0]]
data = np.vstack([np.random.normal(center, 0.5, (n_samples//3, n_features)) 
                  for center in centers])

# æ•°æ®æŒ–æŽ˜
dm = DataMining()
labels, centroids = dm.kmeans_clustering(data, k=3)
silhouette_avg = silhouette_score(data, labels)
print(f"K-meansèšç±»è½®å»“ç³»æ•°: {silhouette_avg:.3f}")

# æ•°æ®å¯è§†åŒ–
dv = DataVisualization()
pca_data, explained_variance = dv.pca_reduction(data)
tsne_data = dv.tsne_reduction(data)

# å¯è§†åŒ–ç»“æžœ
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis')
plt.title('åŽŸå§‹æ•°æ®èšç±»')
plt.xlabel('ç‰¹å¾1')
plt.ylabel('ç‰¹å¾2')

plt.subplot(1, 3, 2)
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=labels, cmap='viridis')
plt.title('PCAé™ç»´')
plt.xlabel('ä¸»æˆåˆ†1')
plt.ylabel('ä¸»æˆåˆ†2')

plt.subplot(1, 3, 3)
plt.scatter(tsne_data[:, 0], tsne_data[:, 1], c=labels, cmap='viridis')
plt.title('t-SNEé™ç»´')
plt.xlabel('t-SNE1')
plt.ylabel('t-SNE2')

plt.tight_layout()
plt.show()

# å¤§æ•°æ®å¤„ç†ç¤ºä¾‹
bdp = BigDataProcessing()

# MapReduceç¤ºä¾‹
data_stream = np.random.normal(0, 1, 1000)
streaming_result = bdp.streaming_algorithm(data_stream, window_size=50)

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(data_stream[:100])
plt.title('åŽŸå§‹æ•°æ®æµ')
plt.xlabel('æ—¶é—´')
plt.ylabel('å€¼')

plt.subplot(1, 2, 2)
plt.plot(streaming_result)
plt.title('æ»‘åŠ¨çª—å£å¹³å‡')
plt.xlabel('æ—¶é—´')
plt.ylabel('å¹³å‡å€¼')

plt.tight_layout()
plt.show()
```

### 5.2 åˆ†å¸ƒå¼è®¡ç®—å®žçŽ°

```python
import numpy as np
from multiprocessing import Pool
import threading
import queue

# åˆ†å¸ƒå¼è®¡ç®—å®žçŽ°
class DistributedComputing:
    def __init__(self, n_workers=4):
        self.n_workers = n_workers
    
    def parallel_map(self, func, data):
        """å¹¶è¡ŒMapæ“ä½œ"""
        with Pool(self.n_workers) as pool:
            results = pool.map(func, data)
        return results
    
    def parallel_reduce(self, data, reduce_func):
        """å¹¶è¡ŒReduceæ“ä½œ"""
        # åˆ†å—å¤„ç†
        chunk_size = len(data) // self.n_workers
        chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]
        
        # å¹¶è¡Œå¤„ç†
        with Pool(self.n_workers) as pool:
            chunk_results = pool.map(reduce_func, chunks)
        
        # åˆå¹¶ç»“æžœ
        final_result = chunk_results[0]
        for result in chunk_results[1:]:
            final_result = reduce_func([final_result, result])
        
        return final_result
    
    def streaming_processor(self, data_stream, window_size=100):
        """æµå¤„ç†å™¨"""
        window = queue.Queue(maxsize=window_size)
        results = []
        
        def process_window():
            while True:
                try:
                    item = data_stream.get(timeout=1)
                    if window.full():
                        window.get()  # ç§»é™¤æœ€æ—§çš„é¡¹ç›®
                    window.put(item)
                    
                    # å¤„ç†çª—å£æ•°æ®
                    window_data = list(window.queue)
                    if len(window_data) > 0:
                        result = np.mean(window_data)
                        results.append(result)
                except queue.Empty:
                    break
        
        # å¯åŠ¨å¤„ç†çº¿ç¨‹
        processor_thread = threading.Thread(target=process_window)
        processor_thread.start()
        processor_thread.join()
        
        return results

# å›¾è®¡ç®—å®žçŽ°
class GraphComputing:
    def __init__(self):
        pass
    
    def pagerank(self, adjacency_matrix, damping=0.85, max_iter=100, tol=1e-6):
        """PageRankç®—æ³•"""
        n_nodes = len(adjacency_matrix)
        
        # åˆå§‹åŒ–PageRankå€¼
        pr = np.ones(n_nodes) / n_nodes
        
        # è®¡ç®—è½¬ç§»çŸ©é˜µ
        out_degrees = adjacency_matrix.sum(axis=1)
        transition_matrix = np.zeros((n_nodes, n_nodes))
        
        for i in range(n_nodes):
            if out_degrees[i] > 0:
                transition_matrix[i] = adjacency_matrix[i] / out_degrees[i]
        
        # è¿­ä»£è®¡ç®—
        for _ in range(max_iter):
            new_pr = (1 - damping) / n_nodes + damping * transition_matrix.T.dot(pr)
            
            if np.linalg.norm(new_pr - pr) < tol:
                break
            pr = new_pr
        
        return pr
    
    def shortest_path(self, adjacency_matrix, start_node):
        """Dijkstraæœ€çŸ­è·¯å¾„ç®—æ³•"""
        n_nodes = len(adjacency_matrix)
        distances = np.full(n_nodes, np.inf)
        distances[start_node] = 0
        visited = set()
        
        while len(visited) < n_nodes:
            # æ‰¾åˆ°æœªè®¿é—®çš„æœ€å°è·ç¦»èŠ‚ç‚¹
            unvisited = [i for i in range(n_nodes) if i not in visited]
            current = min(unvisited, key=lambda x: distances[x])
            visited.add(current)
            
            # æ›´æ–°é‚»å±…è·ç¦»
            for neighbor in range(n_nodes):
                if (adjacency_matrix[current, neighbor] > 0 and 
                    neighbor not in visited):
                    new_distance = (distances[current] + 
                                  adjacency_matrix[current, neighbor])
                    if new_distance < distances[neighbor]:
                        distances[neighbor] = new_distance
        
        return distances

# ä½¿ç”¨ç¤ºä¾‹
# åˆ†å¸ƒå¼è®¡ç®—
dc = DistributedComputing(n_workers=4)

# å¹¶è¡ŒMapç¤ºä¾‹
def square(x):
    return x**2

data = list(range(1000))
squared_data = dc.parallel_map(square, data)
print(f"å¹¶è¡Œè®¡ç®—å¹³æ–¹å’Œ: {sum(squared_data)}")

# å›¾è®¡ç®—ç¤ºä¾‹
gc = GraphComputing()

# åˆ›å»ºç¤ºä¾‹å›¾
n_nodes = 5
adjacency_matrix = np.array([
    [0, 1, 1, 0, 0],
    [1, 0, 1, 1, 0],
    [1, 1, 0, 0, 1],
    [0, 1, 0, 0, 1],
    [0, 0, 1, 1, 0]
])

# PageRankè®¡ç®—
pagerank_scores = gc.pagerank(adjacency_matrix)
print("PageRankåˆ†æ•°:", pagerank_scores)

# æœ€çŸ­è·¯å¾„è®¡ç®—
shortest_distances = gc.shortest_path(adjacency_matrix, start_node=0)
print("ä»ŽèŠ‚ç‚¹0åˆ°å„èŠ‚ç‚¹çš„æœ€çŸ­è·ç¦»:", shortest_distances)

# å¯è§†åŒ–å›¾
import networkx as nx

G = nx.from_numpy_array(adjacency_matrix)
pos = nx.spring_layout(G)

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
nx.draw(G, pos, with_labels=True, node_color='lightblue', 
        node_size=500, font_size=16, font_weight='bold')
plt.title('å›¾ç»“æž„')

plt.subplot(1, 2, 2)
node_colors = pagerank_scores
nx.draw(G, pos, with_labels=True, node_color=node_colors, 
        node_size=500, font_size=16, font_weight='bold', cmap=plt.cm.viridis)
plt.title('PageRankåˆ†æ•°')
plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.viridis))

plt.tight_layout()
plt.show()
```

## 6. åº”ç”¨æ¡ˆä¾‹

### 6.1 æ•°æ®æŒ–æŽ˜åº”ç”¨

**å®¢æˆ·ç»†åˆ†**ï¼š

- ä½¿ç”¨èšç±»åˆ†æžè¯†åˆ«å®¢æˆ·ç¾¤ä½“
- åŸºäºŽè´­ä¹°è¡Œä¸ºè¿›è¡Œå®¢æˆ·åˆ†ç±»
- åˆ¶å®šä¸ªæ€§åŒ–è¥é”€ç­–ç•¥

### 6.2 æ•°æ®å¯è§†åŒ–åº”ç”¨

**æŽ¢ç´¢æ€§æ•°æ®åˆ†æž**ï¼š

- ä½¿ç”¨PCAé™ç»´åˆ†æžé«˜ç»´æ•°æ®
- é€šè¿‡t-SNEå¯è§†åŒ–æ•°æ®åˆ†å¸ƒ
- å‘çŽ°æ•°æ®ä¸­çš„éšè—æ¨¡å¼

### 6.3 å¤§æ•°æ®å¤„ç†åº”ç”¨

**å®žæ—¶æ•°æ®åˆ†æž**ï¼š

- æµå¤„ç†å®žæ—¶äº¤æ˜“æ•°æ®
- åˆ†å¸ƒå¼è®¡ç®—å¤„ç†å¤§è§„æ¨¡æ•°æ®
- å›¾è®¡ç®—åˆ†æžç¤¾äº¤ç½‘ç»œ

## 7. å‰æ²¿å‘å±•

### 7.1 æ·±åº¦å­¦ä¹ ä¸Žæ•°æ®ç§‘å­¦

**æ·±åº¦èšç±»**ï¼š

- è‡ªç¼–ç å™¨ç”¨äºŽé™ç»´èšç±»
- æ·±åº¦åµŒå…¥èšç±»ç®—æ³•
- ç«¯åˆ°ç«¯èšç±»å­¦ä¹ 

### 7.2 å›¾ç¥žç»ç½‘ç»œ

**å›¾å·ç§¯ç½‘ç»œ**ï¼š

- å›¾å·ç§¯å±‚è®¾è®¡
- å›¾æ³¨æ„åŠ›æœºåˆ¶
- å›¾ç¥žç»ç½‘ç»œåº”ç”¨

### 7.3 è”é‚¦å­¦ä¹ 

**åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ **ï¼š

- è”é‚¦å¹³å‡ç®—æ³•
- å·®åˆ†éšç§ä¿æŠ¤
- è”é‚¦å­¦ä¹ ä¼˜åŒ–

## 8. æ€»ç»“ä¸Žå±•æœ›

### 8.1 æ ¸å¿ƒè¦ç‚¹æ€»ç»“

1. **æ•°æ®æŒ–æŽ˜æ•°å­¦åŸºç¡€**ï¼š
   - èšç±»åˆ†æžçš„æ•°å­¦åŽŸç†å’Œç®—æ³•
   - å…³è”è§„åˆ™æŒ–æŽ˜çš„ç»Ÿè®¡æ–¹æ³•
   - å¼‚å¸¸æ£€æµ‹çš„æ•°å­¦æ¨¡åž‹

2. **æ•°æ®å¯è§†åŒ–æ•°å­¦ç†è®º**ï¼š
   - é™ç»´æŠ€æœ¯çš„æ•°å­¦åŽŸç†
   - æµå½¢å­¦ä¹ çš„å‡ ä½•æ–¹æ³•
   - æ‹“æ‰‘æ•°æ®åˆ†æžçš„ä»£æ•°å·¥å…·

3. **å¤§æ•°æ®å¤„ç†æ•°å­¦åŽŸç†**ï¼š
   - åˆ†å¸ƒå¼è®¡ç®—çš„æ•°å­¦æ¨¡åž‹
   - æµå¤„ç†çš„ç®—æ³•è®¾è®¡
   - å›¾è®¡ç®—çš„æ•°å­¦ç†è®º

4. **ç»Ÿè®¡å­¦ä¹ æ•°å­¦æ–¹æ³•**ï¼š
   - ç›‘ç£å­¦ä¹ çš„æ•°å­¦åŸºç¡€
   - æ— ç›‘ç£å­¦ä¹ çš„ç»Ÿè®¡ç†è®º
   - å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–æ–¹æ³•

### 8.2 å‘å±•è¶‹åŠ¿

1. **æŠ€æœ¯å‘å±•**ï¼š
   - æ·±åº¦å­¦ä¹ ä¸Žæ•°æ®ç§‘å­¦çš„èžåˆ
   - å›¾ç¥žç»ç½‘ç»œçš„å‘å±•
   - è”é‚¦å­¦ä¹ çš„åº”ç”¨

2. **æ–¹æ³•åˆ›æ–°**ï¼š
   - å¯è§£é‡Šæœºå™¨å­¦ä¹ 
   - è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ 
   - å› æžœæŽ¨æ–­æ–¹æ³•

3. **åº”ç”¨æ‹“å±•**ï¼š
   - ç”Ÿç‰©ä¿¡æ¯å­¦åº”ç”¨
   - é‡‘èžç§‘æŠ€åº”ç”¨
   - ç¤¾ä¼šç§‘å­¦åº”ç”¨

### 8.3 æŒ‘æˆ˜ä¸Žæœºé‡

**ä¸»è¦æŒ‘æˆ˜**ï¼š

- å¤§è§„æ¨¡æ•°æ®çš„è®¡ç®—å¤æ‚åº¦
- æ•°æ®éšç§å’Œå®‰å…¨é—®é¢˜
- æ¨¡åž‹å¯è§£é‡Šæ€§éœ€æ±‚

**å‘å±•æœºé‡**ï¼š

- äººå·¥æ™ºèƒ½ä¸Žæ•°æ®ç§‘å­¦çš„ç»“åˆ
- è·¨å­¦ç§‘åº”ç”¨çš„æ‹“å±•
- æ–°æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨

---

## ðŸ“š å‚è€ƒæ–‡çŒ®

1. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springer.
2. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
3. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.
4. Leskovec, J., Rajaraman, A., & Ullman, J. D. (2014). Mining of Massive Datasets. Cambridge University Press.
5. Wasserman, L. (2013). All of Statistics: A Concise Course in Statistical Inference. Springer.

## ðŸ”— ç›¸å…³é“¾æŽ¥

- [æ¦‚çŽ‡è®ºåŸºç¡€](../12-åº”ç”¨æ•°å­¦/01-æ¦‚çŽ‡è®º.md)
- [ç»Ÿè®¡å­¦åŸºç¡€](../12-åº”ç”¨æ•°å­¦/02-ç»Ÿè®¡å­¦.md)
- [äººå·¥æ™ºèƒ½æ•°å­¦](../12-åº”ç”¨æ•°å­¦/07-äººå·¥æ™ºèƒ½æ•°å­¦-æ·±åŒ–ç‰ˆ.md)
- [ç½‘ç»œç§‘å­¦æ•°å­¦](../12-åº”ç”¨æ•°å­¦/09-ç½‘ç»œç§‘å­¦æ•°å­¦-æ·±åŒ–ç‰ˆ.md)

---

*æœ¬æ·±åŒ–ç‰ˆæ–‡æ¡£æ·±å…¥æŽ¢è®¨äº†æ•°æ®ç§‘å­¦çš„æ•°å­¦ç†è®ºåŸºç¡€ï¼Œä¸ºç†è§£æ•°æ®æŒ–æŽ˜ã€æ•°æ®å¯è§†åŒ–ã€å¤§æ•°æ®å¤„ç†æä¾›äº†å¼ºå¤§çš„æ•°å­¦å·¥å…·ã€‚*
