# æ•°æ®ç§‘å­¦æ•°å­¦ - æ·±åŒ–ç‰ˆ

## ğŸ“š æ¦‚è¿°

æ•°æ®ç§‘å­¦æ•°å­¦æ˜¯æ•°æ®ç§‘å­¦é¢†åŸŸçš„æ•°å­¦ç†è®ºåŸºç¡€ï¼Œæ¶µç›–äº†æ•°æ®æŒ–æ˜ã€æ•°æ®å¯è§†åŒ–ã€å¤§æ•°æ®å¤„ç†ç­‰æ ¸å¿ƒæŠ€æœ¯çš„æ•°å­¦åŸç†ã€‚æœ¬æ·±åŒ–ç‰ˆå°†æ·±å…¥æ¢è®¨æ•°æ®ç§‘å­¦çš„æ•°å­¦åŸºç¡€ï¼ŒåŒ…æ‹¬ç»Ÿè®¡å­¦ä¹ ã€æœºå™¨å­¦ä¹ ã€æ•°æ®æŒ–æ˜ç­‰æ ¸å¿ƒå†…å®¹ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

1. **æŒæ¡æ•°æ®æŒ–æ˜æ•°å­¦åŸºç¡€**ï¼šç†è§£èšç±»åˆ†æã€å…³è”è§„åˆ™æŒ–æ˜ã€å¼‚å¸¸æ£€æµ‹çš„æ•°å­¦å»ºæ¨¡
2. **æŒæ¡æ•°æ®å¯è§†åŒ–æ•°å­¦ç†è®º**ï¼šç†è§£é™ç»´æŠ€æœ¯ã€æµå½¢å­¦ä¹ ã€æ‹“æ‰‘æ•°æ®åˆ†æçš„æ•°å­¦æ–¹æ³•
3. **æŒæ¡å¤§æ•°æ®å¤„ç†æ•°å­¦åŸç†**ï¼šç†è§£åˆ†å¸ƒå¼è®¡ç®—ã€æµå¤„ç†ã€å›¾è®¡ç®—çš„æ•°å­¦ç†è®º
4. **æŒæ¡ç»Ÿè®¡å­¦ä¹ æ•°å­¦æ–¹æ³•**ï¼šç†è§£ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸºç¡€

## ğŸ“– ç›®å½•

- [æ•°æ®ç§‘å­¦æ•°å­¦ - æ·±åŒ–ç‰ˆ](#æ•°æ®ç§‘å­¦æ•°å­¦---æ·±åŒ–ç‰ˆ)
  - [ğŸ“š æ¦‚è¿°](#-æ¦‚è¿°)
  - [ğŸ¯ å­¦ä¹ ç›®æ ‡](#-å­¦ä¹ ç›®æ ‡)
  - [ğŸ“– ç›®å½•](#-ç›®å½•)
  - [1. æ•°æ®æŒ–æ˜æ•°å­¦ç†è®º](#1-æ•°æ®æŒ–æ˜æ•°å­¦ç†è®º)
    - [1.1 èšç±»åˆ†ææ•°å­¦](#11-èšç±»åˆ†ææ•°å­¦)
      - [1.1.1 K-meansèšç±»](#111-k-meansèšç±»)
      - [1.1.2 å±‚æ¬¡èšç±»](#112-å±‚æ¬¡èšç±»)
      - [1.1.3 å¯†åº¦èšç±»](#113-å¯†åº¦èšç±»)
    - [1.2 å…³è”è§„åˆ™æŒ–æ˜æ•°å­¦](#12-å…³è”è§„åˆ™æŒ–æ˜æ•°å­¦)
      - [1.2.1 æ”¯æŒåº¦å’Œç½®ä¿¡åº¦](#121-æ”¯æŒåº¦å’Œç½®ä¿¡åº¦)
      - [1.2.2 Aprioriç®—æ³•](#122-aprioriç®—æ³•)
      - [1.2.3 FP-Growthç®—æ³•](#123-fp-growthç®—æ³•)
    - [1.3 å¼‚å¸¸æ£€æµ‹æ•°å­¦](#13-å¼‚å¸¸æ£€æµ‹æ•°å­¦)
      - [1.3.1 ç»Ÿè®¡æ–¹æ³•](#131-ç»Ÿè®¡æ–¹æ³•)
      - [1.3.2 åŸºäºè·ç¦»çš„æ–¹æ³•](#132-åŸºäºè·ç¦»çš„æ–¹æ³•)
      - [1.3.3 åŸºäºå¯†åº¦çš„æ–¹æ³•](#133-åŸºäºå¯†åº¦çš„æ–¹æ³•)
  - [2. æ•°æ®å¯è§†åŒ–æ•°å­¦ç†è®º](#2-æ•°æ®å¯è§†åŒ–æ•°å­¦ç†è®º)
    - [2.1 é™ç»´æŠ€æœ¯æ•°å­¦](#21-é™ç»´æŠ€æœ¯æ•°å­¦)
      - [2.1.1 ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰](#211-ä¸»æˆåˆ†åˆ†æpca)
      - [2.1.2 çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰](#212-çº¿æ€§åˆ¤åˆ«åˆ†ælda)
      - [2.1.3 t-SNE](#213-t-sne)
    - [2.2 æµå½¢å­¦ä¹ æ•°å­¦](#22-æµå½¢å­¦ä¹ æ•°å­¦)
      - [2.2.1 å±€éƒ¨çº¿æ€§åµŒå…¥ï¼ˆLLEï¼‰](#221-å±€éƒ¨çº¿æ€§åµŒå…¥lle)
      - [2.2.2 ç­‰è·æ˜ å°„ï¼ˆIsomapï¼‰](#222-ç­‰è·æ˜ å°„isomap)
      - [2.2.3 æ‹‰æ™®æ‹‰æ–¯ç‰¹å¾æ˜ å°„](#223-æ‹‰æ™®æ‹‰æ–¯ç‰¹å¾æ˜ å°„)
    - [2.3 æ‹“æ‰‘æ•°æ®åˆ†ææ•°å­¦](#23-æ‹“æ‰‘æ•°æ®åˆ†ææ•°å­¦)
      - [2.3.1 æŒä¹…åŒè°ƒ](#231-æŒä¹…åŒè°ƒ)
      - [2.3.2 æŒä¹…å›¾](#232-æŒä¹…å›¾)
      - [2.3.3 Mapperç®—æ³•](#233-mapperç®—æ³•)
  - [3. å¤§æ•°æ®å¤„ç†æ•°å­¦ç†è®º](#3-å¤§æ•°æ®å¤„ç†æ•°å­¦ç†è®º)
    - [3.1 åˆ†å¸ƒå¼è®¡ç®—æ•°å­¦](#31-åˆ†å¸ƒå¼è®¡ç®—æ•°å­¦)
      - [3.1.1 MapReduceæ¨¡å‹](#311-mapreduceæ¨¡å‹)
      - [3.1.2 åˆ†å¸ƒå¼ä¼˜åŒ–](#312-åˆ†å¸ƒå¼ä¼˜åŒ–)
      - [3.1.3 ä¸€è‡´æ€§ç®—æ³•](#313-ä¸€è‡´æ€§ç®—æ³•)
    - [3.2 æµå¤„ç†æ•°å­¦](#32-æµå¤„ç†æ•°å­¦)
      - [3.2.1 æ»‘åŠ¨çª—å£](#321-æ»‘åŠ¨çª—å£)
      - [3.2.2 æµç®—æ³•](#322-æµç®—æ³•)
      - [3.2.3 æµèšç±»](#323-æµèšç±»)
    - [3.3 å›¾è®¡ç®—æ•°å­¦](#33-å›¾è®¡ç®—æ•°å­¦)
      - [3.3.1 PageRankç®—æ³•](#331-pagerankç®—æ³•)
      - [3.3.2 ç¤¾åŒºæ£€æµ‹](#332-ç¤¾åŒºæ£€æµ‹)
      - [3.3.3 æœ€çŸ­è·¯å¾„](#333-æœ€çŸ­è·¯å¾„)
  - [4. ç»Ÿè®¡å­¦ä¹ æ•°å­¦ç†è®º](#4-ç»Ÿè®¡å­¦ä¹ æ•°å­¦ç†è®º)
    - [4.1 ç›‘ç£å­¦ä¹ æ•°å­¦](#41-ç›‘ç£å­¦ä¹ æ•°å­¦)
      - [4.1.1 çº¿æ€§å›å½’](#411-çº¿æ€§å›å½’)
      - [4.1.2 é€»è¾‘å›å½’](#412-é€»è¾‘å›å½’)
      - [4.1.3 æ”¯æŒå‘é‡æœº](#413-æ”¯æŒå‘é‡æœº)
    - [4.2 æ— ç›‘ç£å­¦ä¹ æ•°å­¦](#42-æ— ç›‘ç£å­¦ä¹ æ•°å­¦)
      - [4.2.1 ä¸»æˆåˆ†åˆ†æ](#421-ä¸»æˆåˆ†åˆ†æ)
      - [4.2.2 ç‹¬ç«‹æˆåˆ†åˆ†æ](#422-ç‹¬ç«‹æˆåˆ†åˆ†æ)
      - [4.2.3 è‡ªç¼–ç å™¨](#423-è‡ªç¼–ç å™¨)
    - [4.3 å¼ºåŒ–å­¦ä¹ æ•°å­¦](#43-å¼ºåŒ–å­¦ä¹ æ•°å­¦)
      - [4.3.1 Qå­¦ä¹ ](#431-qå­¦ä¹ )
      - [4.3.2 ç­–ç•¥æ¢¯åº¦](#432-ç­–ç•¥æ¢¯åº¦)
      - [4.3.3 Actor-Critic](#433-actor-critic)
  - [5. æŠ€æœ¯å®ç°](#5-æŠ€æœ¯å®ç°)
    - [5.1 Pythonå®ç°](#51-pythonå®ç°)
    - [5.2 åˆ†å¸ƒå¼è®¡ç®—å®ç°](#52-åˆ†å¸ƒå¼è®¡ç®—å®ç°)
  - [6. ğŸ¯ åº”ç”¨æ¡ˆä¾‹ / Applications](#6--åº”ç”¨æ¡ˆä¾‹--applications)
    - [6.1 æ•°æ®æŒ–æ˜åº”ç”¨ / Data Mining Applications](#61-æ•°æ®æŒ–æ˜åº”ç”¨--data-mining-applications)
      - [6.1.1 å®¢æˆ·ç»†åˆ† / Customer Segmentation](#611-å®¢æˆ·ç»†åˆ†--customer-segmentation)
      - [6.1.2 å¼‚å¸¸æ£€æµ‹ / Anomaly Detection](#612-å¼‚å¸¸æ£€æµ‹--anomaly-detection)
      - [6.1.3 å…³è”è§„åˆ™æŒ–æ˜ / Association Rule Mining](#613-å…³è”è§„åˆ™æŒ–æ˜--association-rule-mining)
    - [6.2 æ•°æ®å¯è§†åŒ–åº”ç”¨ / Data Visualization Applications](#62-æ•°æ®å¯è§†åŒ–åº”ç”¨--data-visualization-applications)
      - [6.2.1 æ¢ç´¢æ€§æ•°æ®åˆ†æ / Exploratory Data Analysis](#621-æ¢ç´¢æ€§æ•°æ®åˆ†æ--exploratory-data-analysis)
      - [6.2.2 é«˜ç»´æ•°æ®å¯è§†åŒ– / High-Dimensional Data Visualization](#622-é«˜ç»´æ•°æ®å¯è§†åŒ–--high-dimensional-data-visualization)
      - [6.2.3 æ—¶é—´åºåˆ—å¯è§†åŒ– / Time Series Visualization](#623-æ—¶é—´åºåˆ—å¯è§†åŒ–--time-series-visualization)
    - [6.3 å¤§æ•°æ®å¤„ç†åº”ç”¨ / Big Data Processing Applications](#63-å¤§æ•°æ®å¤„ç†åº”ç”¨--big-data-processing-applications)
      - [6.3.1 å®æ—¶æµå¤„ç† / Real-Time Stream Processing](#631-å®æ—¶æµå¤„ç†--real-time-stream-processing)
      - [6.3.2 åˆ†å¸ƒå¼è®¡ç®— / Distributed Computing](#632-åˆ†å¸ƒå¼è®¡ç®—--distributed-computing)
      - [6.3.3 å›¾è®¡ç®— / Graph Computing](#633-å›¾è®¡ç®—--graph-computing)
    - [6.4 ç»Ÿè®¡å­¦ä¹ åº”ç”¨ / Statistical Learning Applications](#64-ç»Ÿè®¡å­¦ä¹ åº”ç”¨--statistical-learning-applications)
      - [6.4.1 é¢„æµ‹å»ºæ¨¡ / Predictive Modeling](#641-é¢„æµ‹å»ºæ¨¡--predictive-modeling)
      - [6.4.2 ç‰¹å¾å·¥ç¨‹ / Feature Engineering](#642-ç‰¹å¾å·¥ç¨‹--feature-engineering)
      - [6.4.3 æ¨¡å‹è¯„ä¼° / Model Evaluation](#643-æ¨¡å‹è¯„ä¼°--model-evaluation)
    - [6.5 è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ / Natural Language Processing Applications](#65-è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨--natural-language-processing-applications)
      - [6.5.1 æ–‡æœ¬åˆ†ç±» / Text Classification](#651-æ–‡æœ¬åˆ†ç±»--text-classification)
      - [6.5.2 ä¸»é¢˜å»ºæ¨¡ / Topic Modeling](#652-ä¸»é¢˜å»ºæ¨¡--topic-modeling)
    - [6.6 æ¨èç³»ç»Ÿåº”ç”¨ / Recommendation System Applications](#66-æ¨èç³»ç»Ÿåº”ç”¨--recommendation-system-applications)
      - [6.6.1 ååŒè¿‡æ»¤ / Collaborative Filtering](#661-ååŒè¿‡æ»¤--collaborative-filtering)
      - [6.6.2 æ·±åº¦å­¦ä¹ æ¨è / Deep Learning Recommendation](#662-æ·±åº¦å­¦ä¹ æ¨è--deep-learning-recommendation)
    - [6.7 æ—¶é—´åºåˆ—åˆ†æåº”ç”¨ / Time Series Analysis Applications](#67-æ—¶é—´åºåˆ—åˆ†æåº”ç”¨--time-series-analysis-applications)
      - [6.7.1 é¢„æµ‹åˆ†æ / Forecasting](#671-é¢„æµ‹åˆ†æ--forecasting)
      - [6.7.2 å¼‚å¸¸æ£€æµ‹ / Anomaly Detection in Time Series](#672-å¼‚å¸¸æ£€æµ‹--anomaly-detection-in-time-series)
  - [7. å‰æ²¿å‘å±•](#7-å‰æ²¿å‘å±•)
    - [7.1 æ·±åº¦å­¦ä¹ ä¸æ•°æ®ç§‘å­¦](#71-æ·±åº¦å­¦ä¹ ä¸æ•°æ®ç§‘å­¦)
    - [7.2 å›¾ç¥ç»ç½‘ç»œ](#72-å›¾ç¥ç»ç½‘ç»œ)
    - [7.3 è”é‚¦å­¦ä¹ ](#73-è”é‚¦å­¦ä¹ )
  - [8. æ€»ç»“ä¸å±•æœ›](#8-æ€»ç»“ä¸å±•æœ›)
    - [8.1 æ ¸å¿ƒè¦ç‚¹æ€»ç»“](#81-æ ¸å¿ƒè¦ç‚¹æ€»ç»“)
    - [8.2 å‘å±•è¶‹åŠ¿](#82-å‘å±•è¶‹åŠ¿)
    - [8.3 æŒ‘æˆ˜ä¸æœºé‡](#83-æŒ‘æˆ˜ä¸æœºé‡)
  - [ğŸ“š å‚è€ƒæ–‡çŒ®](#-å‚è€ƒæ–‡çŒ®)
  - [ğŸ”— ç›¸å…³é“¾æ¥](#-ç›¸å…³é“¾æ¥)

---

## 1. æ•°æ®æŒ–æ˜æ•°å­¦ç†è®º

### 1.1 èšç±»åˆ†ææ•°å­¦

#### 1.1.1 K-meansèšç±»

**ç›®æ ‡å‡½æ•°**ï¼š
$$J = \sum_{i=1}^{k} \sum_{x \in C_i} \|x - \mu_i\|^2$$

å…¶ä¸­ï¼š

- $C_i$æ˜¯ç¬¬$i$ä¸ªèšç±»
- $\mu_i$æ˜¯ç¬¬$i$ä¸ªèšç±»çš„ä¸­å¿ƒ
- $k$æ˜¯èšç±»æ•°

**ç®—æ³•æ­¥éª¤**ï¼š

1. éšæœºåˆå§‹åŒ–$k$ä¸ªèšç±»ä¸­å¿ƒ
2. å°†æ¯ä¸ªç‚¹åˆ†é…åˆ°æœ€è¿‘çš„èšç±»ä¸­å¿ƒ
3. é‡æ–°è®¡ç®—èšç±»ä¸­å¿ƒ
4. é‡å¤æ­¥éª¤2-3ç›´åˆ°æ”¶æ•›

**æ”¶æ•›æ€§**ï¼š
ç›®æ ‡å‡½æ•°$J$åœ¨æ¯æ¬¡è¿­ä»£åå•è°ƒé€’å‡ï¼Œå› æ­¤ç®—æ³•å¿…ç„¶æ”¶æ•›ã€‚

#### 1.1.2 å±‚æ¬¡èšç±»

**è·ç¦»åº¦é‡**ï¼š

- å•é“¾æ¥ï¼š$d(C_i, C_j) = \min_{x \in C_i, y \in C_j} d(x, y)$
- å®Œå…¨é“¾æ¥ï¼š$d(C_i, C_j) = \max_{x \in C_i, y \in C_j} d(x, y)$
- å¹³å‡é“¾æ¥ï¼š$d(C_i, C_j) = \frac{1}{|C_i||C_j|} \sum_{x \in C_i, y \in C_j} d(x, y)$

**ç®—æ³•æ­¥éª¤**ï¼š

1. æ¯ä¸ªç‚¹ä½œä¸ºä¸€ä¸ªèšç±»
2. æ‰¾åˆ°è·ç¦»æœ€è¿‘çš„ä¸¤ä¸ªèšç±»
3. åˆå¹¶è¿™ä¸¤ä¸ªèšç±»
4. é‡å¤æ­¥éª¤2-3ç›´åˆ°åªå‰©ä¸€ä¸ªèšç±»

#### 1.1.3 å¯†åº¦èšç±»

**DBSCANç®—æ³•**ï¼š

- æ ¸å¿ƒç‚¹ï¼šé‚»åŸŸå†…ç‚¹æ•°$\geq \text{minPts}$
- è¾¹ç•Œç‚¹ï¼šä¸æ˜¯æ ¸å¿ƒç‚¹ä½†åœ¨æ ¸å¿ƒç‚¹é‚»åŸŸå†…
- å™ªå£°ç‚¹ï¼šæ—¢ä¸æ˜¯æ ¸å¿ƒç‚¹ä¹Ÿä¸æ˜¯è¾¹ç•Œç‚¹

**å¯†åº¦å¯è¾¾æ€§**ï¼š
ç‚¹$p$å¯†åº¦å¯è¾¾ç‚¹$q$ï¼Œå¦‚æœå­˜åœ¨ç‚¹åºåˆ—$p_1, p_2, \ldots, p_n$ï¼Œä½¿å¾—$p_1 = p$ï¼Œ$p_n = q$ï¼Œä¸”$p_{i+1}$åœ¨$p_i$çš„$\epsilon$-é‚»åŸŸå†…ã€‚

### 1.2 å…³è”è§„åˆ™æŒ–æ˜æ•°å­¦

#### 1.2.1 æ”¯æŒåº¦å’Œç½®ä¿¡åº¦

**æ”¯æŒåº¦**ï¼š
$$\text{support}(A \to B) = \frac{|\{T \in D : A \cup B \subseteq T\}|}{|D|}$$

**ç½®ä¿¡åº¦**ï¼š
$$\text{confidence}(A \to B) = \frac{\text{support}(A \cup B)}{\text{support}(A)}$$

**æå‡åº¦**ï¼š
$$\text{lift}(A \to B) = \frac{\text{confidence}(A \to B)}{\text{support}(B)}$$

#### 1.2.2 Aprioriç®—æ³•

**å‘ä¸‹é—­åŒ…æ€§è´¨**ï¼š
å¦‚æœé¡¹é›†$X$æ˜¯é¢‘ç¹çš„ï¼Œåˆ™$X$çš„æ‰€æœ‰å­é›†éƒ½æ˜¯é¢‘ç¹çš„ã€‚

**ç®—æ³•æ­¥éª¤**ï¼š

1. ç”Ÿæˆ1-é¡¹é›†
2. ç”Ÿæˆ$k$-é¡¹é›†ï¼ˆ$k \geq 2$ï¼‰
3. è®¡ç®—æ”¯æŒåº¦
4. å‰ªæéé¢‘ç¹é¡¹é›†
5. é‡å¤æ­¥éª¤2-4ç›´åˆ°æ²¡æœ‰é¢‘ç¹é¡¹é›†

#### 1.2.3 FP-Growthç®—æ³•

**FPæ ‘æ„é€ **ï¼š

1. æ‰«ææ•°æ®åº“ï¼Œè®¡ç®—é¡¹çš„æ”¯æŒåº¦
2. æŒ‰æ”¯æŒåº¦é™åºæ’åˆ—é¡¹
3. æ„é€ FPæ ‘

**é¢‘ç¹æ¨¡å¼æŒ–æ˜**ï¼š

1. ä»FPæ ‘ä¸­æå–æ¡ä»¶æ¨¡å¼åŸº
2. æ„é€ æ¡ä»¶FPæ ‘
3. é€’å½’æŒ–æ˜é¢‘ç¹æ¨¡å¼

### 1.3 å¼‚å¸¸æ£€æµ‹æ•°å­¦

#### 1.3.1 ç»Ÿè®¡æ–¹æ³•

**Z-scoreæ–¹æ³•**ï¼š
$$z_i = \frac{x_i - \mu}{\sigma}$$

å…¶ä¸­$\mu$æ˜¯å‡å€¼ï¼Œ$\sigma$æ˜¯æ ‡å‡†å·®ã€‚

**IQRæ–¹æ³•**ï¼š
$$Q_1 = \text{25th percentile}$$
$$Q_3 = \text{75th percentile}$$
$$\text{IQR} = Q_3 - Q_1$$
$$\text{Lower bound} = Q_1 - 1.5 \times \text{IQR}$$
$$\text{Upper bound} = Q_3 + 1.5 \times \text{IQR}$$

#### 1.3.2 åŸºäºè·ç¦»çš„æ–¹æ³•

**k-æœ€è¿‘é‚»**ï¼š
å¯¹äºç‚¹$x$ï¼Œè®¡ç®—åˆ°å…¶$k$ä¸ªæœ€è¿‘é‚»çš„å¹³å‡è·ç¦»ï¼š
$$d_k(x) = \frac{1}{k} \sum_{i=1}^{k} d(x, x_i)$$

**å±€éƒ¨å¼‚å¸¸å› å­ï¼ˆLOFï¼‰**ï¼š
$$\text{LOF}(x) = \frac{\sum_{y \in N_k(x)} \frac{\text{reach-dist}_k(y, x)}{\text{reach-dist}_k(x, y)}}{|N_k(x)|}$$

å…¶ä¸­$\text{reach-dist}_k(x, y) = \max\{d_k(x), d(x, y)\}$ã€‚

#### 1.3.3 åŸºäºå¯†åº¦çš„æ–¹æ³•

**å±€éƒ¨å¯†åº¦**ï¼š
$$\text{local-density}(x) = \frac{1}{\frac{1}{|N_k(x)|} \sum_{y \in N_k(x)} d(x, y)}$$

**å¼‚å¸¸åˆ†æ•°**ï¼š
$$\text{anomaly-score}(x) = \frac{\text{local-density}(x)}{\frac{1}{|N_k(x)|} \sum_{y \in N_k(x)} \text{local-density}(y)}$$

## 2. æ•°æ®å¯è§†åŒ–æ•°å­¦ç†è®º

### 2.1 é™ç»´æŠ€æœ¯æ•°å­¦

#### 2.1.1 ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰

**åæ–¹å·®çŸ©é˜µ**ï¼š
$$C = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})(x_i - \bar{x})^T$$

**ç‰¹å¾å€¼åˆ†è§£**ï¼š
$$C = V \Lambda V^T$$

å…¶ä¸­$V$æ˜¯ç‰¹å¾å‘é‡çŸ©é˜µï¼Œ$\Lambda$æ˜¯ç‰¹å¾å€¼å¯¹è§’çŸ©é˜µã€‚

**æŠ•å½±**ï¼š
$$y_i = V^T x_i$$

å…¶ä¸­$y_i$æ˜¯é™ç»´åçš„æ•°æ®ã€‚

#### 2.1.2 çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰

**ç±»é—´æ•£åº¦çŸ©é˜µ**ï¼š
$$S_B = \sum_{i=1}^{c} n_i (\mu_i - \mu)(\mu_i - \mu)^T$$

**ç±»å†…æ•£åº¦çŸ©é˜µ**ï¼š
$$S_W = \sum_{i=1}^{c} \sum_{x \in C_i} (x - \mu_i)(x - \mu_i)^T$$

**ç›®æ ‡å‡½æ•°**ï¼š
$$\max_w \frac{w^T S_B w}{w^T S_W w}$$

**è§£**ï¼š
$$w = S_W^{-1} (\mu_1 - \mu_2)$$

#### 2.1.3 t-SNE

**ç›¸ä¼¼åº¦è®¡ç®—**ï¼š
$$p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|x_i - x_k\|^2 / 2\sigma_i^2)}$$

**ä½ç»´ç›¸ä¼¼åº¦**ï¼š
$$q_{ij} = \frac{(1 + \|y_i - y_j\|^2)^{-1}}{\sum_{k \neq l} (1 + \|y_k - y_l\|^2)^{-1}}$$

**KLæ•£åº¦**ï¼š
$$C = \sum_i \sum_j p_{ij} \log \frac{p_{ij}}{q_{ij}}$$

### 2.2 æµå½¢å­¦ä¹ æ•°å­¦

#### 2.2.1 å±€éƒ¨çº¿æ€§åµŒå…¥ï¼ˆLLEï¼‰

**é‡æ„æƒé‡**ï¼š
$$\min_W \sum_{i=1}^{n} \|x_i - \sum_{j \in N_i} W_{ij} x_j\|^2$$

çº¦æŸæ¡ä»¶ï¼š
$$\sum_{j \in N_i} W_{ij} = 1$$

**ä½ç»´åµŒå…¥**ï¼š
$$\min_Y \sum_{i=1}^{n} \|y_i - \sum_{j \in N_i} W_{ij} y_j\|^2$$

#### 2.2.2 ç­‰è·æ˜ å°„ï¼ˆIsomapï¼‰

**æµ‹åœ°è·ç¦»**ï¼š
ä½¿ç”¨æœ€çŸ­è·¯å¾„ç®—æ³•è®¡ç®—æµ‹åœ°è·ç¦»ã€‚

**å¤šç»´ç¼©æ”¾ï¼ˆMDSï¼‰**ï¼š
$$\min_Y \sum_{i,j} (d_{ij} - \|y_i - y_j\|)^2$$

å…¶ä¸­$d_{ij}$æ˜¯æµ‹åœ°è·ç¦»ã€‚

#### 2.2.3 æ‹‰æ™®æ‹‰æ–¯ç‰¹å¾æ˜ å°„

**æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ**ï¼š
$$L = D - W$$

å…¶ä¸­$W$æ˜¯ç›¸ä¼¼åº¦çŸ©é˜µï¼Œ$D$æ˜¯åº¦çŸ©é˜µã€‚

**ç‰¹å¾å€¼é—®é¢˜**ï¼š
$$L f = \lambda D f$$

**åµŒå…¥**ï¼š
ä½¿ç”¨æœ€å°çš„$k$ä¸ªéé›¶ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚

### 2.3 æ‹“æ‰‘æ•°æ®åˆ†ææ•°å­¦

#### 2.3.1 æŒä¹…åŒè°ƒ

**å•çº¯å¤å½¢**ï¼š

- 0-å•çº¯å½¢ï¼šç‚¹
- 1-å•çº¯å½¢ï¼šè¾¹
- 2-å•çº¯å½¢ï¼šä¸‰è§’å½¢
- $k$-å•çº¯å½¢ï¼š$k+1$ä¸ªç‚¹çš„å‡¸åŒ…

**è¾¹ç•Œç®—å­**ï¼š
$$\partial_k : C_k \to C_{k-1}$$

å…¶ä¸­$C_k$æ˜¯$k$-é“¾ç¾¤ã€‚

**åŒè°ƒç¾¤**ï¼š
$$H_k = \frac{\ker \partial_k}{\text{im } \partial_{k+1}}$$

#### 2.3.2 æŒä¹…å›¾

**æŒä¹…æ€§**ï¼š
$$(\text{birth}, \text{death})$$

å…¶ä¸­birthæ˜¯ç‰¹å¾å‡ºç°çš„æ—¶åˆ»ï¼Œdeathæ˜¯ç‰¹å¾æ¶ˆå¤±çš„æ—¶åˆ»ã€‚

**æŒä¹…æ€§å›¾**ï¼š
åœ¨å¹³é¢ä¸Šç»˜åˆ¶ç‚¹$(\text{birth}, \text{death})$ã€‚

#### 2.3.3 Mapperç®—æ³•

**è¦†ç›–**ï¼š
å°†æ•°æ®ç©ºé—´åˆ’åˆ†ä¸ºé‡å çš„çƒã€‚

**èšç±»**ï¼š
åœ¨æ¯ä¸ªçƒå†…è¿›è¡Œèšç±»ã€‚

**ç¥ç»å›¾**ï¼š
å°†èšç±»ä½œä¸ºèŠ‚ç‚¹ï¼Œé‡å å…³ç³»ä½œä¸ºè¾¹ã€‚

## 3. å¤§æ•°æ®å¤„ç†æ•°å­¦ç†è®º

### 3.1 åˆ†å¸ƒå¼è®¡ç®—æ•°å­¦

#### 3.1.1 MapReduceæ¨¡å‹

**Mapå‡½æ•°**ï¼š
$$(k_1, v_1) \to \text{list}(k_2, v_2)$$

**Reduceå‡½æ•°**ï¼š
$$(k_2, \text{list}(v_2)) \to \text{list}(k_3, v_3)$$

**è®¡ç®—å¤æ‚åº¦**ï¼š

- æ—¶é—´å¤æ‚åº¦ï¼š$O(n/p + \log p)$
- ç©ºé—´å¤æ‚åº¦ï¼š$O(n)$

å…¶ä¸­$n$æ˜¯æ•°æ®å¤§å°ï¼Œ$p$æ˜¯å¤„ç†å™¨æ•°é‡ã€‚

#### 3.1.2 åˆ†å¸ƒå¼ä¼˜åŒ–

**æ¢¯åº¦ä¸‹é™**ï¼š
$$\theta_{t+1} = \theta_t - \alpha \frac{1}{n} \sum_{i=1}^{n} \nabla f_i(\theta_t)$$

**éšæœºæ¢¯åº¦ä¸‹é™**ï¼š
$$\theta_{t+1} = \theta_t - \alpha \nabla f_{i_t}(\theta_t)$$

å…¶ä¸­$i_t$æ˜¯éšæœºé€‰æ‹©çš„æ ·æœ¬ç´¢å¼•ã€‚

#### 3.1.3 ä¸€è‡´æ€§ç®—æ³•

**å¹³å‡ä¸€è‡´æ€§**ï¼š
$$x_i(t+1) = \sum_{j \in N_i} W_{ij} x_j(t)$$

å…¶ä¸­$W$æ˜¯æƒé‡çŸ©é˜µï¼Œæ»¡è¶³ï¼š
$$\sum_{j} W_{ij} = 1$$

**æ”¶æ•›æ¡ä»¶**ï¼š
$$\lim_{t \to \infty} x_i(t) = \frac{1}{n} \sum_{j=1}^{n} x_j(0)$$

### 3.2 æµå¤„ç†æ•°å­¦

#### 3.2.1 æ»‘åŠ¨çª—å£

**æ—¶é—´çª—å£**ï¼š
$$W(t) = \{x_i : t - w \leq t_i \leq t\}$$

å…¶ä¸­$w$æ˜¯çª—å£å¤§å°ã€‚

**è®¡æ•°çª—å£**ï¼š
$$W(t) = \{x_i : i \in [t-k+1, t]\}$$

å…¶ä¸­$k$æ˜¯çª—å£å¤§å°ã€‚

#### 3.2.2 æµç®—æ³•

**Count-Min Sketch**ï¼š
$$h_i(x) = (a_i x + b_i) \bmod m$$

å…¶ä¸­$a_i, b_i$æ˜¯éšæœºæ•°ã€‚

**ä¼°è®¡é¢‘ç‡**ï¼š
$$\hat{f}(x) = \min_i C[i, h_i(x)]$$

å…¶ä¸­$C$æ˜¯è®¡æ•°å™¨çŸ©é˜µã€‚

#### 3.2.3 æµèšç±»

**BIRCHç®—æ³•**ï¼š

- æ„å»ºCFæ ‘
- èšç±»ç‰¹å¾ï¼š$(N, LS, SS)$
- å…¶ä¸­$N$æ˜¯ç‚¹æ•°ï¼Œ$LS$æ˜¯çº¿æ€§å’Œï¼Œ$SS$æ˜¯å¹³æ–¹å’Œ

**æ›´æ–°è§„åˆ™**ï¼š
$$CF_1 + CF_2 = (N_1 + N_2, LS_1 + LS_2, SS_1 + SS_2)$$

### 3.3 å›¾è®¡ç®—æ•°å­¦

#### 3.3.1 PageRankç®—æ³•

**PageRankæ–¹ç¨‹**ï¼š
$$PR(p) = \frac{1-d}{N} + d \sum_{q \in B_p} \frac{PR(q)}{C(q)}$$

å…¶ä¸­ï¼š

- $d$æ˜¯é˜»å°¼å› å­
- $N$æ˜¯é¡µé¢æ€»æ•°
- $B_p$æ˜¯æŒ‡å‘é¡µé¢$p$çš„é¡µé¢é›†åˆ
- $C(q)$æ˜¯é¡µé¢$q$çš„å‡ºé“¾æ•°

**è¿­ä»£æ±‚è§£**ï¼š
$$PR^{(t+1)} = (1-d) \frac{1}{N} \mathbf{1} + d M PR^{(t)}$$

å…¶ä¸­$M$æ˜¯è½¬ç§»çŸ©é˜µã€‚

#### 3.3.2 ç¤¾åŒºæ£€æµ‹

**æ¨¡å—åº¦**ï¼š
$$Q = \frac{1}{2m} \sum_{ij} \left[A_{ij} - \frac{k_i k_j}{2m}\right] \delta(c_i, c_j)$$

å…¶ä¸­ï¼š

- $A_{ij}$æ˜¯é‚»æ¥çŸ©é˜µ
- $k_i$æ˜¯èŠ‚ç‚¹$i$çš„åº¦
- $m$æ˜¯æ€»è¾¹æ•°
- $c_i$æ˜¯èŠ‚ç‚¹$i$çš„ç¤¾åŒºæ ‡ç­¾

#### 3.3.3 æœ€çŸ­è·¯å¾„

**Dijkstraç®—æ³•**ï¼š

1. åˆå§‹åŒ–è·ç¦»ï¼š$d(s) = 0$ï¼Œ$d(v) = \infty$ for $v \neq s$
2. é€‰æ‹©æœªè®¿é—®çš„æœ€å°è·ç¦»èŠ‚ç‚¹$u$
3. æ›´æ–°é‚»å±…è·ç¦»ï¼š$d(v) = \min(d(v), d(u) + w(u,v))$
4. é‡å¤æ­¥éª¤2-3ç›´åˆ°æ‰€æœ‰èŠ‚ç‚¹è¢«è®¿é—®

**Floyd-Warshallç®—æ³•**ï¼š
$$d_{ij}^{(k)} = \min(d_{ij}^{(k-1)}, d_{ik}^{(k-1)} + d_{kj}^{(k-1)})$$

## 4. ç»Ÿè®¡å­¦ä¹ æ•°å­¦ç†è®º

### 4.1 ç›‘ç£å­¦ä¹ æ•°å­¦

#### 4.1.1 çº¿æ€§å›å½’

**æ¨¡å‹**ï¼š
$$y = X\beta + \epsilon$$

å…¶ä¸­$\epsilon \sim \mathcal{N}(0, \sigma^2 I)$ã€‚

**æœ€å°äºŒä¹˜ä¼°è®¡**ï¼š
$$\hat{\beta} = (X^T X)^{-1} X^T y$$

**é¢„æµ‹**ï¼š
$$\hat{y} = X\hat{\beta}$$

#### 4.1.2 é€»è¾‘å›å½’

**æ¨¡å‹**ï¼š
$$P(y=1|x) = \frac{1}{1 + e^{-x^T \beta}}$$

**å¯¹æ•°ä¼¼ç„¶**ï¼š
$$L(\beta) = \sum_{i=1}^{n} [y_i \log p_i + (1-y_i) \log(1-p_i)]$$

å…¶ä¸­$p_i = P(y_i=1|x_i)$ã€‚

**æ¢¯åº¦**ï¼š
$$\nabla L(\beta) = X^T(y - p)$$

å…¶ä¸­$p = [p_1, \ldots, p_n]^T$ã€‚

#### 4.1.3 æ”¯æŒå‘é‡æœº

**åŸå§‹é—®é¢˜**ï¼š
$$\min_{w,b} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^{n} \xi_i$$

çº¦æŸæ¡ä»¶ï¼š
$$y_i(w^T x_i + b) \geq 1 - \xi_i$$
$$\xi_i \geq 0$$

**å¯¹å¶é—®é¢˜**ï¼š
$$\max_{\alpha} \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j x_i^T x_j$$

çº¦æŸæ¡ä»¶ï¼š
$$0 \leq \alpha_i \leq C$$
$$\sum_{i=1}^{n} \alpha_i y_i = 0$$

### 4.2 æ— ç›‘ç£å­¦ä¹ æ•°å­¦

#### 4.2.1 ä¸»æˆåˆ†åˆ†æ

**ç›®æ ‡å‡½æ•°**ï¼š
$$\max_{w} \frac{w^T S w}{w^T w}$$

å…¶ä¸­$S$æ˜¯åæ–¹å·®çŸ©é˜µã€‚

**ç‰¹å¾å€¼é—®é¢˜**ï¼š
$$S w = \lambda w$$

**ä¸»æˆåˆ†**ï¼š
$$y_i = w_i^T x$$

å…¶ä¸­$w_i$æ˜¯ç¬¬$i$ä¸ªç‰¹å¾å‘é‡ã€‚

#### 4.2.2 ç‹¬ç«‹æˆåˆ†åˆ†æ

**æ¨¡å‹**ï¼š
$$x = As$$

å…¶ä¸­$A$æ˜¯æ··åˆçŸ©é˜µï¼Œ$s$æ˜¯ç‹¬ç«‹æˆåˆ†ã€‚

**ç›®æ ‡å‡½æ•°**ï¼š
$$\max_W \sum_{i=1}^{n} \log p_i(W_i^T x)$$

å…¶ä¸­$W = A^{-1}$ã€‚

#### 4.2.3 è‡ªç¼–ç å™¨

**ç¼–ç å™¨**ï¼š
$$h = f(W_1 x + b_1)$$

**è§£ç å™¨**ï¼š
$$\hat{x} = g(W_2 h + b_2)$$

**ç›®æ ‡å‡½æ•°**ï¼š
$$\min_{W_1, W_2, b_1, b_2} \|x - \hat{x}\|^2$$

### 4.3 å¼ºåŒ–å­¦ä¹ æ•°å­¦

#### 4.3.1 Qå­¦ä¹ 

**Qå‡½æ•°æ›´æ–°**ï¼š
$$Q(s, a) \leftarrow Q(s, a) + \alpha[r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$$

å…¶ä¸­ï¼š

- $\alpha$æ˜¯å­¦ä¹ ç‡
- $\gamma$æ˜¯æŠ˜æ‰£å› å­
- $r$æ˜¯å¥–åŠ±
- $s'$æ˜¯ä¸‹ä¸€ä¸ªçŠ¶æ€

#### 4.3.2 ç­–ç•¥æ¢¯åº¦

**ç›®æ ‡å‡½æ•°**ï¼š
$$J(\theta) = \mathbb{E}_{\pi_\theta}[\sum_{t=0}^{\infty} \gamma^t r_t]$$

**ç­–ç•¥æ¢¯åº¦å®šç†**ï¼š
$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) Q^\pi(s, a)]$$

#### 4.3.3 Actor-Critic

**Actoræ›´æ–°**ï¼š
$$\theta \leftarrow \theta + \alpha_\theta \nabla_\theta \log \pi_\theta(a|s) \delta$$

**Criticæ›´æ–°**ï¼š
$$w \leftarrow w + \alpha_w \delta \nabla_w V_w(s)$$

å…¶ä¸­$\delta = r + \gamma V_w(s') - V_w(s)$æ˜¯TDè¯¯å·®ã€‚

## 5. æŠ€æœ¯å®ç°

### 5.1 Pythonå®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
import pandas as pd

# æ•°æ®æŒ–æ˜å®ç°
class DataMining:
    def __init__(self):
        pass

    def kmeans_clustering(self, data, k, max_iter=100):
        """K-meansèšç±»"""
        # éšæœºåˆå§‹åŒ–èšç±»ä¸­å¿ƒ
        n_samples, n_features = data.shape
        centroids = data[np.random.choice(n_samples, k, replace=False)]

        for _ in range(max_iter):
            # åˆ†é…ç‚¹åˆ°æœ€è¿‘çš„ä¸­å¿ƒ
            distances = np.sqrt(((data - centroids[:, np.newaxis])**2).sum(axis=2))
            labels = np.argmin(distances, axis=0)

            # æ›´æ–°èšç±»ä¸­å¿ƒ
            new_centroids = np.array([data[labels == i].mean(axis=0)
                                     for i in range(k)])

            # æ£€æŸ¥æ”¶æ•›
            if np.allclose(centroids, new_centroids):
                break
            centroids = new_centroids

        return labels, centroids

    def dbscan_clustering(self, data, eps, min_samples):
        """DBSCANèšç±»"""
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(data)
        return labels

    def association_rules(self, transactions, min_support=0.1, min_confidence=0.5):
        """å…³è”è§„åˆ™æŒ–æ˜"""
        # è®¡ç®—é¡¹é›†æ”¯æŒåº¦
        item_counts = {}
        for transaction in transactions:
            for item in transaction:
                item_counts[item] = item_counts.get(item, 0) + 1

        n_transactions = len(transactions)
        frequent_items = {item: count/n_transactions
                         for item, count in item_counts.items()
                         if count/n_transactions >= min_support}

        # ç”Ÿæˆå…³è”è§„åˆ™
        rules = []
        for item1 in frequent_items:
            for item2 in frequent_items:
                if item1 != item2:
                    # è®¡ç®—æ”¯æŒåº¦å’Œç½®ä¿¡åº¦
                    support_both = sum(1 for t in transactions
                                     if item1 in t and item2 in t) / n_transactions
                    support_item1 = frequent_items[item1]
                    confidence = support_both / support_item1

                    if confidence >= min_confidence:
                        rules.append((item1, item2, support_both, confidence))

        return rules

# æ•°æ®å¯è§†åŒ–å®ç°
class DataVisualization:
    def __init__(self):
        pass

    def pca_reduction(self, data, n_components=2):
        """PCAé™ç»´"""
        pca = PCA(n_components=n_components)
        reduced_data = pca.fit_transform(data)
        explained_variance = pca.explained_variance_ratio_
        return reduced_data, explained_variance

    def tsne_reduction(self, data, n_components=2, perplexity=30):
        """t-SNEé™ç»´"""
        tsne = TSNE(n_components=n_components, perplexity=perplexity)
        reduced_data = tsne.fit_transform(data)
        return reduced_data

    def lda_reduction(self, data, labels, n_components=2):
        """LDAé™ç»´"""
        from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
        lda = LinearDiscriminantAnalysis(n_components=n_components)
        reduced_data = lda.fit_transform(data, labels)
        return reduced_data

# å¤§æ•°æ®å¤„ç†å®ç°
class BigDataProcessing:
    def __init__(self):
        pass

    def map_reduce_example(self, data, map_func, reduce_func):
        """MapReduceç¤ºä¾‹"""
        # Mapé˜¶æ®µ
        mapped_data = [map_func(item) for item in data]

        # Shuffleé˜¶æ®µï¼ˆç®€åŒ–ï¼‰
        grouped_data = {}
        for key, value in mapped_data:
            if key not in grouped_data:
                grouped_data[key] = []
            grouped_data[key].append(value)

        # Reduceé˜¶æ®µ
        result = {}
        for key, values in grouped_data.items():
            result[key] = reduce_func(key, values)

        return result

    def streaming_algorithm(self, data_stream, window_size=100):
        """æµå¤„ç†ç®—æ³•"""
        window = []
        results = []

        for item in data_stream:
            window.append(item)
            if len(window) > window_size:
                window.pop(0)

            # è®¡ç®—çª—å£ç»Ÿè®¡é‡
            if len(window) > 0:
                mean_val = np.mean(window)
                results.append(mean_val)

        return results

# ç»Ÿè®¡å­¦ä¹ å®ç°
class StatisticalLearning:
    def __init__(self):
        pass

    def linear_regression(self, X, y):
        """çº¿æ€§å›å½’"""
        # æ·»åŠ åç½®é¡¹
        X_b = np.c_[np.ones((X.shape[0], 1)), X]

        # æœ€å°äºŒä¹˜è§£
        theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
        return theta

    def logistic_regression(self, X, y, learning_rate=0.01, n_iterations=1000):
        """é€»è¾‘å›å½’"""
        n_samples, n_features = X.shape
        theta = np.zeros(n_features)

        for _ in range(n_iterations):
            # è®¡ç®—é¢„æµ‹æ¦‚ç‡
            z = X.dot(theta)
            h = 1 / (1 + np.exp(-z))

            # è®¡ç®—æ¢¯åº¦
            gradient = X.T.dot(h - y) / n_samples

            # æ›´æ–°å‚æ•°
            theta -= learning_rate * gradient

        return theta

    def svm_classifier(self, X, y, C=1.0):
        """æ”¯æŒå‘é‡æœº"""
        from sklearn.svm import SVC
        svm = SVC(kernel='linear', C=C)
        svm.fit(X, y)
        return svm

# ä½¿ç”¨ç¤ºä¾‹
# ç”Ÿæˆç¤ºä¾‹æ•°æ®
np.random.seed(42)
n_samples = 300
n_features = 2

# ç”Ÿæˆèšç±»æ•°æ®
centers = [[0, 0], [2, 2], [4, 0]]
data = np.vstack([np.random.normal(center, 0.5, (n_samples//3, n_features))
                  for center in centers])

# æ•°æ®æŒ–æ˜
dm = DataMining()
labels, centroids = dm.kmeans_clustering(data, k=3)
silhouette_avg = silhouette_score(data, labels)
print(f"K-meansèšç±»è½®å»“ç³»æ•°: {silhouette_avg:.3f}")

# æ•°æ®å¯è§†åŒ–
dv = DataVisualization()
pca_data, explained_variance = dv.pca_reduction(data)
tsne_data = dv.tsne_reduction(data)

# å¯è§†åŒ–ç»“æœ
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis')
plt.title('åŸå§‹æ•°æ®èšç±»')
plt.xlabel('ç‰¹å¾1')
plt.ylabel('ç‰¹å¾2')

plt.subplot(1, 3, 2)
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=labels, cmap='viridis')
plt.title('PCAé™ç»´')
plt.xlabel('ä¸»æˆåˆ†1')
plt.ylabel('ä¸»æˆåˆ†2')

plt.subplot(1, 3, 3)
plt.scatter(tsne_data[:, 0], tsne_data[:, 1], c=labels, cmap='viridis')
plt.title('t-SNEé™ç»´')
plt.xlabel('t-SNE1')
plt.ylabel('t-SNE2')

plt.tight_layout()
plt.show()

# å¤§æ•°æ®å¤„ç†ç¤ºä¾‹
bdp = BigDataProcessing()

# MapReduceç¤ºä¾‹
data_stream = np.random.normal(0, 1, 1000)
streaming_result = bdp.streaming_algorithm(data_stream, window_size=50)

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(data_stream[:100])
plt.title('åŸå§‹æ•°æ®æµ')
plt.xlabel('æ—¶é—´')
plt.ylabel('å€¼')

plt.subplot(1, 2, 2)
plt.plot(streaming_result)
plt.title('æ»‘åŠ¨çª—å£å¹³å‡')
plt.xlabel('æ—¶é—´')
plt.ylabel('å¹³å‡å€¼')

plt.tight_layout()
plt.show()
```

### 5.2 åˆ†å¸ƒå¼è®¡ç®—å®ç°

```python
import numpy as np
from multiprocessing import Pool
import threading
import queue

# åˆ†å¸ƒå¼è®¡ç®—å®ç°
class DistributedComputing:
    def __init__(self, n_workers=4):
        self.n_workers = n_workers

    def parallel_map(self, func, data):
        """å¹¶è¡ŒMapæ“ä½œ"""
        with Pool(self.n_workers) as pool:
            results = pool.map(func, data)
        return results

    def parallel_reduce(self, data, reduce_func):
        """å¹¶è¡ŒReduceæ“ä½œ"""
        # åˆ†å—å¤„ç†
        chunk_size = len(data) // self.n_workers
        chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]

        # å¹¶è¡Œå¤„ç†
        with Pool(self.n_workers) as pool:
            chunk_results = pool.map(reduce_func, chunks)

        # åˆå¹¶ç»“æœ
        final_result = chunk_results[0]
        for result in chunk_results[1:]:
            final_result = reduce_func([final_result, result])

        return final_result

    def streaming_processor(self, data_stream, window_size=100):
        """æµå¤„ç†å™¨"""
        window = queue.Queue(maxsize=window_size)
        results = []

        def process_window():
            while True:
                try:
                    item = data_stream.get(timeout=1)
                    if window.full():
                        window.get()  # ç§»é™¤æœ€æ—§çš„é¡¹ç›®
                    window.put(item)

                    # å¤„ç†çª—å£æ•°æ®
                    window_data = list(window.queue)
                    if len(window_data) > 0:
                        result = np.mean(window_data)
                        results.append(result)
                except queue.Empty:
                    break

        # å¯åŠ¨å¤„ç†çº¿ç¨‹
        processor_thread = threading.Thread(target=process_window)
        processor_thread.start()
        processor_thread.join()

        return results

# å›¾è®¡ç®—å®ç°
class GraphComputing:
    def __init__(self):
        pass

    def pagerank(self, adjacency_matrix, damping=0.85, max_iter=100, tol=1e-6):
        """PageRankç®—æ³•"""
        n_nodes = len(adjacency_matrix)

        # åˆå§‹åŒ–PageRankå€¼
        pr = np.ones(n_nodes) / n_nodes

        # è®¡ç®—è½¬ç§»çŸ©é˜µ
        out_degrees = adjacency_matrix.sum(axis=1)
        transition_matrix = np.zeros((n_nodes, n_nodes))

        for i in range(n_nodes):
            if out_degrees[i] > 0:
                transition_matrix[i] = adjacency_matrix[i] / out_degrees[i]

        # è¿­ä»£è®¡ç®—
        for _ in range(max_iter):
            new_pr = (1 - damping) / n_nodes + damping * transition_matrix.T.dot(pr)

            if np.linalg.norm(new_pr - pr) < tol:
                break
            pr = new_pr

        return pr

    def shortest_path(self, adjacency_matrix, start_node):
        """Dijkstraæœ€çŸ­è·¯å¾„ç®—æ³•"""
        n_nodes = len(adjacency_matrix)
        distances = np.full(n_nodes, np.inf)
        distances[start_node] = 0
        visited = set()

        while len(visited) < n_nodes:
            # æ‰¾åˆ°æœªè®¿é—®çš„æœ€å°è·ç¦»èŠ‚ç‚¹
            unvisited = [i for i in range(n_nodes) if i not in visited]
            current = min(unvisited, key=lambda x: distances[x])
            visited.add(current)

            # æ›´æ–°é‚»å±…è·ç¦»
            for neighbor in range(n_nodes):
                if (adjacency_matrix[current, neighbor] > 0 and
                    neighbor not in visited):
                    new_distance = (distances[current] +
                                  adjacency_matrix[current, neighbor])
                    if new_distance < distances[neighbor]:
                        distances[neighbor] = new_distance

        return distances

# ä½¿ç”¨ç¤ºä¾‹
# åˆ†å¸ƒå¼è®¡ç®—
dc = DistributedComputing(n_workers=4)

# å¹¶è¡ŒMapç¤ºä¾‹
def square(x):
    return x**2

data = list(range(1000))
squared_data = dc.parallel_map(square, data)
print(f"å¹¶è¡Œè®¡ç®—å¹³æ–¹å’Œ: {sum(squared_data)}")

# å›¾è®¡ç®—ç¤ºä¾‹
gc = GraphComputing()

# åˆ›å»ºç¤ºä¾‹å›¾
n_nodes = 5
adjacency_matrix = np.array([
    [0, 1, 1, 0, 0],
    [1, 0, 1, 1, 0],
    [1, 1, 0, 0, 1],
    [0, 1, 0, 0, 1],
    [0, 0, 1, 1, 0]
])

# PageRankè®¡ç®—
pagerank_scores = gc.pagerank(adjacency_matrix)
print("PageRankåˆ†æ•°:", pagerank_scores)

# æœ€çŸ­è·¯å¾„è®¡ç®—
shortest_distances = gc.shortest_path(adjacency_matrix, start_node=0)
print("ä»èŠ‚ç‚¹0åˆ°å„èŠ‚ç‚¹çš„æœ€çŸ­è·ç¦»:", shortest_distances)

# å¯è§†åŒ–å›¾
import networkx as nx

G = nx.from_numpy_array(adjacency_matrix)
pos = nx.spring_layout(G)

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
nx.draw(G, pos, with_labels=True, node_color='lightblue',
        node_size=500, font_size=16, font_weight='bold')
plt.title('å›¾ç»“æ„')

plt.subplot(1, 2, 2)
node_colors = pagerank_scores
nx.draw(G, pos, with_labels=True, node_color=node_colors,
        node_size=500, font_size=16, font_weight='bold', cmap=plt.cm.viridis)
plt.title('PageRankåˆ†æ•°')
plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.viridis))

plt.tight_layout()
plt.show()
```

## 6. ğŸ¯ åº”ç”¨æ¡ˆä¾‹ / Applications

### 6.1 æ•°æ®æŒ–æ˜åº”ç”¨ / Data Mining Applications

#### 6.1.1 å®¢æˆ·ç»†åˆ† / Customer Segmentation

**åº”ç”¨åœºæ™¯**ï¼š

- ç”µå•†å¹³å°å®¢æˆ·åˆ†æ
- é›¶å”®ä¸šå¸‚åœºç»†åˆ†
- é‡‘èæœåŠ¡å®¢æˆ·åˆ†ç±»

**æ•°å­¦æ¨¡å‹**ï¼š

- K-meansèšç±»ï¼š$J = \sum_{i=1}^{k} \sum_{x \in C_i} \|x - \mu_i\|^2$
- å±‚æ¬¡èšç±»ï¼š$d(C_i, C_j) = \min_{x \in C_i, y \in C_j} d(x,y)$
- è½®å»“ç³»æ•°ï¼š$s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}$

**å®é™…ä»·å€¼**ï¼š

- è¯†åˆ«é«˜ä»·å€¼å®¢æˆ·ç¾¤ä½“
- åˆ¶å®šä¸ªæ€§åŒ–è¥é”€ç­–ç•¥
- æé«˜å®¢æˆ·æ»¡æ„åº¦å’Œå¿ è¯šåº¦

#### 6.1.2 å¼‚å¸¸æ£€æµ‹ / Anomaly Detection

**åº”ç”¨åœºæ™¯**ï¼š

- é‡‘èæ¬ºè¯ˆæ£€æµ‹
- ç½‘ç»œå®‰å…¨ç›‘æ§
- è®¾å¤‡æ•…éšœé¢„è­¦

**æ•°å­¦æ¨¡å‹**ï¼š

- ç»Ÿè®¡æ–¹æ³•ï¼š$z = \frac{x - \mu}{\sigma}$ï¼Œå¼‚å¸¸å¦‚æœ$|z| > 3$
- åŸºäºè·ç¦»ï¼š$d(x, \mu) > k \cdot \sigma$
- åŸºäºå¯†åº¦ï¼š$\text{LOF}(x) = \frac{\sum_{o \in N_k(x)} \text{lrd}_k(o)}{\text{lrd}_k(x) \cdot |N_k(x)|}$

**å®é™…ä»·å€¼**ï¼š

- åŠæ—¶å‘ç°å¼‚å¸¸è¡Œä¸º
- å‡å°‘ç»æµæŸå¤±
- æé«˜ç³»ç»Ÿå®‰å…¨æ€§

#### 6.1.3 å…³è”è§„åˆ™æŒ–æ˜ / Association Rule Mining

**åº”ç”¨åœºæ™¯**ï¼š

- è´­ç‰©ç¯®åˆ†æ
- æ¨èç³»ç»Ÿ
- äº¤å‰é”€å”®

**æ•°å­¦æ¨¡å‹**ï¼š

- æ”¯æŒåº¦ï¼š$\text{supp}(X \Rightarrow Y) = P(X \cup Y)$
- ç½®ä¿¡åº¦ï¼š$\text{conf}(X \Rightarrow Y) = P(Y|X) = \frac{P(X \cup Y)}{P(X)}$
- æå‡åº¦ï¼š$\text{lift}(X \Rightarrow Y) = \frac{P(Y|X)}{P(Y)}$

**å®é™…ä»·å€¼**ï¼š

- å‘ç°å•†å“å…³è”å…³ç³»
- ä¼˜åŒ–å•†å“å¸ƒå±€
- æé«˜é”€å”®é¢

### 6.2 æ•°æ®å¯è§†åŒ–åº”ç”¨ / Data Visualization Applications

#### 6.2.1 æ¢ç´¢æ€§æ•°æ®åˆ†æ / Exploratory Data Analysis

**åº”ç”¨åœºæ™¯**ï¼š

- æ•°æ®ç§‘å­¦é¡¹ç›®åˆæœŸåˆ†æ
- ä¸šåŠ¡æ•°æ®åˆ†æ
- ç§‘ç ”æ•°æ®æ¢ç´¢

**æ•°å­¦æ¨¡å‹**ï¼š

- PCAé™ç»´ï¼š$Y = XW$ï¼Œå…¶ä¸­$W$æ˜¯ä¸»æˆåˆ†çŸ©é˜µ
- t-SNEï¼š$p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|x_i - x_k\|^2 / 2\sigma_i^2)}$
- æµå½¢å­¦ä¹ ï¼šä¿æŒå±€éƒ¨é‚»åŸŸç»“æ„

**å®é™…ä»·å€¼**ï¼š

- å¿«é€Ÿç†è§£æ•°æ®ç‰¹å¾
- å‘ç°éšè—æ¨¡å¼
- æŒ‡å¯¼åç»­åˆ†ææ–¹å‘

#### 6.2.2 é«˜ç»´æ•°æ®å¯è§†åŒ– / High-Dimensional Data Visualization

**åº”ç”¨åœºæ™¯**ï¼š

- åŸºå› è¡¨è¾¾æ•°æ®åˆ†æ
- å›¾åƒç‰¹å¾å¯è§†åŒ–
- æ–‡æœ¬æ•°æ®é™ç»´

**æ•°å­¦æ¨¡å‹**ï¼š

- å¤šç»´ç¼©æ”¾ï¼ˆMDSï¼‰ï¼š$\min \sum_{i<j} (d_{ij} - \|\mathbf{y}_i - \mathbf{y}_j\|)^2$
- UMAPï¼šåŸºäºæµå½¢å­¦ä¹ çš„éçº¿æ€§é™ç»´
- ç­‰è·æ˜ å°„ï¼ˆIsomapï¼‰ï¼šä¿æŒæµ‹åœ°è·ç¦»

**å®é™…ä»·å€¼**ï¼š

- ç›´è§‚å±•ç¤ºé«˜ç»´æ•°æ®ç»“æ„
- å‘ç°æ•°æ®èšç±»
- æ”¯æŒäº¤äº’å¼æ¢ç´¢

#### 6.2.3 æ—¶é—´åºåˆ—å¯è§†åŒ– / Time Series Visualization

**åº”ç”¨åœºæ™¯**ï¼š

- è‚¡ç¥¨ä»·æ ¼åˆ†æ
- ä¼ æ„Ÿå™¨æ•°æ®ç›‘æ§
- ä¸šåŠ¡æŒ‡æ ‡è·Ÿè¸ª

**æ•°å­¦æ¨¡å‹**ï¼š

- ç§»åŠ¨å¹³å‡ï¼š$MA_t = \frac{1}{n}\sum_{i=0}^{n-1} x_{t-i}$
- è¶‹åŠ¿åˆ†è§£ï¼š$x_t = T_t + S_t + R_t$
- è‡ªç›¸å…³å‡½æ•°ï¼š$ACF(k) = \frac{\text{Cov}(x_t, x_{t-k})}{\text{Var}(x_t)}$

**å®é™…ä»·å€¼**ï¼š

- è¯†åˆ«è¶‹åŠ¿å’Œå‘¨æœŸæ€§
- é¢„æµ‹æœªæ¥èµ°åŠ¿
- æ”¯æŒå†³ç­–åˆ¶å®š

### 6.3 å¤§æ•°æ®å¤„ç†åº”ç”¨ / Big Data Processing Applications

#### 6.3.1 å®æ—¶æµå¤„ç† / Real-Time Stream Processing

**åº”ç”¨åœºæ™¯**ï¼š

- é‡‘èäº¤æ˜“ç›‘æ§
- ç‰©è”ç½‘æ•°æ®å¤„ç†
- ç¤¾äº¤åª’ä½“åˆ†æ

**æ•°å­¦æ¨¡å‹**ï¼š

- æ»‘åŠ¨çª—å£ï¼š$W_t = \{x_{t-w+1}, ..., x_t\}$
- æµç®—æ³•ï¼š$S_t = f(S_{t-1}, x_t)$ï¼ˆå¢é‡æ›´æ–°ï¼‰
- é‡‡æ ·ï¼š$P(\text{include } x) = p$ï¼ˆå›ºå®šæ¦‚ç‡ï¼‰

**å®é™…ä»·å€¼**ï¼š

- å®æ—¶å“åº”ä¸šåŠ¡éœ€æ±‚
- é™ä½å­˜å‚¨æˆæœ¬
- æé«˜å¤„ç†æ•ˆç‡

#### 6.3.2 åˆ†å¸ƒå¼è®¡ç®— / Distributed Computing

**åº”ç”¨åœºæ™¯**ï¼š

- å¤§è§„æ¨¡æ•°æ®åˆ†æ
- æœºå™¨å­¦ä¹ è®­ç»ƒ
- æœç´¢å¼•æ“ç´¢å¼•

**æ•°å­¦æ¨¡å‹**ï¼š

- MapReduceï¼š$\text{Map}(k, v) \rightarrow \{(k', v')\}$ï¼Œ$\text{Reduce}(k', [v']) \rightarrow v''$
- åˆ†å¸ƒå¼ä¼˜åŒ–ï¼š$\min \sum_{i=1}^{n} f_i(x)$ï¼Œ$x_{t+1} = x_t - \alpha \sum_{i=1}^{n} \nabla f_i(x_t)$
- ä¸€è‡´æ€§ç®—æ³•ï¼š$\text{Consensus} = \arg\min \sum_{i} \|x - x_i\|^2$

**å®é™…ä»·å€¼**ï¼š

- å¤„ç†PBçº§æ•°æ®
- åŠ é€Ÿè®¡ç®—è¿‡ç¨‹
- æé«˜ç³»ç»Ÿå¯æ‰©å±•æ€§

#### 6.3.3 å›¾è®¡ç®— / Graph Computing

**åº”ç”¨åœºæ™¯**ï¼š

- ç¤¾äº¤ç½‘ç»œåˆ†æ
- æ¨èç³»ç»Ÿ
- çŸ¥è¯†å›¾è°±

**æ•°å­¦æ¨¡å‹**ï¼š

- PageRankï¼š$PR(v) = \frac{1-d}{N} + d \sum_{u \in M(v)} \frac{PR(u)}{L(u)}$
- ç¤¾åŒºæ£€æµ‹ï¼š$\text{Modularity} = \frac{1}{2m} \sum_{ij} (A_{ij} - \frac{k_i k_j}{2m}) \delta(c_i, c_j)$
- æœ€çŸ­è·¯å¾„ï¼š$d(s, t) = \min_{P} \sum_{e \in P} w(e)$

**å®é™…ä»·å€¼**ï¼š

- å‘ç°ç½‘ç»œç»“æ„
- è¯†åˆ«å…³é”®èŠ‚ç‚¹
- ä¼˜åŒ–ç½‘ç»œæ€§èƒ½

### 6.4 ç»Ÿè®¡å­¦ä¹ åº”ç”¨ / Statistical Learning Applications

#### 6.4.1 é¢„æµ‹å»ºæ¨¡ / Predictive Modeling

**åº”ç”¨åœºæ™¯**ï¼š

- é”€å”®é¢„æµ‹
- éœ€æ±‚é¢„æµ‹
- é£é™©è¯„ä¼°

**æ•°å­¦æ¨¡å‹**ï¼š

- çº¿æ€§å›å½’ï¼š$y = \beta_0 + \sum_{i=1}^{p} \beta_i x_i + \epsilon$
- é€»è¾‘å›å½’ï¼š$P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \sum \beta_i x_i)}}$
- æ”¯æŒå‘é‡æœºï¼š$\min \frac{1}{2}\|w\|^2 + C\sum_{i} \xi_i$ s.t. $y_i(w^T x_i + b) \geq 1 - \xi_i$

**å®é™…ä»·å€¼**ï¼š

- å‡†ç¡®é¢„æµ‹æœªæ¥è¶‹åŠ¿
- æ”¯æŒä¸šåŠ¡å†³ç­–
- é™ä½é£é™©

#### 6.4.2 ç‰¹å¾å·¥ç¨‹ / Feature Engineering

**åº”ç”¨åœºæ™¯**ï¼š

- æœºå™¨å­¦ä¹ æ¨¡å‹ä¼˜åŒ–
- æ•°æ®é¢„å¤„ç†
- ç‰¹å¾é€‰æ‹©

**æ•°å­¦æ¨¡å‹**ï¼š

- ç‰¹å¾é€‰æ‹©ï¼š$\max I(X_i; Y)$ï¼ˆäº’ä¿¡æ¯ï¼‰
- ç‰¹å¾å˜æ¢ï¼š$x' = f(x)$ï¼ˆå¦‚å¯¹æ•°å˜æ¢ã€æ ‡å‡†åŒ–ï¼‰
- ç‰¹å¾ç»„åˆï¼š$x_{new} = g(x_i, x_j)$ï¼ˆå¦‚å¤šé¡¹å¼ç‰¹å¾ï¼‰

**å®é™…ä»·å€¼**ï¼š

- æé«˜æ¨¡å‹æ€§èƒ½
- å‡å°‘ç‰¹å¾ç»´åº¦
- é™ä½è¿‡æ‹Ÿåˆé£é™©

#### 6.4.3 æ¨¡å‹è¯„ä¼° / Model Evaluation

**åº”ç”¨åœºæ™¯**ï¼š

- æ¨¡å‹é€‰æ‹©
- æ€§èƒ½è¯„ä¼°
- A/Bæµ‹è¯•

**æ•°å­¦æ¨¡å‹**ï¼š

- äº¤å‰éªŒè¯ï¼š$CV = \frac{1}{k}\sum_{i=1}^{k} L(y_i, \hat{y}_i)$
- ROCæ›²çº¿ï¼š$TPR = \frac{TP}{TP + FN}$ï¼Œ$FPR = \frac{FP}{FP + TN}$
- æ··æ·†çŸ©é˜µï¼š$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$

**å®é™…ä»·å€¼**ï¼š

- å®¢è§‚è¯„ä¼°æ¨¡å‹æ€§èƒ½
- é€‰æ‹©æœ€ä¼˜æ¨¡å‹
- æŒ‡å¯¼æ¨¡å‹æ”¹è¿›

### 6.5 è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ / Natural Language Processing Applications

#### 6.5.1 æ–‡æœ¬åˆ†ç±» / Text Classification

**åº”ç”¨åœºæ™¯**ï¼š

- åƒåœ¾é‚®ä»¶æ£€æµ‹
- æƒ…æ„Ÿåˆ†æ
- æ–°é—»åˆ†ç±»

**æ•°å­¦æ¨¡å‹**ï¼š

- è¯è¢‹æ¨¡å‹ï¼š$x = [\text{count}(w_1), ..., \text{count}(w_n)]$
- TF-IDFï¼š$\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \log \frac{N}{\text{DF}(t)}$
- æ–‡æœ¬å‘é‡åŒ–ï¼š$x = \text{embedding}(\text{text})$

**å®é™…ä»·å€¼**ï¼š

- è‡ªåŠ¨åˆ†ç±»å¤§é‡æ–‡æœ¬
- æé«˜å¤„ç†æ•ˆç‡
- æ”¯æŒå†…å®¹ç®¡ç†

#### 6.5.2 ä¸»é¢˜å»ºæ¨¡ / Topic Modeling

**åº”ç”¨åœºæ™¯**ï¼š

- æ–‡æ¡£ä¸»é¢˜å‘ç°
- å†…å®¹æ¨è
- ä¿¡æ¯æ£€ç´¢

**æ•°å­¦æ¨¡å‹**ï¼š

- LDAï¼š$P(w|d) = \sum_{z} P(w|z) P(z|d)$
- ä¸»é¢˜åˆ†å¸ƒï¼š$\theta_d \sim \text{Dir}(\alpha)$
- è¯åˆ†å¸ƒï¼š$\phi_z \sim \text{Dir}(\beta)$

**å®é™…ä»·å€¼**ï¼š

- å‘ç°æ–‡æ¡£ä¸»é¢˜ç»“æ„
- æ”¯æŒå†…å®¹æ¨è
- æé«˜æ£€ç´¢æ•ˆç‡

### 6.6 æ¨èç³»ç»Ÿåº”ç”¨ / Recommendation System Applications

#### 6.6.1 ååŒè¿‡æ»¤ / Collaborative Filtering

**åº”ç”¨åœºæ™¯**ï¼š

- ç”µå•†æ¨è
- è§†é¢‘æ¨è
- éŸ³ä¹æ¨è

**æ•°å­¦æ¨¡å‹**ï¼š

- ç”¨æˆ·ç›¸ä¼¼åº¦ï¼š$\text{sim}(u, v) = \frac{\sum_{i} (r_{ui} - \bar{r}_u)(r_{vi} - \bar{r}_v)}{\sqrt{\sum_{i} (r_{ui} - \bar{r}_u)^2} \sqrt{\sum_{i} (r_{vi} - \bar{r}_v)^2}}$
- é¢„æµ‹è¯„åˆ†ï¼š$\hat{r}_{ui} = \bar{r}_u + \frac{\sum_{v \in N(u)} \text{sim}(u, v)(r_{vi} - \bar{r}_v)}{\sum_{v \in N(u)} |\text{sim}(u, v)|}$
- çŸ©é˜µåˆ†è§£ï¼š$R \approx U \times V^T$

**å®é™…ä»·å€¼**ï¼š

- æé«˜ç”¨æˆ·æ»¡æ„åº¦
- å¢åŠ é”€å”®é¢
- æ”¹å–„ç”¨æˆ·ä½“éªŒ

#### 6.6.2 æ·±åº¦å­¦ä¹ æ¨è / Deep Learning Recommendation

**åº”ç”¨åœºæ™¯**ï¼š

- ä¸ªæ€§åŒ–æ¨è
- å†…å®¹æ¨è
- å¹¿å‘Šæ¨è

**æ•°å­¦æ¨¡å‹**ï¼š

- ç¥ç»ç½‘ç»œï¼š$y = f(W_2 \cdot \text{ReLU}(W_1 x + b_1) + b_2)$
- åµŒå…¥å±‚ï¼š$e_i = \text{Embedding}(i)$
- æ³¨æ„åŠ›æœºåˆ¶ï¼š$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$

**å®é™…ä»·å€¼**ï¼š

- æ•è·å¤æ‚ç‰¹å¾äº¤äº’
- æé«˜æ¨èå‡†ç¡®æ€§
- æ”¯æŒå¤šæ¨¡æ€æ¨è

### 6.7 æ—¶é—´åºåˆ—åˆ†æåº”ç”¨ / Time Series Analysis Applications

#### 6.7.1 é¢„æµ‹åˆ†æ / Forecasting

**åº”ç”¨åœºæ™¯**ï¼š

- è‚¡ç¥¨ä»·æ ¼é¢„æµ‹
- éœ€æ±‚é¢„æµ‹
- å¤©æ°”é¢„æµ‹

**æ•°å­¦æ¨¡å‹**ï¼š

- ARIMAï¼š$(1 - \phi_1 L - ... - \phi_p L^p)(1 - L)^d y_t = (1 + \theta_1 L + ... + \theta_q L^q) \epsilon_t$
- LSTMï¼š$h_t = \text{LSTM}(x_t, h_{t-1})$
- Prophetï¼š$y(t) = g(t) + s(t) + h(t) + \epsilon_t$

**å®é™…ä»·å€¼**ï¼š

- å‡†ç¡®é¢„æµ‹æœªæ¥è¶‹åŠ¿
- æ”¯æŒä¸šåŠ¡è§„åˆ’
- é™ä½åº“å­˜æˆæœ¬

#### 6.7.2 å¼‚å¸¸æ£€æµ‹ / Anomaly Detection in Time Series

**åº”ç”¨åœºæ™¯**ï¼š

- è®¾å¤‡æ•…éšœæ£€æµ‹
- ç½‘ç»œå…¥ä¾µæ£€æµ‹
- é‡‘èæ¬ºè¯ˆæ£€æµ‹

**æ•°å­¦æ¨¡å‹**ï¼š

- ç»Ÿè®¡æ§åˆ¶å›¾ï¼š$UCL = \mu + 3\sigma$ï¼Œ$LCL = \mu - 3\sigma$
- è‡ªç¼–ç å™¨ï¼š$\min \|x - \text{Decode}(\text{Encode}(x))\|^2$
- å­¤ç«‹æ£®æ—ï¼š$\text{Anomaly Score} = 2^{-\frac{E(h(x))}{c(n)}}$

**å®é™…ä»·å€¼**ï¼š

- åŠæ—¶å‘ç°å¼‚å¸¸
- å‡å°‘æŸå¤±
- æé«˜ç³»ç»Ÿå¯é æ€§

## 7. å‰æ²¿å‘å±•

### 7.1 æ·±åº¦å­¦ä¹ ä¸æ•°æ®ç§‘å­¦

**æ·±åº¦èšç±»**ï¼š

- è‡ªç¼–ç å™¨ç”¨äºé™ç»´èšç±»
- æ·±åº¦åµŒå…¥èšç±»ç®—æ³•
- ç«¯åˆ°ç«¯èšç±»å­¦ä¹ 

### 7.2 å›¾ç¥ç»ç½‘ç»œ

**å›¾å·ç§¯ç½‘ç»œ**ï¼š

- å›¾å·ç§¯å±‚è®¾è®¡
- å›¾æ³¨æ„åŠ›æœºåˆ¶
- å›¾ç¥ç»ç½‘ç»œåº”ç”¨

### 7.3 è”é‚¦å­¦ä¹ 

**åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ **ï¼š

- è”é‚¦å¹³å‡ç®—æ³•
- å·®åˆ†éšç§ä¿æŠ¤
- è”é‚¦å­¦ä¹ ä¼˜åŒ–

## 8. æ€»ç»“ä¸å±•æœ›

### 8.1 æ ¸å¿ƒè¦ç‚¹æ€»ç»“

1. **æ•°æ®æŒ–æ˜æ•°å­¦åŸºç¡€**ï¼š
   - èšç±»åˆ†æçš„æ•°å­¦åŸç†å’Œç®—æ³•
   - å…³è”è§„åˆ™æŒ–æ˜çš„ç»Ÿè®¡æ–¹æ³•
   - å¼‚å¸¸æ£€æµ‹çš„æ•°å­¦æ¨¡å‹

2. **æ•°æ®å¯è§†åŒ–æ•°å­¦ç†è®º**ï¼š
   - é™ç»´æŠ€æœ¯çš„æ•°å­¦åŸç†
   - æµå½¢å­¦ä¹ çš„å‡ ä½•æ–¹æ³•
   - æ‹“æ‰‘æ•°æ®åˆ†æçš„ä»£æ•°å·¥å…·

3. **å¤§æ•°æ®å¤„ç†æ•°å­¦åŸç†**ï¼š
   - åˆ†å¸ƒå¼è®¡ç®—çš„æ•°å­¦æ¨¡å‹
   - æµå¤„ç†çš„ç®—æ³•è®¾è®¡
   - å›¾è®¡ç®—çš„æ•°å­¦ç†è®º

4. **ç»Ÿè®¡å­¦ä¹ æ•°å­¦æ–¹æ³•**ï¼š
   - ç›‘ç£å­¦ä¹ çš„æ•°å­¦åŸºç¡€
   - æ— ç›‘ç£å­¦ä¹ çš„ç»Ÿè®¡ç†è®º
   - å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–æ–¹æ³•

### 8.2 å‘å±•è¶‹åŠ¿

1. **æŠ€æœ¯å‘å±•**ï¼š
   - æ·±åº¦å­¦ä¹ ä¸æ•°æ®ç§‘å­¦çš„èåˆ
   - å›¾ç¥ç»ç½‘ç»œçš„å‘å±•
   - è”é‚¦å­¦ä¹ çš„åº”ç”¨

2. **æ–¹æ³•åˆ›æ–°**ï¼š
   - å¯è§£é‡Šæœºå™¨å­¦ä¹ 
   - è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ 
   - å› æœæ¨æ–­æ–¹æ³•

3. **åº”ç”¨æ‹“å±•**ï¼š
   - ç”Ÿç‰©ä¿¡æ¯å­¦åº”ç”¨
   - é‡‘èç§‘æŠ€åº”ç”¨
   - ç¤¾ä¼šç§‘å­¦åº”ç”¨

### 8.3 æŒ‘æˆ˜ä¸æœºé‡

**ä¸»è¦æŒ‘æˆ˜**ï¼š

- å¤§è§„æ¨¡æ•°æ®çš„è®¡ç®—å¤æ‚åº¦
- æ•°æ®éšç§å’Œå®‰å…¨é—®é¢˜
- æ¨¡å‹å¯è§£é‡Šæ€§éœ€æ±‚

**å‘å±•æœºé‡**ï¼š

- äººå·¥æ™ºèƒ½ä¸æ•°æ®ç§‘å­¦çš„ç»“åˆ
- è·¨å­¦ç§‘åº”ç”¨çš„æ‹“å±•
- æ–°æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springer.
2. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
3. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.
4. Leskovec, J., Rajaraman, A., & Ullman, J. D. (2014). Mining of Massive Datasets. Cambridge University Press.
5. Wasserman, L. (2013). All of Statistics: A Concise Course in Statistical Inference. Springer.

## ğŸ”— ç›¸å…³é“¾æ¥

- [æ¦‚ç‡è®ºåŸºç¡€](./01-æ¦‚ç‡è®º.md)
- [ç»Ÿè®¡å­¦åŸºç¡€](./02-ç»Ÿè®¡å­¦.md)
- [äººå·¥æ™ºèƒ½æ•°å­¦](./07-äººå·¥æ™ºèƒ½æ•°å­¦-æ·±åŒ–ç‰ˆ.md)
- [ç½‘ç»œç§‘å­¦æ•°å­¦](./09-ç½‘ç»œç§‘å­¦æ•°å­¦-æ·±åŒ–ç‰ˆ.md)

---

*æœ¬æ·±åŒ–ç‰ˆæ–‡æ¡£æ·±å…¥æ¢è®¨äº†æ•°æ®ç§‘å­¦çš„æ•°å­¦ç†è®ºåŸºç¡€ï¼Œä¸ºç†è§£æ•°æ®æŒ–æ˜ã€æ•°æ®å¯è§†åŒ–ã€å¤§æ•°æ®å¤„ç†æä¾›äº†å¼ºå¤§çš„æ•°å­¦å·¥å…·ã€‚*
