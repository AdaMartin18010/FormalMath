# äººå·¥æ™ºèƒ½æ•°å­¦ - æ·±åŒ–ç‰ˆ

## ğŸ“‹ æ¦‚è¿°

äººå·¥æ™ºèƒ½æ•°å­¦æ˜¯ç ”ç©¶äººå·¥æ™ºèƒ½ç®—æ³•å’ŒæŠ€æœ¯çš„æ•°å­¦åŸºç¡€ï¼ŒåŒ…æ‹¬æ·±åº¦å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ã€ä¼˜åŒ–ç®—æ³•ç­‰æ ¸å¿ƒé¢†åŸŸçš„æ•°å­¦ç†è®ºã€‚
æœ¬æ·±åŒ–ç‰ˆå°†é‡ç‚¹æ¢è®¨äººå·¥æ™ºèƒ½çš„æ•°å­¦åŸç†ã€ç†è®ºåŸºç¡€å’Œå‰æ²¿å‘å±•ã€‚

## ğŸ¯ æ ¸å¿ƒç†è®ºä½“ç³»

### 1. æ·±åº¦å­¦ä¹ æ•°å­¦ç†è®º

#### 1.1 ç¥ç»ç½‘ç»œæ•°å­¦åŸºç¡€

**æ¿€æ´»å‡½æ•°æ•°å­¦ç†è®º**:

æ¿€æ´»å‡½æ•°æ˜¯ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç»„ä»¶ï¼Œå…¶æ•°å­¦æ€§è´¨ç›´æ¥å½±å“ç½‘ç»œçš„å­¦ä¹ èƒ½åŠ›ï¼š

**Sigmoidå‡½æ•°**ï¼š
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

**æ•°å­¦æ€§è´¨**ï¼š

- å€¼åŸŸï¼š$(0, 1)$
- å•è°ƒé€’å¢
- è¿ç»­å¯å¯¼
- å¯¼æ•°ï¼š$\sigma'(x) = \sigma(x)(1 - \sigma(x))$

**ReLUå‡½æ•°**ï¼š
$$\text{ReLU}(x) = \max(0, x)$$

**æ•°å­¦æ€§è´¨**ï¼š

- å€¼åŸŸï¼š$[0, +\infty)$
- åˆ†æ®µçº¿æ€§
- å¯¼æ•°ï¼š$\text{ReLU}'(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}$

**Tanhå‡½æ•°**ï¼š
$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

**æ•°å­¦æ€§è´¨**ï¼š

- å€¼åŸŸï¼š$(-1, 1)$
- å¥‡å‡½æ•°
- å¯¼æ•°ï¼š$\tanh'(x) = 1 - \tanh^2(x)$

**åå‘ä¼ æ’­æ•°å­¦åŸç†**:

åå‘ä¼ æ’­æ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒçš„æ ¸å¿ƒç®—æ³•ï¼ŒåŸºäºé“¾å¼æ³•åˆ™ï¼š

**å‰å‘ä¼ æ’­**ï¼š
$$z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)}$$
$$a^{(l)} = \sigma(z^{(l)})$$

**åå‘ä¼ æ’­**ï¼š
$$\delta^{(l)} = \frac{\partial J}{\partial z^{(l)}} = \frac{\partial J}{\partial a^{(l)}} \odot \sigma'(z^{(l)})$$

**æƒé‡æ¢¯åº¦**ï¼š
$$\frac{\partial J}{\partial W^{(l)}} = \delta^{(l)}(a^{(l-1)})^T$$

**åç½®æ¢¯åº¦**ï¼š
$$\frac{\partial J}{\partial b^{(l)}} = \delta^{(l)}$$

**æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸æ•°å­¦åˆ†æ**:

æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„å…³é”®é—®é¢˜ï¼š

**æ¢¯åº¦æ¶ˆå¤±æ¡ä»¶**ï¼š
å½“ $|\sigma'(z)| < 1$ æ—¶ï¼Œæ¢¯åº¦ä¼šæŒ‡æ•°çº§è¡°å‡ï¼š
$$|\delta^{(l)}| \propto |\sigma'(z)|^{L-l}$$

**æ¢¯åº¦çˆ†ç‚¸æ¡ä»¶**ï¼š
å½“ $|W^{(l)}| > 1$ æ—¶ï¼Œæ¢¯åº¦ä¼šæŒ‡æ•°çº§å¢é•¿ï¼š
$$|\delta^{(l)}| \propto |W^{(l)}|^{L-l}$$

**æƒé‡åˆå§‹åŒ–æ•°å­¦ç†è®º**:

åˆé€‚çš„æƒé‡åˆå§‹åŒ–å¯¹ç½‘ç»œè®­ç»ƒè‡³å…³é‡è¦ï¼š

**Xavieråˆå§‹åŒ–**ï¼š
$$W_{ij} \sim \mathcal{N}(0, \frac{2}{n_{in} + n_{out}})$$

**Heåˆå§‹åŒ–**ï¼š
$$W_{ij} \sim \mathcal{N}(0, \frac{2}{n_{in}})$$

#### 1.2 å·ç§¯ç¥ç»ç½‘ç»œæ•°å­¦æ·±åŒ–

**å·ç§¯è¿ç®—æ•°å­¦åŸç†**:

å·ç§¯è¿ç®—æ˜¯CNNçš„æ ¸å¿ƒæ“ä½œï¼š

**ä¸€ç»´å·ç§¯**ï¼š
$$(f * g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t - \tau)d\tau$$

**äºŒç»´å·ç§¯**ï¼š
$$(I * K)(i, j) = \sum_{m} \sum_{n} I(m, n)K(i-m, j-n)$$

**å·ç§¯çš„æ•°å­¦æ€§è´¨**ï¼š

- çº¿æ€§æ€§ï¼š$(af + bg) * h = a(f * h) + b(g * h)$
- äº¤æ¢å¾‹ï¼š$f * g = g * f$
- ç»“åˆå¾‹ï¼š$(f * g) * h = f * (g * h)$

**æ± åŒ–æ“ä½œæ•°å­¦åˆ†æ**:

æ± åŒ–æ“ä½œç”¨äºé™ç»´å’Œç‰¹å¾æå–ï¼š

**æœ€å¤§æ± åŒ–**ï¼š
$$\text{MaxPool}(I)_{i,j} = \max_{(m,n) \in R_{i,j}} I(m, n)$$

**å¹³å‡æ± åŒ–**ï¼š
$$\text{AvgPool}(I)_{i,j} = \frac{1}{|R_{i,j}|} \sum_{(m,n) \in R_{i,j}} I(m, n)$$

**æ„Ÿå—é‡æ•°å­¦ç†è®º**:

æ„Ÿå—é‡æè¿°äº†è¾“å‡ºç‰¹å¾å›¾ä¸­æ¯ä¸ªåƒç´ å¯¹åº”çš„è¾“å…¥åŒºåŸŸï¼š

**æ„Ÿå—é‡è®¡ç®—å…¬å¼**ï¼š
$$RF_{l} = RF_{l-1} + (k_l - 1) \prod_{i=1}^{l-1} s_i$$

å…¶ä¸­ï¼š

- $RF_l$ æ˜¯ç¬¬$l$å±‚çš„æ„Ÿå—é‡å¤§å°
- $k_l$ æ˜¯ç¬¬$l$å±‚çš„å·ç§¯æ ¸å¤§å°
- $s_i$ æ˜¯ç¬¬$i$å±‚çš„æ­¥é•¿

**å·ç§¯ç½‘ç»œæ¶æ„æ•°å­¦**:

**ResNetæ®‹å·®è¿æ¥**ï¼š
$$H(x) = F(x) + x$$

å…¶ä¸­ $F(x)$ æ˜¯æ®‹å·®å‡½æ•°ï¼Œ$x$ æ˜¯æ’ç­‰æ˜ å°„ã€‚

**æ•°å­¦ä¼˜åŠ¿**ï¼š

- ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
- ä¿æŒä¿¡æ¯æµ
- æ˜“äºä¼˜åŒ–

#### 1.3 å¾ªç¯ç¥ç»ç½‘ç»œæ•°å­¦æ·±åŒ–

**åºåˆ—å»ºæ¨¡æ•°å­¦ç†è®º**:

RNNç”¨äºå¤„ç†åºåˆ—æ•°æ®ï¼Œå…¶æ•°å­¦è¡¨ç¤ºï¼š

**ç®€å•RNN**ï¼š
$$h_t = \tanh(W_h h_{t-1} + W_x x_t + b_h)$$
$$y_t = W_y h_t + b_y$$

**LSTMæ•°å­¦åŸç†**:

LSTMé€šè¿‡é—¨æ§æœºåˆ¶è§£å†³é•¿æœŸä¾èµ–é—®é¢˜ï¼š

**é—å¿˜é—¨**ï¼š
$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$

**è¾“å…¥é—¨**ï¼š
$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$

**å€™é€‰å€¼**ï¼š
$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

**ç»†èƒçŠ¶æ€æ›´æ–°**ï¼š
$$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$$

**è¾“å‡ºé—¨**ï¼š
$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$

**éšè—çŠ¶æ€**ï¼š
$$h_t = o_t \odot \tanh(C_t)$$

**é—¨æ§å¾ªç¯å•å…ƒ(GRU)æ•°å­¦**:

GRUæ˜¯LSTMçš„ç®€åŒ–ç‰ˆæœ¬ï¼š

**æ›´æ–°é—¨**ï¼š
$$z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)$$

**é‡ç½®é—¨**ï¼š
$$r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)$$

**å€™é€‰éšè—çŠ¶æ€**ï¼š
$$\tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h)$$

**éšè—çŠ¶æ€æ›´æ–°**ï¼š
$$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$$

**æ³¨æ„åŠ›æœºåˆ¶æ•°å­¦ç†è®º**:

æ³¨æ„åŠ›æœºåˆ¶æ˜¯Transformerçš„æ ¸å¿ƒï¼š

**æŸ¥è¯¢ã€é”®ã€å€¼**ï¼š
$$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$$

**æ³¨æ„åŠ›æƒé‡**ï¼š
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**å¤šå¤´æ³¨æ„åŠ›**ï¼š
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

å…¶ä¸­ï¼š
$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

#### 1.4 ç”Ÿæˆæ¨¡å‹æ•°å­¦æ·±åŒ–

**ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GAN)æ•°å­¦ç†è®º**:

GANåŒ…å«ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä¸¤ä¸ªç½‘ç»œï¼š

**ç”Ÿæˆå™¨**ï¼š
$$G: \mathcal{Z} \rightarrow \mathcal{X}$$
$$G(z) = x_{fake}$$

**åˆ¤åˆ«å™¨**ï¼š
$$D: \mathcal{X} \rightarrow [0, 1]$$
$$D(x) = \text{æ¦‚ç‡}(x \text{æ˜¯çœŸå®æ•°æ®})$$

**GANç›®æ ‡å‡½æ•°**ï¼š
$$\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]$$

**æ•°å­¦åˆ†æ**ï¼š

- å½“åˆ¤åˆ«å™¨æœ€ä¼˜æ—¶ï¼š$D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$
- å½“ç”Ÿæˆå™¨æœ€ä¼˜æ—¶ï¼š$p_g = p_{data}$

**å˜åˆ†è‡ªç¼–ç å™¨(VAE)æ•°å­¦åŸç†**:

VAEé€šè¿‡å˜åˆ†æ¨æ–­å­¦ä¹ æ•°æ®åˆ†å¸ƒï¼š

**ç¼–ç å™¨**ï¼š
$$q_\phi(z|x) = \mathcal{N}(\mu_\phi(x), \sigma_\phi^2(x))$$

**è§£ç å™¨**ï¼š
$$p_\theta(x|z) = \mathcal{N}(\mu_\theta(z), \sigma_\theta^2(z))$$

**ELBOç›®æ ‡å‡½æ•°**ï¼š
$$\mathcal{L}(\theta, \phi) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) \| p(z))$$

**æ‰©æ•£æ¨¡å‹æ•°å­¦åˆ†æ**:

æ‰©æ•£æ¨¡å‹é€šè¿‡é€æ­¥å»å™ªç”Ÿæˆæ•°æ®ï¼š

**å‰å‘è¿‡ç¨‹**ï¼š
$$q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)$$

**åå‘è¿‡ç¨‹**ï¼š
$$p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$$

**è®­ç»ƒç›®æ ‡**ï¼š
$$\mathcal{L} = \mathbb{E}_{t,x_0,\epsilon}[\|\epsilon - \epsilon_\theta(x_t, t)\|^2]$$

### 2. å¼ºåŒ–å­¦ä¹ æ•°å­¦ç†è®º

#### 2.1 é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æ•°å­¦æ·±åŒ–

**çŠ¶æ€è½¬ç§»æ•°å­¦ç†è®º**:

MDPæ˜¯å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸºç¡€ï¼š

**çŠ¶æ€è½¬ç§»æ¦‚ç‡**ï¼š
$$P(s'|s, a) = \mathbb{P}(S_{t+1} = s'|S_t = s, A_t = a)$$

**å¥–åŠ±å‡½æ•°æ•°å­¦åˆ†æ**ï¼š
$$R(s, a, s') = \mathbb{E}[R_{t+1}|S_t = s, A_t = a, S_{t+1} = s']$$

**ç­–ç•¥å‡½æ•°æ•°å­¦åŸç†**ï¼š
$$\pi(a|s) = \mathbb{P}(A_t = a|S_t = s)$$

**ä»·å€¼å‡½æ•°æ•°å­¦ç†è®º**ï¼š

**çŠ¶æ€ä»·å€¼å‡½æ•°**ï¼š
$$V^\pi(s) = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t = s\right]$$

**åŠ¨ä½œä»·å€¼å‡½æ•°**ï¼š
$$Q^\pi(s, a) = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t = s, A_t = a\right]$$

#### 2.2 åŠ¨æ€è§„åˆ’æ•°å­¦æ·±åŒ–

**è´å°”æ›¼æ–¹ç¨‹æ•°å­¦åŸç†**:

**çŠ¶æ€ä»·å€¼è´å°”æ›¼æ–¹ç¨‹**ï¼š
$$V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s'} P[s'|s, a](R(s, a, s') + \gamma V^\pi(s'))$$

**åŠ¨ä½œä»·å€¼è´å°”æ›¼æ–¹ç¨‹**ï¼š
$$Q^\pi(s, a) = \sum_{s'} P[s'|s, a](R(s, a, s') + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s', a'))$$

**æœ€ä¼˜è´å°”æ›¼æ–¹ç¨‹**ï¼š
$$V^*(s) = \max_a \sum_{s'} P[s'|s, a](R(s, a, s') + \gamma V^*(s'))$$

**ç­–ç•¥è¿­ä»£æ•°å­¦ç†è®º**ï¼š

**ç­–ç•¥è¯„ä¼°**ï¼š
$$V_{k+1}(s) = \sum_{a} \pi(a|s) \sum_{s'} P[s'|s, a](R(s, a, s') + \gamma V_k(s'))$$

**ç­–ç•¥æ”¹è¿›**ï¼š
$$\pi'(s) = \arg\max_a \sum_{s'} P[s'|s, a](R(s, a, s') + \gamma V^\pi(s'))$$

**ä»·å€¼è¿­ä»£æ•°å­¦åˆ†æ**ï¼š
$$V_{k+1}(s) = \max_a \sum_{s'} P[s'|s, a](R(s, a, s') + \gamma V_k(s'))$$

#### 2.3 è’™ç‰¹å¡æ´›æ–¹æ³•æ•°å­¦æ·±åŒ–

**é‡‡æ ·ç†è®ºæ•°å­¦åŸç†**:

è’™ç‰¹å¡æ´›æ–¹æ³•åŸºäºé‡‡æ ·ä¼°è®¡æœŸæœ›ï¼š

**æœŸæœ›ä¼°è®¡**ï¼š
$$\mathbb{E}[X] \approx \frac{1}{n} \sum_{i=1}^{n} x_i$$

**ä¼°è®¡ç†è®ºæ•°å­¦åˆ†æ**ï¼š

**æ— åä¼°è®¡**ï¼š
$$\mathbb{E}[\hat{\mu}] = \mu$$

**æ–¹å·®**ï¼š
$$\text{Var}[\hat{\mu}] = \frac{\sigma^2}{n}$$

**æ–¹å·®å‡å°‘æ•°å­¦æ–¹æ³•**ï¼š

**é‡è¦æ€§é‡‡æ ·**ï¼š
$$\mathbb{E}_p[f(x)] = \mathbb{E}_q\left[\frac{p(x)}{q(x)}f(x)\right]$$

**æ§åˆ¶å˜é‡æ³•**ï¼š
$$\mathbb{E}[f(x)] = \mathbb{E}[f(x) - g(x)] + \mathbb{E}[g(x)]$$

#### 2.4 æ—¶åºå·®åˆ†å­¦ä¹ æ•°å­¦æ·±åŒ–

**TDè¯¯å·®æ•°å­¦ç†è®º**:

TDå­¦ä¹ ç»“åˆäº†è’™ç‰¹å¡æ´›å’ŒåŠ¨æ€è§„åˆ’ï¼š

**TDè¯¯å·®**ï¼š
$$\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$$

**TDå­¦ä¹ æ›´æ–°**ï¼š
$$V(S_t) \leftarrow V(S_t) + \alpha \delta_t$$

**Qå­¦ä¹ æ•°å­¦åŸç†**ï¼š

**Qå­¦ä¹ æ›´æ–°**ï¼š
$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha[R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$$

**SARSAç®—æ³•æ•°å­¦åˆ†æ**ï¼š

**SARSAæ›´æ–°**ï¼š
$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha[R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$$

**å‡½æ•°é€¼è¿‘æ•°å­¦ç†è®º**ï¼š

**çº¿æ€§å‡½æ•°é€¼è¿‘**ï¼š
$$Q(s, a) = \mathbf{w}^T \phi(s, a)$$

**æ›´æ–°è§„åˆ™**ï¼š
$$\mathbf{w} \leftarrow \mathbf{w} + \alpha \delta_t \phi(S_t, A_t)$$

### 3. ä¼˜åŒ–ç®—æ³•æ•°å­¦ç†è®º

#### 3.1 æ¢¯åº¦ä¸‹é™æ•°å­¦æ·±åŒ–

**æ¢¯åº¦ç†è®ºæ•°å­¦åŸç†**:

æ¢¯åº¦ä¸‹é™æ˜¯æœ€åŸºç¡€çš„ä¼˜åŒ–ç®—æ³•ï¼š

**æ¢¯åº¦å®šä¹‰**ï¼š
$$\nabla f(x) = \left[\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n}\right]^T$$

**æ¢¯åº¦ä¸‹é™æ›´æ–°**ï¼š
$$x_{t+1} = x_t - \alpha \nabla f(x_t)$$

**å­¦ä¹ ç‡æ•°å­¦åˆ†æ**ï¼š

**å›ºå®šå­¦ä¹ ç‡**ï¼š
$$\alpha_t = \alpha$$

**è¡°å‡å­¦ä¹ ç‡**ï¼š
$$\alpha_t = \alpha_0 \cdot \text{decay}^t$$

**æ”¶æ•›æ€§æ•°å­¦ç†è®º**ï¼š

**Lipschitzè¿ç»­æ€§**ï¼š
$$\|\nabla f(x) - \nabla f(y)\| \leq L\|x - y\|$$

**æ”¶æ•›å®šç†**ï¼š
å¦‚æœ $f$ æ˜¯å‡¸å‡½æ•°ä¸”Lipschitzè¿ç»­ï¼Œåˆ™ï¼š
$$\|\nabla f(x_t)\|^2 \leq \frac{2L(f(x_0) - f^*)}{t}$$

**éšæœºæ¢¯åº¦ä¸‹é™æ•°å­¦**ï¼š

**éšæœºæ¢¯åº¦**ï¼š
$$\nabla f_i(x) = \nabla f(x; \xi_i)$$

**SGDæ›´æ–°**ï¼š
$$x_{t+1} = x_t - \alpha_t \nabla f_i(x_t)$$

#### 3.2 è‡ªé€‚åº”ä¼˜åŒ–ç®—æ³•æ•°å­¦æ·±åŒ–

**Adamç®—æ³•æ•°å­¦åŸç†**:

Adamç»“åˆäº†åŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼š

**ä¸€é˜¶çŸ©ä¼°è®¡**ï¼š
$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$

**äºŒé˜¶çŸ©ä¼°è®¡**ï¼š
$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$

**åå·®ä¿®æ­£**ï¼š
$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$

**æ›´æ–°è§„åˆ™**ï¼š
$$x_{t+1} = x_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$

**RMSpropç®—æ³•æ•°å­¦åˆ†æ**ï¼š

**ç§»åŠ¨å¹³å‡**ï¼š
$$v_t = \rho v_{t-1} + (1 - \rho) g_t^2$$

**æ›´æ–°è§„åˆ™**ï¼š
$$x_{t+1} = x_t - \frac{\alpha}{\sqrt{v_t} + \epsilon} g_t$$

**AdaGradç®—æ³•æ•°å­¦ç†è®º**ï¼š

**ç´¯ç§¯æ¢¯åº¦å¹³æ–¹**ï¼š
$$G_t = G_{t-1} + g_t^2$$

**æ›´æ–°è§„åˆ™**ï¼š
$$x_{t+1} = x_t - \frac{\alpha}{\sqrt{G_t} + \epsilon} g_t$$

**åŠ¨é‡æ–¹æ³•æ•°å­¦åŸç†**ï¼š

**åŠ¨é‡æ›´æ–°**ï¼š
$$v_t = \mu v_{t-1} + \alpha g_t$$
$$x_{t+1} = x_t - v_t$$

#### 3.3 äºŒé˜¶ä¼˜åŒ–æ–¹æ³•æ•°å­¦æ·±åŒ–

**ç‰›é¡¿æ³•æ•°å­¦ç†è®º**:

ç‰›é¡¿æ³•ä½¿ç”¨äºŒé˜¶ä¿¡æ¯åŠ é€Ÿæ”¶æ•›ï¼š

**ç‰›é¡¿æ›´æ–°**ï¼š
$$x_{t+1} = x_t - H^{-1}(x_t) \nabla f(x_t)$$

å…¶ä¸­ $H(x)$ æ˜¯HessiançŸ©é˜µï¼š
$$H_{ij}(x) = \frac{\partial^2 f}{\partial x_i \partial x_j}$$

**æ”¶æ•›æ€§è´¨**ï¼š

- äºŒæ¬¡æ”¶æ•›
- éœ€è¦è®¡ç®—å’Œå­˜å‚¨HessiançŸ©é˜µ
- å¯¹éå‡¸å‡½æ•°å¯èƒ½ä¸æ”¶æ•›

**æ‹Ÿç‰›é¡¿æ³•æ•°å­¦åŸç†**:

æ‹Ÿç‰›é¡¿æ³•é¿å…è®¡ç®—HessiançŸ©é˜µï¼š

**BFGSæ›´æ–°**ï¼š
$$H_{t+1} = H_t + \frac{y_t y_t^T}{y_t^T s_t} - \frac{H_t s_t s_t^T H_t}{s_t^T H_t s_t}$$

å…¶ä¸­ï¼š
$$s_t = x_{t+1} - x_t, \quad y_t = \nabla f(x_{t+1}) - \nabla f(x_t)$$

**L-BFGSç®—æ³•**ï¼š

- åªå­˜å‚¨æœ€è¿‘çš„$m$ä¸ªå‘é‡å¯¹
- å†…å­˜å¤æ‚åº¦ï¼š$O(mn)$
- è®¡ç®—å¤æ‚åº¦ï¼š$O(mn)$

**å…±è½­æ¢¯åº¦æ³•æ•°å­¦åˆ†æ**ï¼š

**å…±è½­æ–¹å‘**ï¼š
$$p_t^T A p_{t-1} = 0$$

**æ›´æ–°è§„åˆ™**ï¼š
$$x_{t+1} = x_t + \alpha_t p_t$$
$$p_{t+1} = -\nabla f(x_{t+1}) + \beta_t p_t$$

å…¶ä¸­ï¼š
$$\beta_t = \frac{\|\nabla f(x_{t+1})\|^2}{\|\nabla f(x_t)\|^2}$$

#### 3.4 çº¦æŸä¼˜åŒ–æ•°å­¦æ·±åŒ–

**æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•æ•°å­¦ç†è®º**:

**æ‹‰æ ¼æœ—æ—¥å‡½æ•°**ï¼š
$$\mathcal{L}(x, \lambda) = f(x) + \sum_{i=1}^{m} \lambda_i g_i(x)$$

**KKTæ¡ä»¶**ï¼š

1. æ¢¯åº¦æ¡ä»¶ï¼š$\nabla f(x^*) + \sum_{i=1}^{m} \lambda_i^* \nabla g_i(x^*) = 0$
2. å¯è¡Œæ€§ï¼š$g_i(x^*) \leq 0, \quad i = 1, ..., m$
3. äº’è¡¥æ¾å¼›ï¼š$\lambda_i^* g_i(x^*) = 0, \quad i = 1, ..., m$
4. éè´Ÿæ€§ï¼š$\lambda_i^* \geq 0, \quad i = 1, ..., m$

**å¯¹å¶ç†è®ºæ•°å­¦åŸç†**ï¼š

**å¯¹å¶å‡½æ•°**ï¼š
$$g(\lambda) = \inf_x \mathcal{L}(x, \lambda)$$

**å¯¹å¶é—®é¢˜**ï¼š
$$\max_{\lambda \geq 0} g(\lambda)$$

**å¼ºå¯¹å¶æ€§**ï¼š
å¦‚æœåŸé—®é¢˜æ˜¯å‡¸çš„ä¸”æ»¡è¶³Slateræ¡ä»¶ï¼Œåˆ™ï¼š
$$f(x^*) = g(\lambda^*)$$

**å†…ç‚¹æ³•æ•°å­¦åˆ†æ**ï¼š

**éšœç¢å‡½æ•°**ï¼š
$$\phi(x) = f(x) - \mu \sum_{i=1}^{m} \log(-g_i(x))$$

**ä¸­å¿ƒè·¯å¾„**ï¼š
$$x(\mu) = \arg\min_x \phi(x)$$

**æƒ©ç½šå‡½æ•°æ³•æ•°å­¦ç†è®º**ï¼š

**äºŒæ¬¡æƒ©ç½šå‡½æ•°**ï¼š
$$P(x, \rho) = f(x) + \frac{\rho}{2} \sum_{i=1}^{m} [\max(0, g_i(x))]^2$$

**æ›´æ–°è§„åˆ™**ï¼š
$$x_{k+1} = \arg\min_x P(x, \rho_k)$$
$$\rho_{k+1} = \beta \rho_k$$

## ğŸ”¬ å‰æ²¿å‘å±•

### 1. æ·±åº¦å­¦ä¹ å‰æ²¿

#### 1.1 æ³¨æ„åŠ›æœºåˆ¶å‘å±•

**Transformeræ¶æ„**ï¼š

- è‡ªæ³¨æ„åŠ›æœºåˆ¶
- å¤šå¤´æ³¨æ„åŠ›
- ä½ç½®ç¼–ç 
- æ®‹å·®è¿æ¥

**BERTå’ŒGPTç³»åˆ—**ï¼š

- é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
- æ©ç è¯­è¨€å»ºæ¨¡
- è‡ªå›å½’ç”Ÿæˆ
- å¤§è§„æ¨¡é¢„è®­ç»ƒ

#### 1.2 å›¾ç¥ç»ç½‘ç»œ

**å›¾å·ç§¯ç½‘ç»œ(GCN)**ï¼š
$$H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)})$$

**å›¾æ³¨æ„åŠ›ç½‘ç»œ(GAT)**ï¼š
$$\alpha_{ij} = \frac{\exp(\text{LeakyReLU}(a^T[Wh_i \| Wh_j]))}{\sum_{k \in \mathcal{N}_i} \exp(\text{LeakyReLU}(a^T[Wh_i \| Wh_k]))}$$

#### 1.3 å…ƒå­¦ä¹ 

**MAMLç®—æ³•**ï¼š
$$\theta' = \theta - \alpha \nabla_\theta \mathcal{L}_\tau(\theta)$$

**Reptileç®—æ³•**ï¼š
$$\phi = \phi + \epsilon \frac{1}{n} \sum_{i=1}^{n} (\tilde{\phi}_i - \phi)$$

### 2. å¼ºåŒ–å­¦ä¹ å‰æ²¿

#### 2.1 æ·±åº¦å¼ºåŒ–å­¦ä¹ 

**DQNç®—æ³•**ï¼š

- ç»éªŒå›æ”¾
- ç›®æ ‡ç½‘ç»œ
- æ·±åº¦Qç½‘ç»œ

**DDPGç®—æ³•**ï¼š

- ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦
- Actor-Criticæ¶æ„
- è¿ç»­åŠ¨ä½œç©ºé—´

#### 2.2 å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ 

**MADDPGç®—æ³•**ï¼š

- é›†ä¸­å¼è®­ç»ƒ
- åˆ†æ•£å¼æ‰§è¡Œ
- å¤šæ™ºèƒ½ä½“åä½œ

### 3. ä¼˜åŒ–ç®—æ³•å‰æ²¿

#### 3.1 è‡ªé€‚åº”ä¼˜åŒ–

**AdaBeliefç®—æ³•**ï¼š

- è‡ªé€‚åº”å­¦ä¹ ç‡
- ä¿¡å¿µçŠ¶æ€ä¼°è®¡
- å¿«é€Ÿæ”¶æ•›

#### 3.2 äºŒé˜¶æ–¹æ³•

**K-FACç®—æ³•**ï¼š

- Kroneckeråˆ†è§£
- è‡ªç„¶æ¢¯åº¦
- å¤§è§„æ¨¡è®­ç»ƒ

## ğŸ“Š åº”ç”¨æ¡ˆä¾‹

### 1. è®¡ç®—æœºè§†è§‰

#### 1.1 å›¾åƒåˆ†ç±»

**ResNetæ¶æ„**ï¼š

- æ®‹å·®è¿æ¥
- æ‰¹é‡å½’ä¸€åŒ–
- æ·±åº¦ç½‘ç»œè®­ç»ƒ

**EfficientNet**ï¼š

- å¤åˆç¼©æ”¾
- ç½‘ç»œæ¶æ„æœç´¢
- é«˜æ•ˆè®­ç»ƒ

#### 1.2 ç›®æ ‡æ£€æµ‹

**YOLOç®—æ³•**ï¼š

- å®æ—¶æ£€æµ‹
- å•é˜¶æ®µæ£€æµ‹
- ç«¯åˆ°ç«¯è®­ç»ƒ

**Faster R-CNN**ï¼š

- ä¸¤é˜¶æ®µæ£€æµ‹
- åŒºåŸŸæè®®ç½‘ç»œ
- ç²¾ç¡®æ£€æµ‹

### 2. è‡ªç„¶è¯­è¨€å¤„ç†

#### 2.1 è¯­è¨€æ¨¡å‹

**GPTç³»åˆ—**ï¼š

- è‡ªå›å½’ç”Ÿæˆ
- å¤§è§„æ¨¡é¢„è®­ç»ƒ
- å°‘æ ·æœ¬å­¦ä¹ 

**BERTæ¨¡å‹**ï¼š

- åŒå‘ç¼–ç 
- æ©ç è¯­è¨€å»ºæ¨¡
- ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒ

#### 2.2 æœºå™¨ç¿»è¯‘

**Transformeræ¶æ„**ï¼š

- ç¼–ç å™¨-è§£ç å™¨
- è‡ªæ³¨æ„åŠ›æœºåˆ¶
- å¹¶è¡Œè®­ç»ƒ

### 3. æ¨èç³»ç»Ÿ

#### 3.1 ååŒè¿‡æ»¤

**çŸ©é˜µåˆ†è§£**ï¼š
$$R_{ij} = U_i^T V_j$$

**æ·±åº¦æ¨è**ï¼š

- ç¥ç»ç½‘ç»œ
- ç‰¹å¾å·¥ç¨‹
- ç«¯åˆ°ç«¯è®­ç»ƒ

#### 3.2 åºåˆ—æ¨è

**RNNæ¨è**ï¼š

- åºåˆ—å»ºæ¨¡
- æ—¶é—´ä¾èµ–
- åŠ¨æ€æ¨è

## ğŸ› ï¸ æŠ€æœ¯å®ç°

### 1. æ·±åº¦å­¦ä¹ æ¡†æ¶

#### 1.1 PyTorchå®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim

class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# è®­ç»ƒå¾ªç¯
model = NeuralNetwork(input_size, hidden_size, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(num_epochs):
    for batch_x, batch_y in dataloader:
        optimizer.zero_grad()
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
```

#### 1.2 TensorFlowå®ç°

```python
import tensorflow as tf

class NeuralNetwork(tf.keras.Model):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNetwork, self).__init__()
        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu')
        self.fc2 = tf.keras.layers.Dense(output_size)
    
    def call(self, x):
        x = self.fc1(x)
        x = self.fc2(x)
        return x

# è®­ç»ƒ
model = NeuralNetwork(input_size, hidden_size, output_size)
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        predictions = model(x)
        loss = loss_fn(y, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss
```

### 2. å¼ºåŒ–å­¦ä¹ å®ç°

#### 2.1 Q-Learningå®ç°

```python
import numpy as np

class QLearning:
    def __init__(self, state_size, action_size, learning_rate=0.1, discount_factor=0.95, epsilon=0.1):
        self.Q = np.zeros((state_size, action_size))
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon
    
    def choose_action(self, state):
        if np.random.random() < self.epsilon:
            return np.random.randint(self.Q.shape[1])
        return np.argmax(self.Q[state])
    
    def learn(self, state, action, reward, next_state):
        old_value = self.Q[state, action]
        next_max = np.max(self.Q[next_state])
        new_value = (1 - self.lr) * old_value + self.lr * (reward + self.gamma * next_max)
        self.Q[state, action] = new_value
```

#### 2.2 DQNå®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import deque
import random

class DQN(nn.Module):
    def __init__(self, input_size, output_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, output_size)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.model = DQN(state_size, action_size)
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
    
    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))
    
    def act(self, state):
        if np.random.random() <= self.epsilon:
            return random.randrange(self.action_size)
        act_values = self.model.predict(state)
        return np.argmax(act_values[0])
    
    def replay(self, batch_size):
        minibatch = random.sample(self.memory, batch_size)
        for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done:
                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])
            target_f = self.model.predict(state)
            target_f[0][action] = target
            self.model.fit(state, target_f, epochs=1, verbose=0)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
```

### 3. ä¼˜åŒ–ç®—æ³•å®ç°

#### 3.1 Adamä¼˜åŒ–å™¨å®ç°

```python
import numpy as np

class AdamOptimizer:
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.lr = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = None
        self.v = None
        self.t = 0
    
    def update(self, params, grads):
        if self.m is None:
            self.m = np.zeros_like(params)
            self.v = np.zeros_like(params)
        
        self.t += 1
        
        # æ›´æ–°åç½®ä¿®æ­£çš„ä¸€é˜¶çŸ©ä¼°è®¡
        self.m = self.beta1 * self.m + (1 - self.beta1) * grads
        m_hat = self.m / (1 - self.beta1 ** self.t)
        
        # æ›´æ–°åç½®ä¿®æ­£çš„äºŒé˜¶çŸ©ä¼°è®¡
        self.v = self.beta2 * self.v + (1 - self.beta2) * (grads ** 2)
        v_hat = self.v / (1 - self.beta2 ** self.t)
        
        # æ›´æ–°å‚æ•°
        params -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)
        
        return params
```

## ğŸ“ˆ æ€§èƒ½åˆ†æ

### 1. ç®—æ³•å¤æ‚åº¦åˆ†æ

#### 1.1 æ—¶é—´å¤æ‚åº¦

**å‰å‘ä¼ æ’­**ï¼š

- å…¨è¿æ¥å±‚ï¼š$O(n_{in} \times n_{out})$
- å·ç§¯å±‚ï¼š$O(k^2 \times c_{in} \times c_{out} \times h \times w)$
- æ³¨æ„åŠ›æœºåˆ¶ï¼š$O(n^2 \times d)$

**åå‘ä¼ æ’­**ï¼š

- æ¢¯åº¦è®¡ç®—ï¼š$O(n_{params})$
- å‚æ•°æ›´æ–°ï¼š$O(n_{params})$

#### 1.2 ç©ºé—´å¤æ‚åº¦

**æ¨¡å‹å‚æ•°**ï¼š

- å…¨è¿æ¥å±‚ï¼š$O(n_{in} \times n_{out})$
- å·ç§¯å±‚ï¼š$O(k^2 \times c_{in} \times c_{out})$
- æ³¨æ„åŠ›æƒé‡ï¼š$O(n^2)$

**æ¿€æ´»å€¼**ï¼š

- å‰å‘ä¼ æ’­ï¼š$O(n_{layers} \times n_{neurons})$
- æ¢¯åº¦å­˜å‚¨ï¼š$O(n_{params})$

### 2. æ”¶æ•›æ€§åˆ†æ

#### 2.1 ç†è®ºæ”¶æ•›æ€§

**æ¢¯åº¦ä¸‹é™æ”¶æ•›**ï¼š

- å‡¸å‡½æ•°ï¼šçº¿æ€§æ”¶æ•›
- å¼ºå‡¸å‡½æ•°ï¼šæŒ‡æ•°æ”¶æ•›
- éå‡¸å‡½æ•°ï¼šå±€éƒ¨æ”¶æ•›

**éšæœºæ¢¯åº¦ä¸‹é™**ï¼š

- æœŸæœ›æ”¶æ•›ï¼š$O(1/\sqrt{T})$
- é«˜æ¦‚ç‡æ”¶æ•›ï¼š$O(\log T / T)$

#### 2.2 å®é™…æ”¶æ•›æ€§

**æ·±åº¦å­¦ä¹ æ”¶æ•›**ï¼š

- éå‡¸ä¼˜åŒ–
- å±€éƒ¨æœ€ä¼˜
- éç‚¹é—®é¢˜
- æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

### 3. æ³›åŒ–èƒ½åŠ›åˆ†æ

#### 3.1 ç†è®ºæ³›åŒ–ç•Œ

**VCç»´ç†è®º**ï¼š
$$\mathbb{E}[L(h)] \leq \hat{L}(h) + O\left(\sqrt{\frac{d \log n}{n}}\right)$$

**Rademacherå¤æ‚åº¦**ï¼š
$$\mathbb{E}[L(h)] \leq \hat{L}(h) + 2\mathcal{R}_n(\mathcal{H}) + 3\sqrt{\frac{\log(2/\delta)}{2n}}$$

#### 3.2 æ­£åˆ™åŒ–æŠ€æœ¯

**L1æ­£åˆ™åŒ–**ï¼š
$$\mathcal{L}_{reg} = \mathcal{L} + \lambda \sum_{i} |w_i|$$

**L2æ­£åˆ™åŒ–**ï¼š
$$\mathcal{L}_{reg} = \mathcal{L} + \lambda \sum_{i} w_i^2$$

**Dropout**ï¼š

- è®­ç»ƒæ—¶éšæœºå¤±æ´»
- æµ‹è¯•æ—¶ç¼©æ”¾æƒé‡
- é˜²æ­¢è¿‡æ‹Ÿåˆ

## ğŸ¯ æ€»ç»“

äººå·¥æ™ºèƒ½æ•°å­¦æ˜¯äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ç†è®ºåŸºç¡€ï¼Œæ¶µç›–äº†æ·±åº¦å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ã€ä¼˜åŒ–ç®—æ³•ç­‰æ ¸å¿ƒé¢†åŸŸã€‚é€šè¿‡æ·±å…¥ç†è§£è¿™äº›æ•°å­¦åŸç†ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š

1. **æ›´å¥½åœ°è®¾è®¡ç®—æ³•**ï¼šç†è§£æ•°å­¦åŸç†æœ‰åŠ©äºè®¾è®¡æ›´æœ‰æ•ˆçš„ç®—æ³•
2. **ä¼˜åŒ–æ¨¡å‹æ€§èƒ½**ï¼šæŒæ¡ä¼˜åŒ–ç†è®ºå¯ä»¥æå‡æ¨¡å‹è®­ç»ƒæ•ˆæœ
3. **è§£å†³å®é™…é—®é¢˜**ï¼šæ•°å­¦ç†è®ºä¸ºå®é™…åº”ç”¨æä¾›æŒ‡å¯¼
4. **æ¨åŠ¨æŠ€æœ¯å‘å±•**ï¼šæ•°å­¦åˆ›æ–°æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•

äººå·¥æ™ºèƒ½æ•°å­¦å°†ç»§ç»­åœ¨ä»¥ä¸‹æ–¹å‘å‘å±•ï¼š

1. **ç†è®ºæ·±åŒ–**ï¼šè¿›ä¸€æ­¥æ·±åŒ–ç°æœ‰ç†è®ºçš„æ•°å­¦åŸºç¡€
2. **ç®—æ³•åˆ›æ–°**ï¼šåŸºäºæ•°å­¦ç†è®ºå¼€å‘æ–°çš„ç®—æ³•
3. **åº”ç”¨æ‰©å±•**ï¼šå°†æ•°å­¦ç†è®ºåº”ç”¨åˆ°æ›´å¤šé¢†åŸŸ
4. **è·¨å­¦ç§‘èåˆ**ï¼šä¸å…¶ä»–å­¦ç§‘è¿›è¡Œæ·±åº¦èåˆ

---

**æ–‡æ¡£çŠ¶æ€**: äººå·¥æ™ºèƒ½æ•°å­¦æ·±åŒ–ç‰ˆå®Œæˆ  
**æœ€åæ›´æ–°**: 2025å¹´8æœˆ2æ—¥  
**ä¸‹ä¸€æ­¥**: ç»§ç»­æ·±åŒ–ç”Ÿç‰©æ•°å­¦ã€ç½‘ç»œç§‘å­¦æ•°å­¦ã€ä¿¡æ¯è®ºæ•°å­¦ç­‰é¢†åŸŸ
