# 2. ‰ºòÂåñÁêÜËÆ∫ / Optimization Theory

**‰∏ªÈ¢òÁºñÂè∑**: B.08.02
**ÂàõÂª∫Êó•Êúü**: 2025Âπ¥11Êúà21Êó•
**ÊúÄÂêéÊõ¥Êñ∞**: 2025Âπ¥11Êúà21Êó•

---

## ÁõÆÂΩï

- [2. ‰ºòÂåñÁêÜËÆ∫ / Optimization Theory](#2-‰ºòÂåñÁêÜËÆ∫--optimization-theory)
  - [ÁõÆÂΩï](#ÁõÆÂΩï)
  - [2.1 Ê¶ÇËø∞ / Overview](#21-Ê¶ÇËø∞--overview)
  - [üó∫Ô∏è ‰ºòÂåñÁêÜËÆ∫Ê†∏ÂøÉÊ¶ÇÂøµÊÄùÁª¥ÂØºÂõæ](#Ô∏è-‰ºòÂåñÁêÜËÆ∫Ê†∏ÂøÉÊ¶ÇÂøµÊÄùÁª¥ÂØºÂõæ)
  - [üìä ‰ºòÂåñÁêÜËÆ∫Ê†∏ÂøÉÊ¶ÇÂøµÂ§öÁª¥Áü•ËØÜÁü©Èòµ](#-‰ºòÂåñÁêÜËÆ∫Ê†∏ÂøÉÊ¶ÇÂøµÂ§öÁª¥Áü•ËØÜÁü©Èòµ)
  - [2.2 Âü∫Êú¨Ê¶ÇÂøµ / Basic Concepts](#22-Âü∫Êú¨Ê¶ÇÂøµ--basic-concepts)
    - [2.2.1 ‰ºòÂåñÈóÆÈ¢ò / Optimization Problem](#221-‰ºòÂåñÈóÆÈ¢ò--optimization-problem)
    - [2.2.2 ÂèØË°åÂüü / Feasible Region](#222-ÂèØË°åÂüü--feasible-region)
    - [2.2.3 ÊúÄ‰ºòËß£ / Optimal Solution](#223-ÊúÄ‰ºòËß£--optimal-solution)
  - [2.3 Âá∏‰ºòÂåñ / Convex Optimization](#23-Âá∏‰ºòÂåñ--convex-optimization)
    - [2.3.1 Âá∏ÈõÜ‰∏éÂá∏ÂáΩÊï∞ / Convex Sets and Functions](#231-Âá∏ÈõÜ‰∏éÂá∏ÂáΩÊï∞--convex-sets-and-functions)
    - [2.3.2 Âá∏‰ºòÂåñÈóÆÈ¢ò / Convex Optimization Problem](#232-Âá∏‰ºòÂåñÈóÆÈ¢ò--convex-optimization-problem)
  - [2.4 ÊãâÊ†ºÊúóÊó•ÂØπÂÅ∂ / Lagrangian Duality](#24-ÊãâÊ†ºÊúóÊó•ÂØπÂÅ∂--lagrangian-duality)
    - [2.4.1 ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞ / Lagrangian Function](#241-ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞--lagrangian-function)
    - [2.4.2 ÂØπÂÅ∂ÂáΩÊï∞ / Dual Function](#242-ÂØπÂÅ∂ÂáΩÊï∞--dual-function)
    - [2.4.3 ÂØπÂÅ∂ÈóÆÈ¢ò / Dual Problem](#243-ÂØπÂÅ∂ÈóÆÈ¢ò--dual-problem)
  - [2.5 ÊúÄ‰ºòÊÄßÊù°‰ª∂ / Optimality Conditions](#25-ÊúÄ‰ºòÊÄßÊù°‰ª∂--optimality-conditions)
    - [2.5.1 ‰∏ÄÈò∂ÂøÖË¶ÅÊù°‰ª∂ / First-Order Necessary Conditions](#251-‰∏ÄÈò∂ÂøÖË¶ÅÊù°‰ª∂--first-order-necessary-conditions)
    - [2.5.2 ‰∫åÈò∂ÂÖÖÂàÜÊù°‰ª∂ / Second-Order Sufficient Conditions](#252-‰∫åÈò∂ÂÖÖÂàÜÊù°‰ª∂--second-order-sufficient-conditions)
  - [2.6 ÁÆóÊ≥ï / Algorithms](#26-ÁÆóÊ≥ï--algorithms)
    - [2.6.1 Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï / Gradient Descent](#261-Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï--gradient-descent)
    - [2.6.2 ÁâõÈ°øÊ≥ï / Newton's Method](#262-ÁâõÈ°øÊ≥ï--newtons-method)
    - [2.6.3 ÂÜÖÁÇπÊ≥ï / Interior Point Methods](#263-ÂÜÖÁÇπÊ≥ï--interior-point-methods)
  - [2.7 ÂΩ¢ÂºèÂåñÂÆûÁé∞ / Formal Implementation](#27-ÂΩ¢ÂºèÂåñÂÆûÁé∞--formal-implementation)
    - [2.7.1 Lean 4 ÂÆûÁé∞ / Lean 4 Implementation](#271-lean-4-ÂÆûÁé∞--lean-4-implementation)
    - [2.7.2 Haskell ÂÆûÁé∞ / Haskell Implementation](#272-haskell-ÂÆûÁé∞--haskell-implementation)
  - [2.8 Â∫îÁî®‰∏éËÆ°ÁÆó / Applications and Computations](#28-Â∫îÁî®‰∏éËÆ°ÁÆó--applications-and-computations)
    - [2.8.1 Á∫øÊÄßËßÑÂàí / Linear Programming](#281-Á∫øÊÄßËßÑÂàí--linear-programming)
    - [2.8.2 ‰∫åÊ¨°ËßÑÂàí / Quadratic Programming](#282-‰∫åÊ¨°ËßÑÂàí--quadratic-programming)
    - [2.8.3 Âá∏‰ºòÂåñ / Convex Optimization](#283-Âá∏‰ºòÂåñ--convex-optimization)
  - [2.9 È´òÁ∫ß‰∏ªÈ¢ò / Advanced Topics](#29-È´òÁ∫ß‰∏ªÈ¢ò--advanced-topics)
    - [2.9.1 ÈöèÊú∫‰ºòÂåñ / Stochastic Optimization](#291-ÈöèÊú∫‰ºòÂåñ--stochastic-optimization)
    - [2.9.2 Â§öÁõÆÊ†á‰ºòÂåñ / Multi-Objective Optimization](#292-Â§öÁõÆÊ†á‰ºòÂåñ--multi-objective-optimization)
    - [2.9.3 ÂÖ®Â±Ä‰ºòÂåñ / Global Optimization](#293-ÂÖ®Â±Ä‰ºòÂåñ--global-optimization)
  - [2.10 ÊÄªÁªì / Summary](#210-ÊÄªÁªì--summary)
    - [2.10.1 ‰∏ªË¶ÅÊàêÊûú / Main Results](#2101-‰∏ªË¶ÅÊàêÊûú--main-results)
    - [2.10.2 Â∫îÁî®È¢ÜÂüü / Applications](#2102-Â∫îÁî®È¢ÜÂüü--applications)
  - [‰∫§‰∫í‰∏éË°•ÂÖÖËµÑÊ∫ê / Interactive \& Supplementary Resources](#‰∫§‰∫í‰∏éË°•ÂÖÖËµÑÊ∫ê--interactive--supplementary-resources)
    - [‰∫§‰∫íÂºèÂõæË°®Â¢ûÂº∫](#‰∫§‰∫íÂºèÂõæË°®Â¢ûÂº∫)
    - [ÂÆöÁêÜËØÅÊòéË°•ÂÖÖ](#ÂÆöÁêÜËØÅÊòéË°•ÂÖÖ)
    - [Âèç‰æã‰∏éÁâπÊÆäÊÉÖÂÜµË°•ÂÖÖ](#Âèç‰æã‰∏éÁâπÊÆäÊÉÖÂÜµË°•ÂÖÖ)
    - [ÂéÜÂè≤ËÉåÊôØË°•ÂÖÖ](#ÂéÜÂè≤ËÉåÊôØË°•ÂÖÖ)

## 2.1 Ê¶ÇËø∞ / Overview

‰ºòÂåñÁêÜËÆ∫ÊòØÁ†îÁ©∂Âú®ÁªôÂÆöÁ∫¶ÊùüÊù°‰ª∂‰∏ãÂØªÊâæÊúÄ‰ºòËß£ÁöÑÊñπÊ≥ïËÆ∫„ÄÇ
ÂÆÉÂú®Êï∞Â≠¶„ÄÅÂ∑•Á®ã„ÄÅÁªèÊµéÂ≠¶ÂíåËÆ°ÁÆóÊú∫ÁßëÂ≠¶Á≠âÈ¢ÜÂüüÊúâÂπøÊ≥õÂ∫îÁî®ÔºåÊòØÁé∞‰ª£Â∫îÁî®Êï∞Â≠¶ÁöÑÊ†∏ÂøÉÂàÜÊîØ‰πã‰∏Ä„ÄÇ

## üó∫Ô∏è ‰ºòÂåñÁêÜËÆ∫Ê†∏ÂøÉÊ¶ÇÂøµÊÄùÁª¥ÂØºÂõæ

```mermaid
mindmap
  root((‰ºòÂåñÁêÜËÆ∫))
    Âü∫Êú¨Ê¶ÇÂøµ
      ‰ºòÂåñÈóÆÈ¢ò
        ÁõÆÊ†áÂáΩÊï∞
        Á∫¶ÊùüÊù°‰ª∂
        ÂèØË°åÂüü
      ÊúÄ‰ºòËß£
        ÂÖ®Â±ÄÊúÄ‰ºò
        Â±ÄÈÉ®ÊúÄ‰ºò
        ÊúÄ‰ºòÊÄßÊù°‰ª∂
    Âá∏‰ºòÂåñ
      Âá∏ÈõÜ
        Âá∏ÈõÜÂÆö‰πâ
        Âá∏ÈõÜÊÄßË¥®
      Âá∏ÂáΩÊï∞
        Âá∏ÂáΩÊï∞ÂÆö‰πâ
        Âá∏ÂáΩÊï∞ÊÄßË¥®
      Âá∏‰ºòÂåñÈóÆÈ¢ò
        Âá∏‰ºòÂåñ
        ÂØπÂÅ∂ÊÄß
    ÊãâÊ†ºÊúóÊó•ÂØπÂÅ∂
      ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞
        ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞
        ÂØπÂÅ∂ÂáΩÊï∞
      ÂØπÂÅ∂ÈóÆÈ¢ò
        ÂØπÂÅ∂ÈóÆÈ¢ò
        ÂØπÂÅ∂Èó¥Èöô
    ÊúÄ‰ºòÊÄßÊù°‰ª∂
      ‰∏ÄÈò∂Êù°‰ª∂
        KKTÊù°‰ª∂
        Ê¢ØÂ∫¶Êù°‰ª∂
      ‰∫åÈò∂Êù°‰ª∂
        ‰∫åÈò∂Êù°‰ª∂
        ÂÖÖÂàÜÊù°‰ª∂
    ÁÆóÊ≥ï
      Ê¢ØÂ∫¶‰∏ãÈôç
        Ê¢ØÂ∫¶‰∏ãÈôç
        Êî∂ÊïõÊÄß
      ÁâõÈ°øÊ≥ï
        ÁâõÈ°øÊ≥ï
        ‰∫åÊ¨°Êî∂Êïõ
      ÂÜÖÁÇπÊ≥ï
        ÂÜÖÁÇπÊ≥ï
        Â§öÈ°πÂºèÊó∂Èó¥
    Â∫îÁî®
      Á∫øÊÄßËßÑÂàí
        Á∫øÊÄßËßÑÂàí
        ÂçïÁ∫ØÂΩ¢Ê≥ï
      ‰∫åÊ¨°ËßÑÂàí
        ‰∫åÊ¨°ËßÑÂàí
        Ê±ÇËß£ÊñπÊ≥ï
      Âá∏‰ºòÂåñ
        Âá∏‰ºòÂåñ
        Â∫îÁî®
```

## üìä ‰ºòÂåñÁêÜËÆ∫Ê†∏ÂøÉÊ¶ÇÂøµÂ§öÁª¥Áü•ËØÜÁü©Èòµ

| Ê¶ÇÂøµÁ±ªÂà´ | Ê†∏ÂøÉÊ¶ÇÂøµ | ÂÆö‰πâË¶ÅÁÇπ | ÂÖ≥ÈîÆÊÄßË¥® | ÂÖ∏Âûã‰æãÂ≠ê | Â∫îÁî®Âú∫ÊôØ |
|---------|---------|---------|---------|---------|---------|
| Âü∫Êú¨Ê¶ÇÂøµ | ‰ºòÂåñÈóÆÈ¢ò | ÁõÆÊ†áÂáΩÊï∞+Á∫¶Êùü | ÊúÄ‰ºòÂåñ | min f(x) | ‰ºòÂåñÂü∫Á°Ä |
| Âü∫Êú¨Ê¶ÇÂøµ | ÂèØË°åÂüü | Á∫¶ÊùüÈõÜÂêà | ÂèØË°åËß£ | F | ‰ºòÂåñÂü∫Á°Ä |
| Âü∫Êú¨Ê¶ÇÂøµ | ÂÖ®Â±ÄÊúÄ‰ºò | ÂÖ®Â±ÄÊúÄ‰ºòËß£ | ÊúÄ‰ºòÊÄß | x* | ‰ºòÂåñÂü∫Á°Ä |
| Âü∫Êú¨Ê¶ÇÂøµ | Â±ÄÈÉ®ÊúÄ‰ºò | Â±ÄÈÉ®ÊúÄ‰ºòËß£ | Â±ÄÈÉ®ÊÄß | x* | ‰ºòÂåñÂü∫Á°Ä |
| Âá∏‰ºòÂåñ | Âá∏ÈõÜ | Âá∏ÈõÜÂÆö‰πâ | Âá∏ÊÄß | C | Âá∏‰ºòÂåñ |
| Âá∏‰ºòÂåñ | Âá∏ÂáΩÊï∞ | Âá∏ÂáΩÊï∞ÂÆö‰πâ | Âá∏ÊÄß | f(x) | Âá∏‰ºòÂåñ |
| Âá∏‰ºòÂåñ | Âá∏‰ºòÂåñÈóÆÈ¢ò | Âá∏‰ºòÂåñ | ÂÖ®Â±ÄÊúÄ‰ºò | Âá∏‰ºòÂåñ | Âá∏‰ºòÂåñ |
| ÊãâÊ†ºÊúóÊó•ÂØπÂÅ∂ | ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞ | ÊãâÊ†ºÊúóÊó• | ÂØπÂÅ∂ÊÄß | L(x,Œª) | ÂØπÂÅ∂ÁêÜËÆ∫ |
| ÊãâÊ†ºÊúóÊó•ÂØπÂÅ∂ | ÂØπÂÅ∂ÂáΩÊï∞ | ÂØπÂÅ∂ÂáΩÊï∞ | ÂØπÂÅ∂ÊÄß | g(Œª) | ÂØπÂÅ∂ÁêÜËÆ∫ |
| ÊãâÊ†ºÊúóÊó•ÂØπÂÅ∂ | ÂØπÂÅ∂ÈóÆÈ¢ò | ÂØπÂÅ∂ÈóÆÈ¢ò | ÂØπÂÅ∂ÊÄß | ÂØπÂÅ∂ÈóÆÈ¢ò | ÂØπÂÅ∂ÁêÜËÆ∫ |
| ÊúÄ‰ºòÊÄßÊù°‰ª∂ | KKTÊù°‰ª∂ | ÊúÄ‰ºòÊÄßÊù°‰ª∂ | ÂøÖË¶ÅÊÄß | KKT | ÊúÄ‰ºòÊÄß |
| ÊúÄ‰ºòÊÄßÊù°‰ª∂ | ‰∏ÄÈò∂Êù°‰ª∂ | Ê¢ØÂ∫¶Êù°‰ª∂ | ÂøÖË¶ÅÊÄß | ‚àáf=0 | ÊúÄ‰ºòÊÄß |
| ÊúÄ‰ºòÊÄßÊù°‰ª∂ | ‰∫åÈò∂Êù°‰ª∂ | Êµ∑Â°ûÁü©Èòµ | ÂÖÖÂàÜÊÄß | ‚àá¬≤f | ÊúÄ‰ºòÊÄß |
| ÁÆóÊ≥ï | Ê¢ØÂ∫¶‰∏ãÈôç | Ê¢ØÂ∫¶Ê≥ï | Êî∂ÊïõÊÄß | Ê¢ØÂ∫¶‰∏ãÈôç | ‰ºòÂåñÁÆóÊ≥ï |
| ÁÆóÊ≥ï | ÁâõÈ°øÊ≥ï | ÁâõÈ°øÊ≥ï | ‰∫åÊ¨°Êî∂Êïõ | ÁâõÈ°øÊ≥ï | ‰ºòÂåñÁÆóÊ≥ï |
| ÁÆóÊ≥ï | ÂÜÖÁÇπÊ≥ï | ÂÜÖÁÇπÊ≥ï | Â§öÈ°πÂºèÊó∂Èó¥ | ÂÜÖÁÇπÊ≥ï | ‰ºòÂåñÁÆóÊ≥ï |
| Â∫îÁî® | Á∫øÊÄßËßÑÂàí | LPÈóÆÈ¢ò | ÂçïÁ∫ØÂΩ¢Ê≥ï | LP | Â∫îÁî®‰ºòÂåñ |
| Â∫îÁî® | ‰∫åÊ¨°ËßÑÂàí | QPÈóÆÈ¢ò | Ê±ÇËß£ÊñπÊ≥ï | QP | Â∫îÁî®‰ºòÂåñ |
| Â∫îÁî® | ÈöèÊú∫‰ºòÂåñ | ÈöèÊú∫‰ºòÂåñ | ÈöèÊú∫ÊÄß | ÈöèÊú∫‰ºòÂåñ | Â∫îÁî®‰ºòÂåñ |
| Â∫îÁî® | Â§öÁõÆÊ†á‰ºòÂåñ | Â§öÁõÆÊ†á | ParetoÊúÄ‰ºò | Â§öÁõÆÊ†á | Â∫îÁî®‰ºòÂåñ |
| Â∫îÁî® | ÂÖ®Â±Ä‰ºòÂåñ | ÂÖ®Â±Ä‰ºòÂåñ | ÂÖ®Â±ÄÊÄß | ÂÖ®Â±Ä‰ºòÂåñ | Â∫îÁî®‰ºòÂåñ |

## 2.2 Âü∫Êú¨Ê¶ÇÂøµ / Basic Concepts

### 2.2.1 ‰ºòÂåñÈóÆÈ¢ò / Optimization Problem

**ÂÆö‰πâ 2.1** (‰ºòÂåñÈóÆÈ¢ò / Optimization Problem)
Ê†áÂáÜ‰ºòÂåñÈóÆÈ¢òÂΩ¢Âºè‰∏∫Ôºö
$$\min_{x \in \mathbb{R}^n} f(x)$$
$$\text{subject to } g_i(x) \leq 0, \quad i = 1, \ldots, m$$
$$\quad \quad \quad \quad h_j(x) = 0, \quad j = 1, \ldots, p$$

ÂÖ∂‰∏≠Ôºö

- $f: \mathbb{R}^n \to \mathbb{R}$ ÊòØÁõÆÊ†áÂáΩÊï∞
- $g_i: \mathbb{R}^n \to \mathbb{R}$ ÊòØ‰∏çÁ≠âÂºèÁ∫¶Êùü
- $h_j: \mathbb{R}^n \to \mathbb{R}$ ÊòØÁ≠âÂºèÁ∫¶Êùü

### 2.2.2 ÂèØË°åÂüü / Feasible Region

**ÂÆö‰πâ 2.2** (ÂèØË°åÂüü / Feasible Region)
ÂèØË°åÂüüÊòØÊª°Ë∂≥ÊâÄÊúâÁ∫¶ÊùüÁöÑÁÇπÁöÑÈõÜÂêàÔºö
$$\mathcal{F} = \{x \in \mathbb{R}^n : g_i(x) \leq 0, h_j(x) = 0, \forall i, j\}$$

### 2.2.3 ÊúÄ‰ºòËß£ / Optimal Solution

**ÂÆö‰πâ 2.3** (ÂÖ®Â±ÄÊúÄ‰ºòËß£ / Global Optimal Solution)
$x^*$ ÊòØÂÖ®Â±ÄÊúÄ‰ºòËß£ÔºåÂ¶ÇÊûúÔºö
$$f(x^*) \leq f(x), \quad \forall x \in \mathcal{F}$$

**ÂÆö‰πâ 2.4** (Â±ÄÈÉ®ÊúÄ‰ºòËß£ / Local Optimal Solution)
$x^*$ ÊòØÂ±ÄÈÉ®ÊúÄ‰ºòËß£ÔºåÂ¶ÇÊûúÂ≠òÂú® $\epsilon > 0$Ôºå‰ΩøÂæóÔºö
$$f(x^*) \leq f(x), \quad \forall x \in \mathcal{F} \cap B_\epsilon(x^*)$$

## 2.3 Âá∏‰ºòÂåñ / Convex Optimization

### 2.3.1 Âá∏ÈõÜ‰∏éÂá∏ÂáΩÊï∞ / Convex Sets and Functions

**ÂÆö‰πâ 2.5** (Âá∏ÈõÜ / Convex Set)
ÈõÜÂêà $C \subseteq \mathbb{R}^n$ ÊòØÂá∏ÈõÜÔºåÂ¶ÇÊûúÔºö
$$\lambda x + (1-\lambda)y \in C, \quad \forall x, y \in C, \lambda \in [0, 1]$$

**ÂÆö‰πâ 2.6** (Âá∏ÂáΩÊï∞ / Convex Function)
ÂáΩÊï∞ $f: \mathbb{R}^n \to \mathbb{R}$ ÊòØÂá∏ÂáΩÊï∞ÔºåÂ¶ÇÊûúÔºö
$$f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y), \quad \forall x, y \in \mathbb{R}^n, \lambda \in [0, 1]$$

### 2.3.2 Âá∏‰ºòÂåñÈóÆÈ¢ò / Convex Optimization Problem

**ÂÆö‰πâ 2.7** (Âá∏‰ºòÂåñÈóÆÈ¢ò / Convex Optimization Problem)
Âá∏‰ºòÂåñÈóÆÈ¢òÊòØÂΩ¢Âºè‰∏∫Ôºö
$$\min_{x \in \mathbb{R}^n} f(x)$$
$$\text{subject to } g_i(x) \leq 0, \quad i = 1, \ldots, m$$
$$\quad \quad \quad \quad Ax = b$$

ÂÖ∂‰∏≠ $f$ Âíå $g_i$ ÈÉΩÊòØÂá∏ÂáΩÊï∞Ôºå$A \in \mathbb{R}^{p \times n}$„ÄÇ

**ÂÆöÁêÜ 2.1** (Âá∏‰ºòÂåñÁöÑÂ±ÄÈÉ®ÊúÄ‰ºòÊÄß / Local Optimality in Convex Optimization)
Âú®Âá∏‰ºòÂåñÈóÆÈ¢ò‰∏≠ÔºåÂ±ÄÈÉ®ÊúÄ‰ºòËß£‰πüÊòØÂÖ®Â±ÄÊúÄ‰ºòËß£„ÄÇ

## 2.4 ÊãâÊ†ºÊúóÊó•ÂØπÂÅ∂ / Lagrangian Duality

### 2.4.1 ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞ / Lagrangian Function

**ÂÆö‰πâ 2.8** (ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞ / Lagrangian Function)
ÂØπ‰∫é‰ºòÂåñÈóÆÈ¢òÔºåÊãâÊ†ºÊúóÊó•ÂáΩÊï∞ÂÆö‰πâ‰∏∫Ôºö
$$\mathcal{L}(x, \lambda, \mu) = f(x) + \sum_{i=1}^m \lambda_i g_i(x) + \sum_{j=1}^p \mu_j h_j(x)$$

ÂÖ∂‰∏≠ $\lambda_i \geq 0$ ÊòØÊãâÊ†ºÊúóÊó•‰πòÂ≠ê„ÄÇ

### 2.4.2 ÂØπÂÅ∂ÂáΩÊï∞ / Dual Function

**ÂÆö‰πâ 2.9** (ÂØπÂÅ∂ÂáΩÊï∞ / Dual Function)
ÂØπÂÅ∂ÂáΩÊï∞ÂÆö‰πâ‰∏∫Ôºö
$$g(\lambda, \mu) = \inf_{x \in \mathbb{R}^n} \mathcal{L}(x, \lambda, \mu)$$

### 2.4.3 ÂØπÂÅ∂ÈóÆÈ¢ò / Dual Problem

**ÂÆö‰πâ 2.10** (ÂØπÂÅ∂ÈóÆÈ¢ò / Dual Problem)
ÂØπÂÅ∂ÈóÆÈ¢òÊòØÔºö
$$\max_{\lambda \geq 0, \mu} g(\lambda, \mu)$$

**ÂÆöÁêÜ 2.2** (Âº±ÂØπÂÅ∂ÊÄß / Weak Duality)
ÂØπÂÅ∂ÈóÆÈ¢òÁöÑÁõÆÊ†áÂáΩÊï∞ÂÄº‰∏çË∂ÖËøáÂéüÈóÆÈ¢òÁöÑÊúÄ‰ºòÂÄºÔºö
$$g(\lambda, \mu) \leq f(x^*), \quad \forall \lambda \geq 0, \mu$$

## 2.5 ÊúÄ‰ºòÊÄßÊù°‰ª∂ / Optimality Conditions

### 2.5.1 ‰∏ÄÈò∂ÂøÖË¶ÅÊù°‰ª∂ / First-Order Necessary Conditions

**ÂÆöÁêÜ 2.3** (KKTÊù°‰ª∂ / KKT Conditions)
Â¶ÇÊûú $x^*$ ÊòØÊ≠£ÂàôÁÇπ‰∏îÊòØÂ±ÄÈÉ®ÊúÄ‰ºòËß£ÔºåÂàôÂ≠òÂú®ÊãâÊ†ºÊúóÊó•‰πòÂ≠ê $\lambda^* \geq 0$ Âíå $\mu^*$Ôºå‰ΩøÂæóÔºö

1. **Âπ≥Á®≥ÊÄß**Ôºö$\nabla f(x^*) + \sum_{i=1}^m \lambda_i^* \nabla g_i(x^*) + \sum_{j=1}^p \mu_j^* \nabla h_j(x^*) = 0$
2. **ÂéüÂßãÂèØË°åÊÄß**Ôºö$g_i(x^*) \leq 0, h_j(x^*) = 0$
3. **ÂØπÂÅ∂ÂèØË°åÊÄß**Ôºö$\lambda_i^* \geq 0$
4. **‰∫íË°•ÊùæÂºõÊÄß**Ôºö$\lambda_i^* g_i(x^*) = 0$

### 2.5.2 ‰∫åÈò∂ÂÖÖÂàÜÊù°‰ª∂ / Second-Order Sufficient Conditions

**ÂÆöÁêÜ 2.4** (‰∫åÈò∂ÂÖÖÂàÜÊù°‰ª∂ / Second-Order Sufficient Conditions)
Â¶ÇÊûú $x^*$ Êª°Ë∂≥KKTÊù°‰ª∂Ôºå‰∏îÂØπ‰ªªÊÑèÈùûÈõ∂ÂêëÈáè $d$ Êª°Ë∂≥Ôºö
$$d^T \nabla^2 f(x^*) d > 0$$

Âàô $x^*$ ÊòØ‰∏•Ê†ºÂ±ÄÈÉ®ÊúÄ‰ºòËß£„ÄÇ

## 2.6 ÁÆóÊ≥ï / Algorithms

### 2.6.1 Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï / Gradient Descent

**ÁÆóÊ≥ï 2.1** (Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï / Gradient Descent)

```python
def gradient_descent(f, grad_f, x0, alpha, max_iter):
    x = x0
    for k in range(max_iter):
        x = x - alpha * grad_f(x)
    return x
```

**Êî∂ÊïõÊÄß**Ôºö

- Â¶ÇÊûú $f$ ÊòØÂá∏ÂáΩÊï∞‰∏î $\nabla f$ ÊòØLipschitzËøûÁª≠ÁöÑÔºåÂàôÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÊî∂ÊïõÂà∞ÂÖ®Â±ÄÊúÄ‰ºòËß£
- Êî∂ÊïõÈÄüÂ∫¶‰∏∫ $O(1/k)$

### 2.6.2 ÁâõÈ°øÊ≥ï / Newton's Method

**ÁÆóÊ≥ï 2.2** (ÁâõÈ°øÊ≥ï / Newton's Method)

```python
def newton_method(f, grad_f, hess_f, x0, max_iter):
    x = x0
    for k in range(max_iter):
        H = hess_f(x)
        g = grad_f(x)
        d = -np.linalg.solve(H, g)
        x = x + d
    return x
```

**Êî∂ÊïõÊÄß**Ôºö

- Â¶ÇÊûú $f$ ÊòØÂº∫Âá∏ÂáΩÊï∞ÔºåÂàôÁâõÈ°øÊ≥ïÂÖ∑Êúâ‰∫åÊ¨°Êî∂ÊïõÊÄß
- Êî∂ÊïõÈÄüÂ∫¶‰∏∫ $O(\log \log(1/\epsilon))$

### 2.6.3 ÂÜÖÁÇπÊ≥ï / Interior Point Methods

**ÁÆóÊ≥ï 2.3** (ÂÜÖÁÇπÊ≥ï / Interior Point Method)
ÂÜÖÁÇπÊ≥ïÈÄöËøáÂºïÂÖ•ÈöúÁ¢çÂáΩÊï∞Â∞ÜÁ∫¶Êùü‰ºòÂåñÈóÆÈ¢òËΩ¨Âåñ‰∏∫Êó†Á∫¶ÊùüÈóÆÈ¢òÔºö
$$\min_{x} f(x) - \mu \sum_{i=1}^m \log(-g_i(x))$$

ÂÖ∂‰∏≠ $\mu > 0$ ÊòØÈöúÁ¢çÂèÇÊï∞„ÄÇ

## 2.7 ÂΩ¢ÂºèÂåñÂÆûÁé∞ / Formal Implementation

### 2.7.1 Lean 4 ÂÆûÁé∞ / Lean 4 Implementation

```lean
-- ‰ºòÂåñÈóÆÈ¢òÁöÑÂÆö‰πâ
structure OptimizationProblem (n : ‚Ñï) where
  objective : ‚Ñù^n ‚Üí ‚Ñù
  inequality_constraints : List (‚Ñù^n ‚Üí ‚Ñù)
  equality_constraints : List (‚Ñù^n ‚Üí ‚Ñù)

-- ÂèØË°åÂüü
def feasible_region {n : ‚Ñï} (P : OptimizationProblem n) : Set (‚Ñù^n) :=
  { x : ‚Ñù^n |
    (‚àÄ g ‚àà P.inequality_constraints, g x ‚â§ 0) ‚àß
    (‚àÄ h ‚àà P.equality_constraints, h x = 0) }

-- ÂÖ®Â±ÄÊúÄ‰ºòËß£
def global_optimal {n : ‚Ñï} (P : OptimizationProblem n) (x* : ‚Ñù^n) : Prop :=
  x* ‚àà feasible_region P ‚àß
  ‚àÄ x ‚àà feasible_region P, P.objective x* ‚â§ P.objective x

-- ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞
def lagrangian {n : ‚Ñï} (P : OptimizationProblem n)
  (x : ‚Ñù^n) (Œª : ‚Ñù^m) (Œº : ‚Ñù^p) : ‚Ñù :=
  P.objective x +
  ‚àë i, Œª[i] * P.inequality_constraints[i] x +
  ‚àë j, Œº[j] * P.equality_constraints[j] x

-- KKTÊù°‰ª∂
structure KKT_Conditions {n m p : ‚Ñï} (P : OptimizationProblem n) (x* : ‚Ñù^n) where
  stationarity : ‚àá(lagrangian P x* Œª* Œº*) = 0
  primal_feasibility : x* ‚àà feasible_region P
  dual_feasibility : Œª* ‚â• 0
  complementary_slackness : ‚àÄ i, Œª*[i] * P.inequality_constraints[i] x* = 0

-- Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï
def gradient_descent {n : ‚Ñï} (f : ‚Ñù^n ‚Üí ‚Ñù) (‚àáf : ‚Ñù^n ‚Üí ‚Ñù^n)
  (x0 : ‚Ñù^n) (Œ± : ‚Ñù) (max_iter : ‚Ñï) : ‚Ñù^n :=
  let rec iterate (x : ‚Ñù^n) (k : ‚Ñï) : ‚Ñù^n :=
    if k ‚â• max_iter then x
    else iterate (x - Œ± ‚Ä¢ ‚àáf x) (k + 1)
  iterate x0 0

-- ÁâõÈ°øÊ≥ï
def newton_method {n : ‚Ñï} (f : ‚Ñù^n ‚Üí ‚Ñù) (‚àáf : ‚Ñù^n ‚Üí ‚Ñù^n) (‚àá¬≤f : ‚Ñù^n ‚Üí Matrix ‚Ñù n n)
  (x0 : ‚Ñù^n) (max_iter : ‚Ñï) : ‚Ñù^n :=
  let rec iterate (x : ‚Ñù^n) (k : ‚Ñï) : ‚Ñù^n :=
    if k ‚â• max_iter then x
    else
      let d := -(‚àá¬≤f x)‚Åª¬π ‚Ä¢ ‚àáf x
      iterate (x + d) (k + 1)
  iterate x0 0
```

### 2.7.2 Haskell ÂÆûÁé∞ / Haskell Implementation

```haskell
-- ‰ºòÂåñÈóÆÈ¢òÁöÑÊï∞ÊçÆÁ±ªÂûã
data OptimizationProblem n = OptimizationProblem
  { objective :: Vector n ‚Ñù -> ‚Ñù
  , inequalityConstraints :: [Vector n ‚Ñù -> ‚Ñù]
  , equalityConstraints :: [Vector n ‚Ñù -> ‚Ñù]
  }

-- ÂèØË°åÂüü
feasibleRegion :: OptimizationProblem n -> Set (Vector n ‚Ñù)
feasibleRegion p = Set.fromList [x | x <- allVectors, isFeasible p x]
  where
    isFeasible p x = all (\g -> g x <= 0) (inequalityConstraints p) &&
                     all (\h -> h x == 0) (equalityConstraints p)

-- ÂÖ®Â±ÄÊúÄ‰ºòËß£
globalOptimal :: OptimizationProblem n -> Vector n ‚Ñù -> Bool
globalOptimal p x* = x* `Set.member` feasibleRegion p &&
                     all (\x -> objective p x* <= objective p x) (feasibleRegion p)

-- ÊãâÊ†ºÊúóÊó•ÂáΩÊï∞
lagrangian :: OptimizationProblem n -> Vector n ‚Ñù -> Vector m ‚Ñù -> Vector p ‚Ñù -> ‚Ñù
lagrangian p x Œª Œº = objective p x +
                     sum [Œª[i] * g x | (i, g) <- zip [0..] (inequalityConstraints p)] +
                     sum [Œº[j] * h x | (j, h) <- zip [0..] (equalityConstraints p)]

-- KKTÊù°‰ª∂
data KKTConditions n m p = KKTConditions
  { stationarity :: Vector (n + m + p) ‚Ñù
  , primalFeasibility :: Bool
  , dualFeasibility :: Bool
  , complementarySlackness :: Bool
  }

-- Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï
gradientDescent :: (Vector n ‚Ñù -> ‚Ñù) -> (Vector n ‚Ñù -> Vector n ‚Ñù) ->
                  Vector n ‚Ñù -> ‚Ñù -> Int -> Vector n ‚Ñù
gradientDescent f ‚àáf x0 Œ± maxIter = iterate x0 0
  where
    iterate x k
      | k >= maxIter = x
      | otherwise = iterate (x - Œ± *^ ‚àáf x) (k + 1)

-- ÁâõÈ°øÊ≥ï
newtonMethod :: (Vector n ‚Ñù -> ‚Ñù) -> (Vector n ‚Ñù -> Vector n ‚Ñù) ->
               (Vector n ‚Ñù -> Matrix n n ‚Ñù) -> Vector n ‚Ñù -> Int -> Vector n ‚Ñù
newtonMethod f ‚àáf ‚àá¬≤f x0 maxIter = iterate x0 0
  where
    iterate x k
      | k >= maxIter = x
      | otherwise = iterate (x + d) (k + 1)
      where
        d = negate (inv (‚àá¬≤f x) `multiply` ‚àáf x)

-- ÂÜÖÁÇπÊ≥ï
interiorPointMethod :: OptimizationProblem n -> Vector n ‚Ñù -> ‚Ñù -> Vector n ‚Ñù
interiorPointMethod p x0 Œº =
  let barrierFunction x = objective p x - Œº * sum [log (-g x) | g <- inequalityConstraints p]
  in gradientDescent barrierFunction (gradient barrierFunction) x0 0.01 1000
```

## 2.8 Â∫îÁî®‰∏éËÆ°ÁÆó / Applications and Computations

### 2.8.1 Á∫øÊÄßËßÑÂàí / Linear Programming

**ÂÆö‰πâ 2.11** (Á∫øÊÄßËßÑÂàí / Linear Programming)
Á∫øÊÄßËßÑÂàíÊòØÁõÆÊ†áÂáΩÊï∞ÂíåÁ∫¶ÊùüÈÉΩÊòØÁ∫øÊÄßÁöÑ‰ºòÂåñÈóÆÈ¢òÔºö
$$\min_{x} c^T x$$
$$\text{subject to } Ax \leq b, x \geq 0$$

**ÁÆóÊ≥ï**Ôºö

- ÂçïÁ∫ØÂΩ¢Ê≥ïÔºàSimplex MethodÔºâ
- ÂÜÖÁÇπÊ≥ïÔºàInterior Point MethodÔºâ

### 2.8.2 ‰∫åÊ¨°ËßÑÂàí / Quadratic Programming

**ÂÆö‰πâ 2.12** (‰∫åÊ¨°ËßÑÂàí / Quadratic Programming)
‰∫åÊ¨°ËßÑÂàíÊòØÁõÆÊ†áÂáΩÊï∞‰∏∫‰∫åÊ¨°ÂáΩÊï∞ÁöÑ‰ºòÂåñÈóÆÈ¢òÔºö
$$\min_{x} \frac{1}{2} x^T Q x + c^T x$$
$$\text{subject to } Ax \leq b$$

### 2.8.3 Âá∏‰ºòÂåñ / Convex Optimization

**ÂÆö‰πâ 2.13** (Âá∏‰ºòÂåñ / Convex Optimization)
Âá∏‰ºòÂåñÊòØÁõÆÊ†áÂáΩÊï∞ÂíåÁ∫¶ÊùüÈÉΩÊòØÂá∏ÂáΩÊï∞ÁöÑ‰ºòÂåñÈóÆÈ¢ò„ÄÇ

**ÁÆóÊ≥ï**Ôºö

- Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï
- ÁâõÈ°øÊ≥ï
- ÂÜÖÁÇπÊ≥ï

## 2.9 È´òÁ∫ß‰∏ªÈ¢ò / Advanced Topics

### 2.9.1 ÈöèÊú∫‰ºòÂåñ / Stochastic Optimization

**ÂÆö‰πâ 2.14** (ÈöèÊú∫‰ºòÂåñ / Stochastic Optimization)
ÈöèÊú∫‰ºòÂåñÂ§ÑÁêÜÁõÆÊ†áÂáΩÊï∞ÊàñÁ∫¶ÊùüÂåÖÂê´ÈöèÊú∫ÂèòÈáèÁöÑ‰ºòÂåñÈóÆÈ¢òÔºö
$$\min_{x} \mathbb{E}[f(x, \xi)]$$

### 2.9.2 Â§öÁõÆÊ†á‰ºòÂåñ / Multi-Objective Optimization

**ÂÆö‰πâ 2.15** (Â§öÁõÆÊ†á‰ºòÂåñ / Multi-Objective Optimization)
Â§öÁõÆÊ†á‰ºòÂåñÂêåÊó∂‰ºòÂåñÂ§ö‰∏™ÁõÆÊ†áÂáΩÊï∞Ôºö
$$\min_{x} (f_1(x), f_2(x), \ldots, f_k(x))$$

### 2.9.3 ÂÖ®Â±Ä‰ºòÂåñ / Global Optimization

**ÂÆö‰πâ 2.16** (ÂÖ®Â±Ä‰ºòÂåñ / Global Optimization)
ÂÖ®Â±Ä‰ºòÂåñÂØªÊâæÈùûÂá∏ÂáΩÊï∞ÁöÑÂÖ®Â±ÄÊúÄ‰ºòËß£„ÄÇ

**ÁÆóÊ≥ï**Ôºö

- ÈÅó‰º†ÁÆóÊ≥ï
- Ê®°ÊãüÈÄÄÁÅ´
- Á≤íÂ≠êÁæ§‰ºòÂåñ

## 2.10 ÊÄªÁªì / Summary

‰ºòÂåñÁêÜËÆ∫‰∏∫ÂêÑÁßçÂÆûÈôÖÈóÆÈ¢òÊèê‰æõ‰∫ÜÂº∫Â§ßÁöÑÊï∞Â≠¶Â∑•ÂÖ∑ÂíåÁÆóÊ≥ï„ÄÇ

### 2.10.1 ‰∏ªË¶ÅÊàêÊûú / Main Results

1. **ÊúÄ‰ºòÊÄßÊù°‰ª∂**ÔºöKKTÊù°‰ª∂‰∏∫Á∫¶Êùü‰ºòÂåñÊèê‰æõ‰∫ÜÂøÖË¶ÅÊù°‰ª∂
2. **ÂØπÂÅ∂ÁêÜËÆ∫**ÔºöÊãâÊ†ºÊúóÊó•ÂØπÂÅ∂‰∏∫‰ºòÂåñÈóÆÈ¢òÊèê‰æõ‰∫ÜÊñ∞ÁöÑËßÜËßí
3. **ÁÆóÊ≥ïËÆæËÆ°**ÔºöÊ¢ØÂ∫¶‰∏ãÈôç„ÄÅÁâõÈ°øÊ≥ï„ÄÅÂÜÖÁÇπÊ≥ïÁ≠âÈ´òÊïàÁÆóÊ≥ï
4. **Êî∂ÊïõÊÄßÂàÜÊûê**ÔºöÂêÑÁßçÁÆóÊ≥ïÁöÑÊî∂ÊïõÊÄßÁêÜËÆ∫ÂíåÂ§çÊùÇÂ∫¶ÂàÜÊûê

### 2.10.2 Â∫îÁî®È¢ÜÂüü / Applications

- **Êú∫Âô®Â≠¶‰π†**ÔºöÊîØÊåÅÂêëÈáèÊú∫„ÄÅÁ•ûÁªèÁΩëÁªúËÆ≠ÁªÉ
- **ÈáëËûçÂ∑•Á®ã**ÔºöÊäïËµÑÁªÑÂêà‰ºòÂåñ„ÄÅÈ£éÈô©ÁÆ°ÁêÜ
- **Â∑•Á®ãËÆæËÆ°**ÔºöÁªìÊûÑ‰ºòÂåñ„ÄÅÂèÇÊï∞‰º∞ËÆ°
- **ÁªèÊµéÂ≠¶**ÔºöÊïàÁî®ÊúÄÂ§ßÂåñ„ÄÅÊàêÊú¨ÊúÄÂ∞èÂåñ

---

**ÂèÇËÄÉÊñáÁåÆ / References**:

1. Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press.
2. Nocedal, J., & Wright, S. J. (2006). *Numerical Optimization*. Springer.
3. Bertsekas, D. P. (1999). *Nonlinear Programming*. Athena Scientific.
4. Rockafellar, R. T. (1970). *Convex Analysis*. Princeton University Press.

## ‰∫§‰∫í‰∏éË°•ÂÖÖËµÑÊ∫ê / Interactive & Supplementary Resources

### ‰∫§‰∫íÂºèÂõæË°®Â¢ûÂº∫

- [‰ºòÂåñÁÆóÊ≥ïÂèØËßÜÂåñ](../‰∫§‰∫íÂºèÂõæË°®Â¢ûÂº∫-2025Âπ¥1Êúà.md#‰ºòÂåñÁÆóÊ≥ïÂèØËßÜÂåñÂô®)
- [Êî∂ÊïõÊÄßÂàÜÊûêÂ∑•ÂÖ∑](../‰∫§‰∫íÂºèÂõæË°®Â¢ûÂº∫-2025Âπ¥1Êúà.md#Êî∂ÊïõÊÄßÂàÜÊûêÂô®)
- [ÁõÆÊ†áÂáΩÊï∞Êé¢Á¥¢Âô®](../‰∫§‰∫íÂºèÂõæË°®Â¢ûÂº∫-2025Âπ¥1Êúà.md#ÁõÆÊ†áÂáΩÊï∞Êé¢Á¥¢Âô®)

### ÂÆöÁêÜËØÅÊòéË°•ÂÖÖ

- [KKTÊù°‰ª∂ËØÅÊòé](../ÂÆöÁêÜËØÅÊòéË°•ÂÖÖ-2025Âπ¥1Êúà.md#KKTÊù°‰ª∂ËØÅÊòé)
- [ÂçïÁ∫ØÂΩ¢Ê≥ïÊî∂ÊïõÊÄß](../ÂÆöÁêÜËØÅÊòéË°•ÂÖÖ-2025Âπ¥1Êúà.md#ÂçïÁ∫ØÂΩ¢Ê≥ïÊî∂ÊïõÊÄß)
- [Ê¢ØÂ∫¶‰∏ãÈôçÊî∂ÊïõÊÄß](../ÂÆöÁêÜËØÅÊòéË°•ÂÖÖ-2025Âπ¥1Êúà.md#Ê¢ØÂ∫¶‰∏ãÈôçÊî∂ÊïõÊÄß)

### Âèç‰æã‰∏éÁâπÊÆäÊÉÖÂÜµË°•ÂÖÖ

- [ÈùûÂá∏‰ºòÂåñÂèç‰æã](../Âèç‰æã‰∏éÁâπÊÆäÊÉÖÂÜµË°•ÂÖÖ-2025Âπ¥1Êúà.md#ÈùûÂá∏‰ºòÂåñÂèç‰æã)
- [Â±ÄÈÉ®ÊúÄ‰ºòÈô∑Èò±](../Âèç‰æã‰∏éÁâπÊÆäÊÉÖÂÜµË°•ÂÖÖ-2025Âπ¥1Êúà.md#Â±ÄÈÉ®ÊúÄ‰ºòÈô∑Èò±)
- [Á∫¶ÊùüËøùÂèçÊÉÖÂÜµ](../Âèç‰æã‰∏éÁâπÊÆäÊÉÖÂÜµË°•ÂÖÖ-2025Âπ¥1Êúà.md#Á∫¶ÊùüËøùÂèçÊÉÖÂÜµ)

### ÂéÜÂè≤ËÉåÊôØË°•ÂÖÖ

- [‰ºòÂåñÁêÜËÆ∫ÂèëÂ±ïÂè≤](../ÂéÜÂè≤ËÉåÊôØË°•ÂÖÖ-2025Âπ¥1Êúà.md#‰ºòÂåñÁêÜËÆ∫ÂèëÂ±ïÂè≤)
- [ÈáçË¶ÅÊï∞Â≠¶ÂÆ∂Ë¥°ÁåÆ](../ÂéÜÂè≤ËÉåÊôØË°•ÂÖÖ-2025Âπ¥1Êúà.md#‰ºòÂåñÁêÜËÆ∫ÈáçË¶Å‰∫∫Áâ©)
- [ÁÆóÊ≥ïÂèëÂ±ïÂéÜÁ®ã](../ÂéÜÂè≤ËÉåÊôØË°•ÂÖÖ-2025Âπ¥1Êúà.md#‰ºòÂåñÁÆóÊ≥ïÂèëÂ±ïÂéÜÁ®ã)
